[{"title":"语义计算基础","path":"/2024/11/09/语义计算基础/","content":"编译原理 笔记 3 语法制导的语义计算基础 以上下文无关文法为基础，有两种分类： 属性文法：侧重语义计算规则 翻译模式：侧重语义计算过程 属性文法 基于CFG的改进： 为每个文法符号关联多个属性（类的成员） 为每个产生式关联一个语义规则集合（或称为语义动作） 语义动作就是使用当前产生式的时候，需要调用某个函数，或需要对产生式中出现的某个文法符号的某个属性进行赋值 如果产生式A→αA\\to \\alphaA→α关联了语义规则b≔f(c1,…,ck)b\\coloneqq f(c_{1}, \\dots, c_{k})b:=f(c1​,…,ck​) 如果bbb是AAA的属性，则称bbb是AAA的综合属性 如果bbb是B∈αB\\in\\alphaB∈α的属性，则称bbb是BBB的继承属性 基于属性文法的语义计算 计算方法分为两类： 树遍历方法 单遍的方法 基于树遍历方法的语义计算 构造输入串的语法分析树 构造依赖图 如果依赖图无圈，则可以按照该图的任何一种拓扑排序（排序中不改变有向边的关系）计算所有属性值 如果依赖图有圈，则不可以按照这种方法进行计算 依赖图的构造需要遍历两遍分析树，按照如下方法： 123456789101112131415graph = DependencyGraph()for node in tree.internal_nodes: # create nodes for all attributes of this non-terminator graph.create_nodes(node.productions.attributes) # create nodes for those functions instead of an assignment graph.create_nodes(node.productions.functions, virtual==True)for node in tree.internal_nodes: for rule in node.productions: # the vertex on behalf of this production vertex = rule.vertex parameters = rule.rhs.parameters if isinstance(rule, Assignment) else rule.parameters for para in parameters: graph.add_edge(vertex, para.vertex) 在计算完成后，我们可以得到一个完整的、已知每个节点的所有属性值的语法分析书，这棵树被称为带标注的语法分析树 基于单遍方法的语义计算 可以看出，树遍历分析需要遍历两次语法分析树，并且会有非良定义的情况，因此由另一种基于单遍的方法，分为自下而上和自上而下两种，只适用于特定的两类文法，我们讨论两种： S-属性文法：只包含综合属性 L-属性文法：继承属性的计算只依赖于产生式左端文法符号的继承属性，因为综合属性在此处还算不出来 S-属性文法的语义计算 通常采用自下而上的方法进行 如果采用LR分析方法，则多加一个语义栈，存放综合属性的值 由于S-属性文法只包含有综合属性，因此在需要通过产生式来规约的时候，产生式右端的各个符号所包含的，出现在该产生式所关联的语义规则右端的属性值，应当已经计算完成 利用LR分析进行S-属性文法语义计算的例子 L-属性文法的语义计算 采用DFS后序遍历的方法进行，首先遍历每一个产生式右端所需要的参数，计算其值之后再计算左端的值 利用LL分析进行L-属性文法语义计算的例子 基于翻译模式的语义计算 与基于属性文法的语义计算类似，但是允许语义规则集合出现在产生式右端的任何位置，显式地表示动作和属性计算的次序 因此我们需要限制每个属性值在被访问到的时候已经存在： 类S-属性文法：仅包含综合属性的情形，直接将语义规则集合放在产生式右端的末尾即可 类L-属性文法：右端符号的继承属性的计算必须位于该符号之前，并且其只能依赖于产生式左边符号的属性；计算产生式左端非终结符相应的语义规则置于产生式的尾部 翻译模式文法举例 我们只使用单遍的方法 自上而下 处理比较简单，在每一个ParseX函数中，将对应的语义规则集合加入到合适的位置即可，注意此时ParseX函数可能有参数了，参数即为X的继承属性 如果原来文法含有左递归，则需要在消除左递归的同时对翻译模式中的语义规则进行变换，变换方式为： 将： A→A1Y{A.a≔g(A1.a,Y.y)}A→X{A.a≔f(X.x)}\\begin{align*} A &amp;\\to A_{1}Y\\{A.a \\coloneqq g(A_{1}.a, Y.y)\\} \\\\ A &amp;\\to X\\{A.a \\coloneqq f(X.x)\\} \\end{align*} AA​→A1​Y{A.a:=g(A1​.a,Y.y)}→X{A.a:=f(X.x)}​ 变为： A→X{R.i≔f(X.x)}R{A.a≔R.s}R→Y{R1.i≔g(R.i,Y.y)}R1{R.s≔R1.s}R→ε{R.s≔R.i}\\begin{align*} A &amp;\\to X\\{R.i \\coloneqq f(X.x)\\}R\\{A.a \\coloneqq R.s\\} \\\\ R &amp;\\to Y\\{R_{1}.i \\coloneqq g(R.i, Y.y)\\}R_{1}\\{R.s\\coloneqq R_{1}.s\\} \\\\ R &amp;\\to \\varepsilon\\{R.s\\coloneqq R.i\\} \\end{align*} ARR​→X{R.i:=f(X.x)}R{A.a:=R.s}→Y{R1​.i:=g(R.i,Y.y)}R1​{R.s:=R1​.s}→ε{R.s:=R.i}​ 可以理解为，RRR的综合属性R.iR.iR.i用于计算，代替了原文法中A.aA.aA.a的作用，但是由于我们的计算现在是递归进行但是文法没有递归，所以我们需要增加一个继承属性R.sR.sR.s用于从下往上传递信息 详情请参考： sygl233的博客https://sygl233.github.io/2024/11/08/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-8/ 自下而上 类S-属性文法的计算是相似的，核心是类L-属性文法的自下而上计算（非常复杂，仅做简介） 从翻译模式中去掉嵌在产生式中间的语义规则集： 若规则集合中未关联任何属性，则引入新终结符NNN与N→ε{⋯ }N\\to \\varepsilon\\{\\cdots\\}N→ε{⋯}，后方跟随规则集合 若规则集合中关联了一些属性，则引入新终结符NNN与N→ε{⋯ }N\\to \\varepsilon\\{\\cdots\\}N→ε{⋯}，后方跟随规则集合，并在适当的地方增加复写规则 继承属性的访问：产生式右端的符号串中，靠左符号的综合属性可以替代靠右符号的继承属性 例如A→XYA\\to XYA→XY中，如果关联的语义规则需要用到继承属性Y.iY.iY.i，并且存在复写规则Y.i≔X.sY.i\\coloneqq X.sY.i:=X.s则可以用XXX的综合属性X.sX.sX.s来代替 TODO","tags":["笔记","编原","语法分析","自底向上"],"categories":["编译原理"]},{"title":"消息验证","path":"/2024/11/06/消息验证/","content":"计算机网络安全技术 笔记 2 消息验证 在网络中，攻击手段包括： 伪装：攻击者产生非法信息或欺诈应答 内容修改：修改内容 顺序修改：对双方发送消息的顺序进行修改 计时修改：对消息的延时和重放 发送方否认：发送方否认发送过消息 接收方否认：接收方否认接受过消息 消息验证的主要目的是验证收到的消息确实来自真正的发送方并且未被修改，认证的过程通过认证符进行，产生认证符的函数称为认证函数，认证函数可以分为三类： 消息加密：直接加密整个消息 消息认证码MAC：通过MAC函数处理消息，将结果作为认证符 HASH函数：将hash函数当成认证符 主要使用的是Hash 消息加密 对称加密 例如对每个消息附加一个错误检测码，或称之为帧校验序列FCS 利用FCS进行对称加密 实际应用：TCP/IP协议 公钥加密 可以提供保密性，如果需要提供认证（数字签名），则发送方需要首先利用自身的私钥进行加密，再利用接收方的公钥进行加密 消息验证码MAC MAC类似于一个加密函数，但是不要求可逆性，它利用密码生成一个固定长度的短数据块，并将该数据块附加在消息之后 发送方与接收方共享一个密钥K，发送方将消息的MAC附在消息之后，接收方也对消息计算MAC，判断是否被修改 当然我们也可以将携带了MAC的消息进行加密，或者将加密后的消息计算MAC，来达到认证+保密的效果 将同一消息广播给很多接受者 在信息交换中，接受者随机对消息进行认证 对明文形式的计算机程序进行认证 只需要认证而不需要保密：如SNMPv3 需要将认证和保密性分开处理 肉丁土豆 hash码也称为消息摘要(Message Digest, MD) 用于认证的方法 将消息hash之后连接，再用对称密码加密，对称密码用于认证，hash用于保密 将消息hash之后用对称密码加密，再附在消息后，本质上就是一种MAC 将消息hash之后用发送方的私钥加密，再附在消息后，提供了数字签名与认证能力 将消息hash之后用发送方的私钥加密，附在消息后，再使用对称密码加密，提供了签名、认证与保密性 发送方与接收方共享秘密值SSS，将SSS附在消息后再hash，而不直接通信SSS 发送方与接收方共享秘密值SSS，将SSS附在消息后再hash，得到的结果用对称密码加密 hash特点 H:M→HH: \\mathcal{M} \\to \\mathcal{H}H:M→H是满足如下条件的映射： M=Σ∗\\mathcal{M} = \\Sigma^{*}M=Σ∗ H={w ∣ len(w)≡n(const)}\\mathcal{H} = \\{w\\,|\\,\\mathrm{len}(w) \\equiv n\\rm(const)\\}H={w∣len(w)≡n(const)} ∀m∈MH(m)\\forall m \\in \\mathcal{M}\\quad H(m)∀m∈MH(m)是可计算的 ∀h∈HH−1(h)\\forall h \\in \\mathcal{H}\\quad H^{-1}(h)∀h∈HH−1(h)是不可计算的 给定mmm，找到m′≠mm&#x27; eq mm′=m使得H(m)=H(m′)H(m) = H(m&#x27;)H(m)=H(m′)是不可计算的，称为抗弱碰撞 找到任何x≠yx eq yx=y使得H(x)=H(y)H(x) = H(y)H(x)=H(y)是不可计算的，称为抗强碰撞 一般结构 一般迭代地进行计算，给定一个初始值CV0\\mathrm{CV}_{0}CV0​，将输入分为等长的LLL组，最后一组如果长度不足则padding： H(m)=CVLCVi=f(CVi−1,Yi−1)\\begin{align*} H(m) &amp;= \\mathrm{CV}_{L} \\\\ \\mathrm{CV}_{i} &amp;= f(\\mathrm{CV}_{i-1}, Y_{i-1}) \\end{align*} H(m)CVi​​=CVL​=f(CVi−1​,Yi−1​)​ fff称之为压缩函数，本质上我们需要找到合适的fff MD5 一种针对32位，倾向于小端机的算法 输入：任意长度 分组长度：512比特 输出长度：128比特 增加填充位 直接在整个消息之后填充10∗10^{*}10∗，使得填充后消息的长度lll满足：l≡448 mod 512l \\equiv 448 \\text{ mod } 512 l≡448 mod 512 填充长度 设第一步之后的报文长度为lll，则用64位二进制数表示l mod 264l\\text{ mod } 2^{64}l mod 264，并将结果附在最后，得到总长度是512倍数的消息 初始化缓存 初始化用于存储CV\\mathrm{CV}CV的128位缓冲区，使用4个32位寄存器表示，初始值为： 32'h01234567 32'h89abcdef 32'hfedcba98 32'h76543210 迭代计算MD5迭代过程 每次迭代即为：c,d,a←b,c,db←a+((a+gcd(b,c,d)+XK+Ti)&lt;&lt;&lt;3)\\begin{align*} c, d, a &amp;\\leftarrow b, c, d \\\\ b &amp;\\leftarrow a + ((a + gcd(b, c, d) + X_{K} + T_{i}) &lt;&lt;&lt; 3) \\end{align*} c,d,ab​←b,c,d←a+((a+gcd(b,c,d)+XK​+Ti​)&lt;&lt;&lt;3)​ 逻辑函数为： F(b,c,d)=(b⊕c)∨(¬b⊕d)F(b, c, d) = (b\\oplus c)\\vee( eg b \\oplus d)F(b,c,d)=(b⊕c)∨(¬b⊕d) G(b,c,d)=(b⊕d)∨(c⊕¬d)G(b, c, d) = (b\\oplus d)\\vee(c \\oplus eg d)G(b,c,d)=(b⊕d)∨(c⊕¬d) H(b,c,d)=b⊕c⊕dH(b, c, d) = b\\oplus c\\oplus dH(b,c,d)=b⊕c⊕d I(b,c,d)=c⊕(b∨¬d)I(b, c, d) = c\\oplus (b\\vee eg d)I(b,c,d)=c⊕(b∨¬d) 输出 SHA SHA即Secure Hash Algorithm，其基于MD4算法，因此大部分环节和MD算法是类似的 SHA-1输入少于2642^{64}264位，输出为160位，分组长度512位 增加填充位 填充长度 初始化MD缓存 迭代计算，十轮压缩分为两组，每轮执行16迭代20步 输出 SHA压缩函数 RIPEMD-160 RIPEMD-160输入任意长（这里和后面的表冲突了），输出为160位，分组长度512位 增加填充位 填充长度 初始化MD缓存 迭代计算，每轮16步迭代 输出 Hash函数比较 数字签名算法DSS 消息认证可以保证通信双方不收第三方的攻击，但是不能处理通信双方伪造消息或否认发送过消息，这个操作","tags":["笔记","网安","消息验证"]},{"title":"集群使用问题记录","path":"/2024/11/02/集群使用记录/","content":"集群开发过程中遇到的一些问题的记录 由于科研需求，我需要在一个集群上完成一些实验，这也是本人第一次接触集群与HPC，所以遇到了很多问题，在此作简要记录，以便后期需要 注：集群是aarch64架构 复现WebRL WebRL是一个基于现有模型，为其添加一个RL来实现更好效果的WebAgent，我的工作是复现这篇文章，由于这个仓库并没有在github上开源，因此缺少了很多community的帮助，并且其README写的非常高屋建瓴，如果直接跟随的话会报大错，经过了大概7h的研究，现在能够在这个集群上初始化了 在写这些文字的时候，llama模型的权限还没申请到，因此还不能确保开始train之后没问题鸣鸣鸣别骂我 Update：数据集没开源🤡 服务器登录 用ssh即可，登录没什么要说的，不过服务器是有权限ban掉免密登录操作的，建议在遇到一直无法免密的时候先询问一下负责人 SLURM指令 Slurm基础指令https://blog.csdn.net/lejun_wang1984/article/details/135180652 Slurm详细教程https://wiki.cheng-group.net/ikkem-hpc-manual/slurm/sbatch/ 由于我们一开始登录到集群的时候是到了登录节点，这个节点下是没有卡的，因此查看会显示没卡，我们需要先进入一个计算节点 12srun --pty --gres=gpu:1 bash # 进入交互模式，拿到一张卡nvidia-smi # 或gpustat等命令 注：--gres=gpu:1是旧版语法，表示申请1个gpu计算资源，新版语法写为--gpus=1 环境配置 如果需要使用NVIDIA的显卡，则需要使用对应的编程语言CUDA，CUDA对应的编译器为nvcc(NVIDIA CUDA Compiler)，用于将CUDA程序编译成可以在NVIDIA GPU上运行的代码 可以类比C语言和gcc的关系 在conda环境下安装nvcc非常简单，例如： 1conda install nvidia/label/cuda-12.1.0::cuda-nvcc 这代表会通过nvidia渠道，在当前环境下安装CUDA 12.1.0版本的nvcc编译器。 之后可以根据仓库中的README进行操作，上述步骤基本不会在任何README中出现 有的README中会让我们使用pip install -e .的命令，这条命令的本意就是往当前环境下安装一堆python包，其详细含义是： .代表当前路径，也即会在当前路径下开始搜索，将所有的python包安装到环境中 -e代表采用editable模式，也即不会将路径下的所有包都下载到环境中，而是建立了一个软连接，我的理解是在环境中存了一个引用，这样我们可以直接修改路径下的代码，而不需要反复的pip install 通常采用这个方式的原因是，我们希望将当前的包更够更方便的被使用，避免使用繁杂的路径或跨根目录导入 环境变量 这个任务中，我使用的是LLama-Factory这个库的某个版本来train一个baseline，根据README，我需要使用llamafactory-cli来train，遇到的第一个问题是环境变量问题，报错如下 1ValueError: Please use `FORCE TORCHRUN=1` to launch Deepspeed training 因此按照其指示设置全局变量即可，在sbatch提交的脚本中添加 1export FORCE_TORCHRUN=1 即可 随后遇见一个AssertError，排查（直接看源码）后发现同样是环境变量没设置好，添加： 1export NPROC_PER_NODE=$SLURM_NTASKS_PER_NODE 后面的环境变量由Slurm提供 版本问题 最终我们遇到了版本问题，在解决后发现是torch版本与deepspeed版本不兼容导致的 首先，deepspeed版本过高会导致无法检测到集群中的CUDA，遂降版本0.15.3-&gt;0.13.5 检测不到CUDA会导致其去编译一些xx/deepspeed/ops/csrc/cpu/路径下的文件，但是由于本集群是aarch64架构，因此出现了C++编译失败的问题，可以参考下面这个issue https://github.com/microsoft/DeepSpeed/issues/5640https://github.com/microsoft/DeepSpeed/issues/5640 降版本之后，出现了ImportError，参考下面的issue，简略来说就是把torch旧版本的代码复制过来（异术） https://github.com/microsoft/DeepSpeed/issues/5603https://github.com/microsoft/DeepSpeed/issues/5603","tags":["Slurm","DeepSpeed","CUDA"],"categories":["科研"]},{"title":"DLRM","path":"/2024/11/01/DLRM/","content":"DLRM 总结 DLRM AdaEmbed 提出了In-training pruning system AdaEmbed，动态优化embedding table 应用了VHDI AdaEmbed Overview 具体算法为： AdaEmbed Algorithm 包含了三个主要模块： AdaEmbed Coordinator: 从Agents处收集importance，决定prune strategy，协调prune的执行 Memory Manager：每一个Agent会内置一个，用于管理Embedding table的物理内存 Embedding Monitor：决定importance，向Coordinator通信 同一个table内，每一个embedding row会有一个EI(i)EI(i)EI(i)来决定其重要性，更新方法为： EI(i)=freqt(i)+∣∣∇gt(i)∣∣+EI(i−1)EI(i) = freq_{t}(i) + || abla_{g_{t}}(i)|| + EI(i-1) EI(i)=freqt​(i)+∣∣∇gt​​(i)∣∣+EI(i−1) 并且每TTT次迭代进行一定比例的衰减 不同table之间，首先对每个table的importance，利用这个特征的分布的下95%分位数做归一化，之后将embedding vector dim相同的feature划分成一个group，在group内做prune，每个组结果的embedding table大小由总大小按比例分配得到 决定prune的时机需要判断embedding vectors中importance变化足够剧烈的有多少，但是全部判断overhead太大，因此在每一个agent中sample一小部分出来判断，并据此估计全局 为了避免过多的内存开销，引入VHDI（感觉很像页表） VHDI Overview 重分配一个embedding的时候直接zero-initialization FIITED 基于以下观察： 不同特征之间的重要性不同 每个特征对应嵌入表中，行的重要性不同 每一个嵌入向量的维度重要性不同 重要性会随着时间发生改变 FIIT Overview 将嵌入表中的每一个嵌入向量分成了多个chunk，嵌入表的store与prune是基于chunk进行的，减少了内存使用 FIIT Overview 当一个prune了的chunk需要重新keep的时候，其值会被重新初始化 在做训练的过程中，对于不等长的embedding vector，采用zero-padding的方式 每次迭代的过程中，更新chunk utility value的方法是： u(i+1)=γu(i)+a(i)g(i)u(i + 1) = \\gamma u(i) + a(i)g(i) u(i+1)=γu(i)+a(i)g(i) 其中a(i)a(i)a(i)是访问次数，g(i)g(i)g(i)是梯度的2-范数 chunk的pruning ratio可以指定也可以训 同时也可以prune某一些dimension，相当于将不同embedding tables的同一个维度当成一个样本，决定其importance并决定是否prune 如果直接实现这个算法，会导致有大量的内存碎块，因此使用AdmEmbed中提出的VHDI算法，并添加一个chunk manager FIIT with VHDI 优化了四个方面的开销： hash表查找 Embedding table查找 更新每一个chunk的utility value Prune 方法是： 并行计算每一个chunk的utility value与并行prune 将前一个batch的utility value update放在后一个batch的forward过程中并行完成 采用pre-fetching技术优化两次查表"},{"title":"信号处理原理 5","path":"/2024/10/29/信号处理原理5/","content":"信号处理原理 笔记 5 离散时间信号的Fourier分析 DTFT 离散时间傅里叶变换 由于真实信号一般没有解析表达式，因此需要通过抽样值来计算出理想抽样信号的频谱密度函数： F(ω)=TF^(ω)=T∑n=−∞∞f(nT)e−jnωT,ω∈[−ωs2,ωs2]F(\\omega) = T\\hat{F}(\\omega) = T\\sum\\limits_{n=-\\infty}^{\\infty}f(nT)e^{-jn\\omega T} ,\\quad \\omega\\in[-\\frac{\\omega_{s}}{2}, \\frac{\\omega_{s}}{2}]F(ω)=TF^(ω)=Tn=−∞∑∞​f(nT)e−jnωT,ω∈[−2ωs​​,2ωs​​] 上式在满足奈奎斯特采样定理的时候严格成立，如果不满足则会导致混叠现象的发生，此时第一个等于号改为≈\\approx≈ 从中可以看出，f(nT)f(nT)f(nT)实际上是F^(ω)\\hat{F}(\\omega)F^(ω)的FS系数，因此有： f(nT)=1ωs∫−ωs/2ωs/2F^(ω)ejnωTdωf(nT) = \\frac{1}{\\omega_{s}}\\int_{-\\omega_{s}/2}^{\\omega_{s}/2} \\hat{F}(\\omega)e^{jn\\omega T}d\\omega f(nT)=ωs​1​∫−ωs​/2ωs​/2​F^(ω)ejnωTdω 注意指数上的符号 频率归一化 为了方便，将采样频率归一，也即将f(nT)f(nT)f(nT)中的TTT在数学上化简为1，将频率归一化的离散信号称为数字信号，将数字信号DTFT归一化频谱为数字频谱 在归一化之后，采样频率为ωs=2πT=2π\\omega_{s} = \\frac{2\\pi}{T} = 2\\piωs​=T2π​=2π 代入公式有： X(ω)=DTFT[x(n)]=∑n=−∞∞x(n)e−jnωx(n)=12π∫−ππX(ω)ejnωdω\\begin{align*} X(\\omega) = \\mathrm{DTFT}[x(n)] = \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-jn\\omega} \\\\ x(n) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi}X(\\omega)e^{jn\\omega}d\\omega \\end{align*} X(ω)=DTFT[x(n)]=n=−∞∑∞​x(n)e−jnωx(n)=2π1​∫−ππ​X(ω)ejnωdω​ 我们将抽样时的真实频率（又称为模拟频率）记为Ω\\OmegaΩ，引入归一化时间之后的数字频率为ω\\omegaω，则： 模拟频率的Nyquist区间为：[−Ωs2,Ωs2][-\\dfrac{\\Omega_{s}}{2}, \\dfrac{\\Omega_{s}}{2}][−2Ωs​​,2Ωs​​] 数字频率的Nyquist区间为[−π,π][-\\pi, \\pi][−π,π] 二者的关系为ω=ΩTs\\omega = \\Omega T_{s}ω=ΩTs​ 性质 周期性 X(ω)=X(ω+2π)X(\\omega) = X(\\omega + 2\\pi) X(ω)=X(ω+2π) 证明是显然的 线性 DTFT(∑kakxk)=∑kakDTFT(xk)\\mathrm{DTFT}(\\sum\\limits_{k} a_{k}x_{k}) = \\sum\\limits_{k} a_{k}\\mathrm{DTFT}(x_{k}) DTFT(k∑​ak​xk​)=k∑​ak​DTFT(xk​) 原因是DTFT本质上其实是一种特殊频率的采样 平移特性 DTFT[x(n−n0)]=e−jωn0X(ω)DTFT[ejω0nx(n)]=X(ω−ω0)\\begin{align*} \\mathrm{DTFT}[x(n-n_{0})] &amp;= e^{-j\\omega n_{0}}X(\\omega) \\\\ \\mathrm{DTFT}[e^{j\\omega_{0} n}x(n)] &amp;= X(\\omega - \\omega_{0}) \\end{align*} DTFT[x(n−n0​)]DTFT[ejω0​nx(n)]​=e−jωn0​X(ω)=X(ω−ω0​)​ 证明和FT的证明是类似的 反褶与共轭 DTFT[x(−n)]=X(−ω)DTFT[x∗(n)]=X∗(−ω)\\begin{align*} \\mathrm{DTFT}[x(-n)] &amp;= X(-\\omega) \\\\ \\mathrm{DTFT}[x^{*}(n)] &amp;= X^{*}(-\\omega) \\end{align*} DTFT[x(−n)]DTFT[x∗(n)]​=X(−ω)=X∗(−ω)​ 证明和FT的证明是类似的 频域与时域变化 DTFT[nxn]=j[ddωX(ω)]DTFT[xa(n)]=X(aω)\\begin{align*} \\mathrm{DTFT}[nx_{n}] &amp;= j\\bigl[\\frac{d}{d\\omega}X(\\omega)\\bigr] \\\\ \\mathrm{DTFT}[x_{a}(n)] &amp;= X(a\\omega) \\end{align*} DTFT[nxn​]DTFT[xa​(n)]​=j[dωd​X(ω)]=X(aω)​ 上式中，由于xnx_{n}xn​的定义域为Z\\mathbb{Z}Z，所以我们定义其压缩后的结果为： xa(n)={x(na)na∈Z0na∉Z\\begin{align*} x_{a}(n) = \\begin{cases} x(\\frac{n}{a}) &amp; \\frac{n}{a} \\in \\mathbb{Z} \\\\ 0 &amp; \\frac{n}{a} otin \\mathbb{Z} \\end{cases} \\end{align*} xa​(n)={x(an​)0​an​∈Zan​∈/Z​​ 卷积定理 首先我们需要定义两种新的卷积形式： 离散卷积： x1(n)∗x2(n)=∑k=−∞∞x1(n−k)x2(k)x_{1}(n) * x_{2}(n) = \\sum\\limits_{k=-\\infty}^{\\infty}x_{1}(n-k)x_{2}(k) x1​(n)∗x2​(n)=k=−∞∑∞​x1​(n−k)x2​(k) 圆周卷积： X1(ω)⊗X2(ω)=∫−ππX1(ω′)X2(ω−ω′)dω′X_{1}(\\omega) \\otimes X_{2}(\\omega) = \\int_{-\\pi}^{\\pi}X_{1}(\\omega&#x27;)X_{2}(\\omega - \\omega&#x27;)d\\omega&#x27; X1​(ω)⊗X2​(ω)=∫−ππ​X1​(ω′)X2​(ω−ω′)dω′ 于是： DTFT[x1(n)∗x2(n)]=X1(ω)⋅X2(ω)DTFT[x1(n)⋅x2(n)]=12πX1(ω)⊗X2(ω)\\begin{align*} \\mathrm{DTFT}[x_{1}(n) * x_{2}(n)] &amp;= X_{1}(\\omega)\\cdot X_{2}(\\omega) \\\\ \\mathrm{DTFT}[x_{1}(n) \\cdot x_{2}(n)] &amp;= \\frac{1}{2\\pi}X_{1}(\\omega) \\otimes X_{2}(\\omega) \\\\ \\end{align*} DTFT[x1​(n)∗x2​(n)]DTFT[x1​(n)⋅x2​(n)]​=X1​(ω)⋅X2​(ω)=2π1​X1​(ω)⊗X2​(ω)​ Parseval能量定理 ∑n=−∞∞∣x(n)∣2=12π∫−ππ∣X(ω)∣2dω\\sum\\limits_{n=-\\infty}^{\\infty}|x(n)|^{2} = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}|X(\\omega)|^{2}d\\omega n=−∞∑∞​∣x(n)∣2=2π1​∫−ππ​∣X(ω)∣2dω 有限长DTFT 现实中无法获得无限长的时域频谱，于是我们需要一个窗函数进行截取 w(n)={10≤n≤L−10else\\begin{align*} w(n) = \\begin{cases} 1 &amp; 0 \\leq n \\leq L - 1 \\\\ 0 &amp; \\rm else \\end{cases} \\end{align*} w(n)={10​0≤n≤L−1else​​ 截取后的信号为： xL(n)=x(n)w(n)={x(n)0≤n≤L−10else\\begin{align*} x_{L}(n) = x(n)w(n) = \\begin{cases} x(n) &amp; 0\\leq n \\leq L - 1 \\\\ 0 &amp; \\rm else \\end{cases} \\end{align*} xL​(n)=x(n)w(n)={x(n)0​0≤n≤L−1else​​ 对应的频谱变为： XL(ω)=∑n=−∞∞xL(n)e−jωn=∑n=0L−1x(n)e−jωnX_{L}(\\omega) = \\sum\\limits_{n=-\\infty}^{\\infty}x_{L}(n)e^{-j\\omega n} = \\sum\\limits_{n=0}^{L - 1}x(n)e^{-j\\omega n} XL​(ω)=n=−∞∑∞​xL​(n)e−jωn=n=0∑L−1​x(n)e−jωn 另一种理解是，我们利用DTFT的卷积特性来理解 XL(ω)=DTFT[x(n)w(n)]=12πX(ω)⊗W(ω)=12π∫−ππX(ω′)W(ω−ω′)dω′\\begin{align*} X_{L}(\\omega) &amp;= \\mathrm{DTFT}[x(n)w(n)] \\\\ &amp;= \\frac{1}{2\\pi}X(\\omega)\\otimes W(\\omega) \\\\ &amp;= \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}X(\\omega&#x27;)W(\\omega - \\omega&#x27;)d\\omega&#x27; \\end{align*} XL​(ω)​=DTFT[x(n)w(n)]=2π1​X(ω)⊗W(ω)=2π1​∫−ππ​X(ω′)W(ω−ω′)dω′​ 于是乎我们需要首先来求窗函数的频谱： W(ω)=DTFT[w(n)]=∑n=−∞∞w(n)e−jnω=∑n=0L−1e−jnω=1−e−jLω1−e−jω=1−cos⁡(Lω)+jsin⁡(Lω)1−cos⁡(jω)+jsin⁡(jω)=2sin⁡2(Lω2)+2jsin⁡(Lω2)cos⁡Lω22sin⁡2(ω2)+2jsin⁡(ω2)cos⁡ω2=sin⁡(Lω2)sin⁡(ω2)⋅cos⁡(Lω2)−jsin⁡(Lω)cos⁡(ω2)−jsin⁡(ω)=sin⁡(Lω2)sin⁡(ω2)e−j(L−1)ω2\\begin{align*} W(\\omega) &amp;= \\mathrm{DTFT}[w(n)] \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}w(n)e^{-jn\\omega} \\\\ &amp;= \\sum\\limits_{n=0}^{L-1}e^{-jn\\omega} \\\\ &amp;= \\frac{1 - e^{-jL\\omega}}{1 - e^{-j\\omega}} \\\\ &amp;= \\frac{1 - \\cos(L\\omega) + j\\sin(L\\omega)}{1 - \\cos(j\\omega) + j\\sin(j\\omega)} \\\\ &amp;= \\frac{2\\sin^{2}(\\frac{L\\omega}{2}) + 2j\\sin(\\frac{L\\omega}{2})\\cos{\\frac{L\\omega}{2}}}{2\\sin^{2}(\\frac{\\omega}{2}) + 2j\\sin(\\frac{\\omega}{2})\\cos{\\frac{\\omega}{2}}} \\\\ &amp;= \\frac{\\sin(\\frac{L\\omega}{2})}{\\sin(\\frac{\\omega}{2})}\\cdot \\frac{\\cos(\\frac{L\\omega}{2}) - j\\sin(L\\omega)}{\\cos(\\frac{\\omega}{2}) - j\\sin(\\omega)} \\\\ &amp;= \\frac{\\sin(\\frac{L\\omega}{2})}{\\sin(\\frac{\\omega}{2})} e^{-\\frac{j(L-1)\\omega}{2}} \\end{align*} W(ω)​=DTFT[w(n)]=n=−∞∑∞​w(n)e−jnω=n=0∑L−1​e−jnω=1−e−jω1−e−jLω​=1−cos(jω)+jsin(jω)1−cos(Lω)+jsin(Lω)​=2sin2(2ω​)+2jsin(2ω​)cos2ω​2sin2(2Lω​)+2jsin(2Lω​)cos2Lω​​=sin(2ω​)sin(2Lω​)​⋅cos(2ω​)−jsin(ω)cos(2Lω​)−jsin(Lω)​=sin(2ω​)sin(2Lω​)​e−2j(L−1)ω​​ 我们来分析窗函数的幅度频谱函数∣W(ω)∣=sin⁡(Lω2)sin⁡(ω2)x∈(−π,π)|W(\\omega)| = \\dfrac{\\sin(\\frac{L\\omega}{2})}{\\sin(\\frac{\\omega}{2})}\\quad x\\in(-\\pi, \\pi)∣W(ω)∣=sin(2ω​)sin(2Lω​)​x∈(−π,π) 窗函数的幅度频谱函数的图像 如上图所示，在(−π,π)(-\\pi, \\pi)(−π,π)内，窗函数的频谱会有2⌊L2⌋2\\lfloor\\dfrac{L}{2}\\rfloor2⌊2L​⌋个零点，其中频谱强度最大的为原点，我们定义主瓣宽度为： ΔωW=2πL\\Delta \\omega_{W} = \\frac{2\\pi}{L} ΔωW​=L2π​ 可以看到，绝对值为主瓣内的频谱占据了所有频谱的绝大部分能量，而旁瓣，也即频率绝对值在(2πL,π)(\\dfrac{2\\pi}{L}, \\pi)(L2π​,π)之间的部分其实相当于能量的损耗，也即出现了频率泄露 分辨率问题 我们考虑下面这个例子： x(n)=A1ejω1n+A2ejω2nx(n) = A_{1}e^{j\\omega_{1}n} + A_{2}e^{j\\omega_{2}n} x(n)=A1​ejω1​n+A2​ejω2​n 很明显其频谱为： X(ω)=A1δ(ω−ω1)+A2δ(ω−ω2)X(\\omega) = A_{1}\\delta(\\omega - \\omega_{1}) + A_{2}\\delta(\\omega - \\omega_{2}) X(ω)=A1​δ(ω−ω1​)+A2​δ(ω−ω2​) 画出图像可以看出，只要ω1≠ω2\\omega_{1} eq \\omega_{2}ω1​=ω2​，这两个冲激函数就不会重叠，因此其实他的分辨率是无限高的 但是我们加窗之后有： xL(n)=A1ejω1n+A2ejω2nn=0,1,…L−1x_{L}(n) = A_{1}e^{j\\omega_{1}n} + A_{2}e^{j\\omega_{2}n} \\quad n = 0, 1, \\dots L-1 xL​(n)=A1​ejω1​n+A2​ejω2​nn=0,1,…L−1 频谱为： XL(ω)=A1W(ω−ω1)+A2W(ω−ω2)X_{L}(\\omega) = A_{1}W(\\omega - \\omega_{1}) + A_{2}W(\\omega - \\omega_{2}) XL​(ω)=A1​W(ω−ω1​)+A2​W(ω−ω2​) 加窗后的频谱函数 也即Δω\\Delta \\omegaΔω越小，分辨的难度越大，我们定义不可分辨的范围是二者的零点进入了对方的主瓣内，也即可以分辨的范围是： Δω≥ΔωW=2πL\\Delta\\omega \\geq \\Delta\\omega_{W} = \\frac{2\\pi}{L} Δω≥ΔωW​=L2π​ 因此在这个例子中，如果希望能清晰分辨，窗的宽度应该满足L≥2π/ΔωL \\geq 2\\pi/\\Delta\\omegaL≥2π/Δω 针对DTFT一题作业的思考 作业原题如下： 已知x(n)x(n)x(n)的DTFT为X(ω)X(\\omega)X(ω)，试求x(2n+1)x(2n+1)x(2n+1)的DTFT 常规做法为： 注意到：X(ω)=∑n=−∞∞x(n)e−jnωX(ω+π)=∑n=−∞∞x(n)e−jn(ω+π)=∑n=−∞∞x(n)e−jnωe−jnπ=∑n=−∞∞(−1)nx(n)e−jnω\\begin{align*} X(\\omega) &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-j n\\omega} \\\\ X(\\omega + \\pi) &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-j n(\\omega + \\pi)} \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-j n\\omega}e^{-j n \\pi} \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}(-1)^{n}x(n)e^{-j n\\omega}\\end{align*}X(ω)X(ω+π)​=n=−∞∑∞​x(n)e−jnω=n=−∞∑∞​x(n)e−jn(ω+π)=n=−∞∑∞​x(n)e−jnωe−jnπ=n=−∞∑∞​(−1)nx(n)e−jnω​因此我们有：DTFT[x(2n)]=∑n=−∞∞x(2n)e−jnω=∑n=−∞∞x(2n)e−j(2n)ω2=X(ω/2)+X((ω/2)+π)2\\begin{align*} \\mathrm{DTFT}[x(2n)] &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(2n)e^{-jn\\omega} \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(2n)e^{-j ( 2n)\\frac{\\omega}{2}} \\\\ &amp;= \\frac{X(\\omega/2) + X((\\omega / 2) + \\pi)}{2}\\end{align*}DTFT[x(2n)]​=n=−∞∑∞​x(2n)e−jnω=n=−∞∑∞​x(2n)e−j(2n)2ω​=2X(ω/2)+X((ω/2)+π)​​于是我们可以首先利用时域上的频移特性，再利用上述发现扩张特性找到最终答案DTFT[x(2n+1)]=ejω⋅X(ω/2)+X((ω/2)+π)2\\begin{align*} \\mathrm{DTFT}[x(2n + 1)] &amp;= e^{j\\omega}\\cdot \\frac{X(\\omega/2) + X((\\omega / 2) + \\pi)}{2}\\end{align*}DTFT[x(2n+1)]​=ejω⋅2X(ω/2)+X((ω/2)+π)​​ 这题让我们不禁开始思考，我们能否求出采样频率经过任意线性变换之后得到的新信号x(an+b)x(an+b)x(an+b)，其中a,b∈Za, b \\in \\mathbb{Z}a,b∈Z，其DTFT与之前的之间有什么关系呢，和上面思路类似，我们最重要的是需要求出x(n)→x(an)x(n) \\to x(an)x(n)→x(an)这一步 注意到，我们在上述过程，即a=2a = 2a=2时，利用了X(ω+0)X(\\omega + 0)X(ω+0)与X(ω+π)X(\\omega + \\pi)X(ω+π)两个频谱函数进行叠加，而000和π\\piπ恰好是z(n)=e−j⋅(an)ω−1z(n) = e^{-j\\cdot(an)\\omega} - 1z(n)=e−j⋅(an)ω−1的两个零点！ 记： ck=2kπaλk=eckk=0,1,…,a−1\\begin{align*} c_{k} &amp;= \\frac{2k\\pi}{a} \\\\ \\lambda_{k} &amp;= e^{c_{k}} \\\\ k &amp;= 0, 1, \\dots, a - 1 \\end{align*} ck​λk​k​=a2kπ​=eck​=0,1,…,a−1​ 则λk\\lambda_{k}λk​为aaa次单位根 此时有： Xk(ω)=X(ω−ck)=∑n=−∞∞x(n)e−jn(ω−ck)=λk∑n=−∞∞x(n)e−jnω\\begin{align*} X_{k}(\\omega) &amp;= X(\\omega - c_{k}) \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-j n(\\omega - c_{k})} \\\\ &amp;= \\lambda_{k}\\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-jn\\omega} \\end{align*} Xk​(ω)​=X(ω−ck​)=n=−∞∑∞​x(n)e−jn(ω−ck​)=λk​n=−∞∑∞​x(n)e−jnω​ 我们知道对于MMM次单位根与任意非负整数rrr，存在： ∑m=0M−1(λr)m={1−λrM1−λr=0r&gt;0Mr=0\\begin{align*} \\sum\\limits_{m=0}^{M-1}(\\lambda^{r})^{m} &amp;= \\begin{cases}\\frac{1 - \\lambda^{rM}}{1 - \\lambda^{r}} = 0 &amp; r &gt; 0 \\\\ M &amp; r = 0 \\end{cases} \\end{align*} m=0∑M−1​(λr)m​={1−λr1−λrM​=0M​r&gt;0r=0​​ 记n=qa+rn = qa + rn=qa+r，则Xk(ω)X_{k}(\\omega)Xk​(ω)可变化为： Xk(ωa)=X(ω−2kπa)=∑n=−∞∞x(n)e−j(qa+r)ω−2kπa=∑n=−∞∞x(n)e−jqω+rωaλkr=λkr∑n=−∞∞x(n)e−jqω+rωa\\begin{align*} X_{k}(\\frac{\\omega}{a}) &amp;= X(\\frac{\\omega - 2k\\pi}{a}) \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-j(qa+r)\\frac{\\omega - 2k\\pi}{a}} \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-jq\\omega + \\frac{r\\omega}{a}}\\lambda_{k}^{r} \\\\ &amp;= \\lambda_{k}^{r}\\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-jq\\omega + \\frac{r\\omega}{a}} \\end{align*} Xk​(aω​)​=X(aω−2kπ​)=n=−∞∑∞​x(n)e−j(qa+r)aω−2kπ​=n=−∞∑∞​x(n)e−jqω+arω​λkr​=λkr​n=−∞∑∞​x(n)e−jqω+arω​​ 于是乎，我们有： ∑k=0a−1Xk(ωa)=(∑k=0a−1λkr)∑n=−∞∞x(n)e−jqω+jrωa=a∑q=−∞∞x(qa)e−jqω=aDTFT[x(an)]\\begin{align*} \\sum\\limits_{k=0}^{a-1}X_{k}(\\frac{\\omega}{a}) &amp;= (\\sum\\limits_{k=0}^{a-1}\\lambda_{k}^{r})\\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-jq\\omega + \\frac{jr\\omega}{a}} \\\\ &amp;= a\\sum\\limits_{q=-\\infty}^{\\infty}x(qa)e^{-jq\\omega} \\\\ &amp;= a\\mathrm{DTFT}[x(an)] \\end{align*} k=0∑a−1​Xk​(aω​)​=(k=0∑a−1​λkr​)n=−∞∑∞​x(n)e−jqω+ajrω​=aq=−∞∑∞​x(qa)e−jqω=aDTFT[x(an)]​ 我们便得到了DTFT[x(an)]\\mathrm{DTFT}[x(an)]DTFT[x(an)]的表达式 DTFT[x(an)]=1a∑k=0a−1X(ω−2kπa)\\begin{align*} \\mathrm{DTFT}[x(an)] &amp;= \\frac{1}{a}\\sum\\limits_{k=0}^{a-1}X(\\frac{\\omega - 2k\\pi}{a} ) \\\\ \\end{align*} DTFT[x(an)]​=a1​k=0∑a−1​X(aω−2kπ​)​ 那怎么对应的求出DTFT[x(an+b)]\\mathrm{DTFT}[x(an + b)]DTFT[x(an+b)]的表达式呢，显然不能直接套用时移的公式，因为我们不能保证a ∣ ba \\,|\\, ba∣b，我们观察上述推导过程中将Xk(ωa)X_{k}(\\frac{\\omega}{a})Xk​(aω​)求和的一步，这一步通过单位根等比求和筛选出了所有满足a ∣ na \\,|\\, na∣n的n，而我们现在需要的是筛选出n mod b≡an\\text{ mod }b\\equiv an mod b≡a的部分，于是我们可以对前面的等比数列做一个变换： ∑k=0a−1λk−bXk(ωa)=(∑k=0a−1λkr−b)∑n=−∞∞x(n)e−jqω+jrωa=a∑q=−∞∞x(aq+b)e−jqω+jbωa=aejbωaDTFT[x(an+b)]\\begin{align*} \\sum\\limits_{k=0}^{a-1}\\lambda_{k}^{-b}X_{k}(\\frac{\\omega}{a}) &amp;= (\\sum\\limits_{k=0}^{a-1}\\lambda_{k}^{r-b})\\sum\\limits_{n=-\\infty}^{\\infty}x(n)e^{-jq\\omega + \\frac{jr\\omega}{a}} \\\\ &amp;= a\\sum\\limits_{q=-\\infty}^{\\infty}x(aq+b)e^{-jq\\omega+\\frac{jb\\omega}{a}} \\\\ &amp;= ae^{\\frac{jb\\omega}{a}}\\mathrm{DTFT}[x(an + b)] \\end{align*} k=0∑a−1​λk−b​Xk​(aω​)​=(k=0∑a−1​λkr−b​)n=−∞∑∞​x(n)e−jqω+ajrω​=aq=−∞∑∞​x(aq+b)e−jqω+ajbω​=aeajbω​DTFT[x(an+b)]​ 即得： DTFT[x(an)]=1a∑k=0a−1X(ω−2kπa)DTFT[x(an+b)]=1ae−jωba∑k=0a−1λk−bX(ω−2kπa)\\begin{align*} \\mathrm{DTFT}[x(an)] &amp;= \\frac{1}{a}\\sum\\limits_{k=0}^{a-1}X(\\frac{\\omega - 2k\\pi}{a} ) \\\\ \\mathrm{DTFT}[x(an + b)] &amp;= \\frac{1}{a}e^{-j\\omega \\frac{b}{a}}\\sum\\limits_{k=0}^{a-1}\\lambda_{k}^{-b}X(\\frac{\\omega - 2k\\pi}{a}) \\\\\\end{align*}DTFT[x(an)]DTFT[x(an+b)]​=a1​k=0∑a−1​X(aω−2kπ​)=a1​e−jωab​k=0∑a−1​λk−b​X(aω−2kπ​)​","tags":["笔记","信原","数学基础"],"categories":["信号处理原理"]},{"title":"DistriFusion & PipeFusion","path":"/2024/10/26/DistriFusion-PipeFusion/","content":"DistriFusion &amp; PipeFusion 相关内容 多卡并行的两种方式 DistriFusion 针对U-Net架构的Diffusion Model进行改进 DistriFusion概括 传统diffusion缺少多卡并行能力，如果直接将image切分的话，缺少通信的patch产生的图片会不合预期，而使用同步通信则会导致通信时间过长，消除了并行的优势 考虑到diffusion中相近step之间输入具有近似的特点，于是做异步通信： DistriFusion详细架构 第ttt步第lll层会有记录此时的完整输入AtlA_{t}^{l}Atl​，在切分到NNN个设备上后，每个设备仅有Atl(i)A_{t}^{l(i)}Atl(i)​这样一部分的输入，此时At+1lA_{t+1}^{l}At+1l​会异步传到这里（通过AllGather），并做一次scatter分配到每台设备上，使得每台设备只需要自己提供1N\\frac{1}{N}N1​的输入即可，其他的输入都复用上一步的 由于Scatter的结果中只有1N\\frac{1}{N}N1​是新的，因此可以修改FlF_{l}Fl​使其支持稀疏操作： 如果FlF_{l}Fl​是卷积或线性层等，则直接将新的一部分作为输入即可 如果FlF_{l}Fl​是Self-Attention，则将其换为Cross-Attention，即使用新图片的QQQ与全图片的KVKVKV作Attention然而这一步通信$KV$可能导致内存开销过大 但是U-Net需要进行GN，而不完整的图片无法计算GN，新算法： 局部GN更新算法 也即在每台设备上用局部信息来更新并估计全局，并分别作GN，之后通过异步通信将每台设备上的信息整合起来，计算精确的全局信息 最后，在实验的时候，并没有一开始就使用DistriFusion，因为初始的一些步骤主要是构建整体布局和全局语义，随着采样的进行，每一步主要变成了复原局部细节。因此采用了预热步骤，也即先使用了若干轮的传统同步通信 PipeFusion 针对DiT进行改进 DistriFusion有一些不足： 应用于DiT时，需要在每台设备之间对于每一层的KVKVKV矩阵均进行通信，导致通信缓存巨大，造成内存浪费 对带宽要求高，需要将每一层的完整中间激活都在timestep之间进行传递 于是提出了新的方法PipeFusion PipeFusion与DistriFusion的对比 PipeFusion也利用了DistriFusion中观察到的现象，并将其命名为输入临时冗余 PipeFusion的架构 图中，Transformer中的LLL层被划分到NNN台设备上，每台设备只负责其中LN\\frac{L}{N}NL​部分的计算，也即每台设备只需要保存自己负责的层所对应的KVKVKV矩阵即可，避免了这一步带来的通信开销和内存开销 其中每一台设备上有一个buffer用来存储激活，当处理前面的patch时，由于一个device会随着流水线的流动逐步处理所有的patch，因此buffer中是存储每一个patch对应的activation，当计算完成一个patch的时候就将其对应的activation更新，使得其相比DistriFusion来说应用了更多的fresh activation 每台设备上都会有一个Activation Buffer DistriFusion中的新GN算法、预热步骤等都可以应用到PipeFusion中，但是有一些修改： DiT不需要GN 将预热步骤分配在其他计算资源上，减少其开销","tags":["DistriFusion","PipeFusion","Multi GPU"],"categories":["科研"]},{"title":"Sequence-P","path":"/2024/10/26/Sequence-P/","content":"Sequence Parallelism 相关 初版 SP架构 初版符号表 环形计算，devices按行切分QKVQKVQKV矩阵，得到Qn,Kn,Vn∈RL/N×AQ^{n}, K^{n}, V^{n} \\in \\mathbb{R}^{L/N\\times A}Qn,Kn,Vn∈RL/N×A，之后执行N−1N-1N−1轮环形通信，计算Sn=concati=1N(Qn(Ki)T)∈RL/N×LS^{n} = \\mathrm{concat}_{i=1}^{N}\\bigl(Q^{n}(K^{i})^{T}\\bigr)\\in \\mathrm{R}^{L/N\\times L}Sn=concati=1N​(Qn(Ki)T)∈RL/N×L，之后再进行N−1N-1N−1轮环形通信，计算attention输出On=SnV=concati=1N(SinVi)O^{n} = S^{n}V = \\mathrm{concat}_{i=1}^{N}\\bigl(S^{n}_{i}V^{i}\\bigr)On=SnV=concati=1N​(Sin​Vi)，其中SinS^{n}_{i}Sin​代表SnS^{n}Sn按列切分为NNN段 性能比较 SP与TP性能比较1 SP与TP性能比较2 因此若SP&gt;TPSP &gt; TPSP&gt;TP则： BL&gt;32HBL &gt; 32HBL&gt;32H BL&gt;16AZBL &gt; 16 AZBL&gt;16AZ 通信比较 二者相同，都是8(N−1)BZ(L/N)A8(N-1)BZ(L/N)A8(N−1)BZ(L/N)A，详细推导省略 对于TP来说的优势 在与PP结合的时候可以减少一次all-gather，这是因为TP会在进入下一阶段的时候对输入也做split，在计算结束之后在all-gather，但是由于SP在初始阶段就会做split，所以就可以失去这个操作 Ulysses Ulysses架构 沿着head cnt做切分，这样一张卡上是若干个完整的头，可以直接计算Attention，并且可以用一些常规的方法加速，例如FlashAttention等 注意在上图中d=hc×hsd = hc\\times hsd=hc×hs，是将多头的直接concat起来得到的向量 Ring-Attention Ring Attention架构 Flash Attention的并行化版本，利用异步P2P通信，在一个设备上计算局部attention的同时传递K-V块","tags":["Sequence Parallelism"],"categories":["科研"]},{"title":"自底向上语法分析","path":"/2024/10/22/自底向上语法分析/","content":"编译原理 笔记 2 自底向上语法分析 自底向上的思想是从要分析的终结符串开始，每一步规约都是将一个字串用一个产生式替换，直到达到开始符号或报错 基本改进方法 与自顶向下相似，直接分析的版本不确定性太高，因此需要进行限制 选择可规约串 对于一个句型，可规约串定义为该句型的短语 对于文法G=(VN,VT,P,S)G = (V_{N}, V_{T}, P, S)G=(VN​,VT​,P,S)，设α,β,δ∈(VN,VT)∗,w∈VT∗\\alpha, \\beta, \\delta\\in(V_{N}, V_{T})^{*}, w\\in V_{T}^{*}α,β,δ∈(VN​,VT​)∗,w∈VT∗​： 若S⇒∗αAδS\\mathop{\\Rightarrow}\\limits^{*}\\alpha A\\deltaS⇒∗αAδ且A⇒+βA\\mathop{\\Rightarrow}\\limits^{+}\\betaA⇒+β，则称β\\betaβ是句型αβδ\\alpha\\beta\\deltaαβδ相对非终结符AAA的短语 若S⇒∗αAδS\\mathop{\\Rightarrow}\\limits^{*}\\alpha A\\deltaS⇒∗αAδ且A⇒βA\\Rightarrow\\betaA⇒β，则称β\\betaβ是句型αβδ\\alpha\\beta\\deltaαβδ相对非终结符AAA的直接短语 若S⇒rm∗αAwS\\mathop{\\Rightarrow}\\limits_{rm}^{*}\\alpha AwSrm⇒∗​αAw且A⇒βA\\Rightarrow\\betaA⇒β，则称β\\betaβ是右句型αβw\\alpha\\beta wαβw相对非终结符AAA的句柄 短语可以理解为可以被规约的子串，直接短语就是可以通过一步就能规约到的子串，而句柄是所有直接短语中最靠左的一个（因为在句柄的定义中要求了最右推导，因此右边的直接短语已经被推导完了） 例如对于文法： S→ABA→aAA→εB→bB→bB\\begin{align*} S &amp;\\to AB \\\\ A &amp;\\to aA \\\\ A &amp;\\to \\varepsilon \\\\ B &amp;\\to b \\\\ B &amp;\\to bB \\end{align*} SAABB​→AB→aA→ε→b→bB​ 则： 句子aaabaaabaaab的短语包括ε,a,aa,aaa,aaab,b\\varepsilon, a, aa, aaa, aaab, bε,a,aa,aaa,aaab,b，直接短语包括ε,b\\varepsilon, bε,b，句柄为ε\\varepsilonε 句型aaAbaaAbaaAb的短语包括aA,aaA,aaAb,baA, aaA, aaAb, baA,aaA,aaAb,b，直接短语包括aA,baA, baA,b，句柄为aAaAaA 如果文法是二义的，则右句型的最右推导可能有多个，则句柄也可能会有多个 最右推导与最左规约 通常采用移进-规约分析 与自顶向下分析比较 功能较强：规约的时候可以获取完整的串信息，而推导只能获得局部信息 利于出错处理 有成熟的自动化技术 移进-规约分析 有一个分析引擎和下推栈，分析引擎能完成的动作包括： Reduce：按照确定的方式对栈顶短语进行规约 Shift：从输入序列移进一个单词到栈顶 Error：发现错误，进行处理或恢复 Accept：分析成功 移进-规约分析例子 上图中的步骤逆向过来就是一种最右推导，称之为规范推导 移进-规约冲突 到达一个既可移进又可规约的状态，例如文法： S→if E then S ∣ if E then S else S\\begin{align*} S &amp;\\to \\text{if } E \\text{ then } S \\,|\\, \\text{if } E \\text{ then } S \\text { else } S \\end{align*} S​→if E then S∣if E then S else S​ 则对于串if E then if E then S else S\\text{if } E \\text{ then if } E \\text{ then } S \\text { else } Sif E then if E then S else S即可移进又可规约 规约-规约冲突 到达了一个状态，可能有多种短语可以用于规约，例如产生式： A→aA ∣ aaAA \\to aA \\,|\\, aaA A→aA∣aaA 则对于aaabaaabaaab时，对于栈中的aaAaaAaaA，无法确定该用哪一个短语 表驱动方法 解决上面两个冲突的方法，即通过一张分析表来决定状态机的下一步行为，常见的表有LR分析中的LR分析表、算符优先分析中的算法优先分析表 LR分析 代表从左（L）到右扫描输入序列，产生最右（R）推导，是一种表驱动的移进-规约分析 LR分析表构造 主要是构造两张表ACTION表和GOTO表： ACTION表：根据栈顶状态和当前输入符号来确定下一步动作 GOTO表：根据栈顶状态和规约用的非终结符决定下一个状态 每一个状态为一个set&lt;pair&gt;，每一个pair代表了在当前状态下，消耗的栈顶符号与去掉这个符号后下一步允许什么串进入，本质上是一个大型状态转移图 LR分析表对应的状态转移图实例 这张表具体的构造方法按照分析方法不同分别讨论 LR分析算法 12345678910111213if (ACTION[i, a] == sj) &#123; // shift PUSH j; // push state j to stack READ w; // read one symbol in the input&#125;else if (ACTION[i, a] == rj) &#123; // reduce // the j-th formula is A -&gt; beta POP len(beta); // pop |beta| states from the tiop of stack PUSH GOTO[k, A]; // k is the current stack top&#125;else if (ACTION[i, a] == acc) &#123; // accept return;&#125;else exit(1); // error 带符号栈的LR分析算法 除了状态栈之外，还有一个符号栈，存储的读入但是没有规约的符号，算法修改为： 12345678910111213if (ACTION[i, a] == sj) &#123; // shift PUSH (j, a); // push state j and symbol a to stack READ w; // read one symbol in the input&#125;else if (ACTION[i, a] == rj) &#123; // reduce // the j-th formula is A -&gt; beta POP len(beta); // pop |beta| states from the tiop of stack PUSH (GOTO[k, A], A); // k is the current stack top&#125;else if (ACTION[i, a] == acc) &#123; // accept return;&#125;else exit(1); // error LR(0)分析 首先引入一些概念 对于文法G=(VN,VT,P,S)G = (V_{N}, V_{T}, P, S)G=(VN​,VT​,P,S)，增加产生式S′→SS&#x27;\\to SS′→S，其中S′∉VN∪VTS&#x27; otin V_{N}\\cup V_{T}S′∈/VN​∪VT​，则得到增广文法G′=(VN,VT,P,S′)G&#x27; = (V_{N}, V_{T}, P, S&#x27;)G′=(VN​,VT​,P,S′) 对于文法G=(VN,VT,P,S)G = (V_{N}, V_{T}, P, S)G=(VN​,VT​,P,S)，以及串α,β∈(VN∪VT)∗,w∈VT∗\\alpha, \\beta\\in(V_{N}\\cup V_{T})^{*}, w\\in V_{T}^{*}α,β∈(VN​∪VT​)∗,w∈VT∗​，其中β\\betaβ为句柄，则αβ\\alpha\\betaαβ任何前缀γ\\gammaγ称为文法GGG的活前缀对于增广文法，原开始符号SSS是G′G&#x27;G′的活前缀 LR(0) FSM LR(0)分析的主要工具，是一个以VN∪VTV_{N}\\cup V_{T}VN​∪VT​为字母表的DFA，可以通过增广后的文法直接构造 可以证明，GGG的LR(0)FSM对应的语言是G′G&#x27;G′所有活前缀的集合 FSM的状态 LR(0) FSM的状态是一个特殊的LR(0)项集，一个LR(0)项是指在右端某一位置有圆点的产生式，圆点代表着已分析过的串与该产生式匹配的位置 项有如下分类： 移进项：A→α.aβA\\to \\alpha .a\\betaA→α.aβ 待约项：A→α.BβA\\to \\alpha .B\\betaA→α.Bβ 规约项：A→α.A\\to \\alpha .A→α. 接收项：S′→S.S&#x27;\\to S .S′→S. 我们将LR(0) FSM的每一项定义为一个项集的闭包(CLOSURE) 123456789def CLOSURE(I): J = I while (1): for (j, p) in zip(J, P): if j.hasform(&quot;A -&gt; α.Bβ&quot;) and p.hasform(&quot;B -&gt; γ&quot;): J.append(&quot;B -&gt; .γ&quot;) if J.hasNotChanged(): break return J 则其状态转移函数定义为： GO(I,X)=CLOSURE(J)J={A→αX.β ∣ A→α.Xβ∈I}\\begin{align*} GO(I, X) &amp;= \\text{CLOSURE}(J) \\\\ J &amp;= \\{A\\to \\alpha X.\\beta \\,|\\, A\\to \\alpha.X\\beta \\in I \\} \\end{align*} GO(I,X)J​=CLOSURE(J)={A→αX.β∣A→α.Xβ∈I}​ 则构造状态集合的方法就是从{CLOSURE({S′→.S})}\\{\\text{CLOSURE}(\\{S&#x27;\\to .S\\})\\}{CLOSURE({S′→.S})}开始，利用上述的状态转移函数逐步扩展状态集合 LR(0)FSM的构造举例 增广文法的每个活前缀都有唯一与之相对应的状态 LR(0)分析表的构造 令状态集合为C={I0,…,In}C = \\{I_{0} ,\\dots,I_{n}\\}C={I0​,…,In​}，则构造ACTION与GOTO表的方法为： GOTO(k,A)=j⇔GO(Ik,A)=Ij\\mathrm{GOTO}(k, A) = j \\Leftrightarrow \\mathrm{GO}(I_{k}, A) = I_{j}GOTO(k,A)=j⇔GO(Ik​,A)=Ij​ ACTION(k,a)=sj⇔A→α.aβ∈Ik &amp; GO(Ik,a)=Ij\\mathrm{ACTION}(k, a) = sj \\Leftrightarrow A\\to\\alpha.a\\beta \\in I_{k} \\,\\&amp;\\, \\mathrm{GO}(I_{k}, a) = I_{j}ACTION(k,a)=sj⇔A→α.aβ∈Ik​&amp;GO(Ik​,a)=Ij​ ACTION(k,a)=rj⇔A→α.∈Ik\\mathrm{ACTION}(k, a) = rj \\Leftrightarrow A\\to\\alpha.\\in I_{k}ACTION(k,a)=rj⇔A→α.∈Ik​ ACTION(k,#)=acc⇔S′→S.∈Ik\\mathrm{ACTION}(k, \\#) = acc \\Leftrightarrow S&#x27;\\to S. \\in I_{k}ACTION(k,#)=acc⇔S′→S.∈Ik​ LR(0)文法 按照上述定义构造的分析表，如果各表项均没有多重定义，则称为LR(0)文法，等价于要求每个状态满足： 不同时含有移进和规约项（不能有移进-规约冲突） 不含有两个以上规约项（不能有规约-规约冲突） 一个非LR(0)文法的例子为： E→E+T ∣ TT→T∗F ∣ FF→(E) ∣ v ∣ d\\begin{align*} E &amp;\\to E + T \\,|\\, T \\\\ T &amp;\\to T * F \\,|\\, F \\\\ F &amp;\\to (E) \\,|\\, v \\,|\\, d \\end{align*} ETF​→E+T∣T→T∗F∣F→(E)∣v∣d​ 短语TTT即存在移进-规约冲突 SLR(1)分析 由于LR(0)限制非常严格（要求看到栈顶状态就能决定动作），因此做一些条件的放松，常见的是允许向前查看一个符号成为LR(1)，首先来研究其简化版本SLR(1) SLR(1)通过判断下一个输入符号是否属于要规约的非终结符的Follow集合，来解决移进-规约冲突和规约-规约冲突 如果一个状态中规约项的非终结符的Follow集两两无交，则可以解决规约-规约冲突 如果一个状态中所有规约项中要规约的非终结符的Follow集与所有要移进的符号集互不相交，则可以解决移进-规约冲突 SLR(1)分析表 其分析表的构造相对于LR(0)来说只用做简单的修改，也即在填充ACTION(k,a)=rj\\mathrm{ACTION}(k, a) = rjACTION(k,a)=rj时，需要保证a∈Follow(A)a\\in \\mathrm{Follow}(A)a∈Follow(A) 按照上述定义构造的分析表，如果各表项均没有多重定义，则称为SLR(1)文法，等价于要求每个状态满足： 对于任意两项A→α.A\\to \\alpha.A→α.与B→β.B\\to \\beta.B→β.，有Follow(A)∩Follow(B)=∅\\mathrm{Follow}(A) \\cap \\mathrm{Follow}(B) = \\emptysetFollow(A)∩Follow(B)=∅ 对于任意两项A→α.aβA\\to \\alpha.a\\betaA→α.aβ与B→γ.B\\to \\gamma.B→γ.，均有a∉Follow(B)a otin \\mathrm{Follow}(B)a∈/Follow(B) 一个非SLR(1)文法的例子为： E→(L,E) ∣ FL→L,E ∣ EF→(F) ∣ d\\begin{align*} E &amp;\\to (L,E) \\,|\\, F \\\\ L &amp;\\to L,E \\,|\\, E \\\\ F &amp;\\to (F) \\,|\\, d \\end{align*} ELF​→(L,E)∣F→L,E∣E→(F)∣d​ 短语(F)(F)(F)会被错误的规约为(E)(E)(E) LR(1)分析 在SLR(1)的基础上进一步改进，将传统的项增加一个终结符，表示产生式右端完整匹配后所允许留在符号串中的下一个终结符，即项会形如： A→α.β , aA\\to \\alpha.\\beta\\,,\\,a A→α.β,a 其中a∈VT∪{#}a\\in V_{T}\\cup\\{\\#\\}a∈VT​∪{#} 对于形如A→α. , aA\\to \\alpha.\\,,\\,aA→α.,a的想，只有当下一个输入为aaa的时候才能进行规约 LR(1)FSM LR(1)FSM的状态是LR(1)项集的闭包，闭包算法为： 123456789def CLOSURE(I): J = I while (1): for (j, p) in zip(J, P): if j.hasform(&quot;[A -&gt; α.Bβ, a]&quot;) and p.hasform(&quot;B -&gt; γ&quot;): J.append(&quot;[B -&gt; .γ, b]&quot;) for b in First(βa) if J.hasNotChanged(): break return J 初态定义为I0=CLOSURE({[S′→.S,#]})I_{0} = \\mathrm{CLOSURE}(\\{[S&#x27;\\to .S, \\#]\\})I0​=CLOSURE({[S′→.S,#]}) 状态转移函数定义为： GO(I,X)=CLOSURE(J)J={[A→αX.β , a] ∣ [A→α.Xβ , a]∈I}\\begin{align*} GO(I, X) &amp;= \\text{CLOSURE}(J) \\\\ J &amp;= \\{[A\\to \\alpha X.\\beta\\,,\\, a] \\,|\\, [A\\to \\alpha.X\\beta \\,,\\, a] \\in I \\} \\end{align*} GO(I,X)J​=CLOSURE(J)={[A→αX.β,a]∣[A→α.Xβ,a]∈I}​ 从初态开始利用上述转移函数扩展，可以得到项集规范族 在上述非SLR(1)的例子中，E→F.E\\to F.E→F.所期待的下一个输入符号没有)))，因此(F)(F)(F)中的FFF不会被规约为EEE，而是会选择移进 LR(1) 分析表构造 GOTO(k,A)=j⇔GO(Ik,A)=Ij\\mathrm{GOTO}(k, A) = j \\Leftrightarrow \\mathrm{GO}(I_{k}, A) = I_{j}GOTO(k,A)=j⇔GO(Ik​,A)=Ij​ ACTION(k,a)=sj⇔[A→α.aβ , b]∈Ik &amp; GO(Ik,a)=Ij\\mathrm{ACTION}(k, a) = sj \\Leftrightarrow [A\\to\\alpha.a\\beta\\,,\\,b] \\in I_{k} \\,\\&amp;\\, \\mathrm{GO}(I_{k}, a) = I_{j}ACTION(k,a)=sj⇔[A→α.aβ,b]∈Ik​&amp;GO(Ik​,a)=Ij​ ACTION(k,b)=rj⇔[A→α. , b]∈Ik\\mathrm{ACTION}(k, b) = rj \\Leftrightarrow [A\\to\\alpha.\\,,\\, b]\\in I_{k}ACTION(k,b)=rj⇔[A→α.,b]∈Ik​ ACTION(k,#)=acc⇔[S′→S. , #]∈Ik\\mathrm{ACTION}(k, \\#) = acc \\Leftrightarrow [S&#x27;\\to S.\\,,\\, \\#] \\in I_{k}ACTION(k,#)=acc⇔[S′→S.,#]∈Ik​ LR(1)文法 按上述算法构造的分析表若每项都无重复定义，则称为LR(1)文法，其FSM每一个状态都满足： 对于任意两个项目[A→α.aβ , b][A\\to \\alpha.a\\beta\\,,\\, b][A→α.aβ,b]与[B→γ.,c][B\\to \\gamma., c][B→γ.,c]，一定有a≠ca eq ca=c 对于任意两个项目[A→α. , a][A\\to \\alpha.\\,,\\, a][A→α.,a]与[B→β , b][B\\to \\beta\\,,\\, b][B→β,b]，一定有a≠ba eq ba=b LR(k)文法 可以扩展到LR(k)文法，但是状态机的复杂度过高，无实际意义，并且可以证明LR(N+)\\mathrm{LR}(\\mathbb{N}^{+})LR(N+)都是等价的 LALR(1)分析 是一个LR(1)与SLR(1)的折中版本 将LR(1)的项的第一部分称为芯，如果LR(1)FSM的两个状态的芯构成的集合完全相同，那么我们称其为同芯的状态 将LR(1)FSM中的同芯状态合并，如果得到的新状态没有规约-规约冲突，则得到的新文法是LALR(1)文法 LALR(1)FSM 构造方法为brute-force方法： 构造增广文法的LR(1)FSM状态集 合并同芯状态 LALR(1)FSM状态的后继状态是其合并前所有同芯状态的后继状态的并 LALR(1)FSM状态数与LR(0)状态数相同，但是其能力强于SLR(1)","tags":["笔记","编原","语法分析","自底向上"],"categories":["编译原理"]},{"title":"信号处理原理 4","path":"/2024/10/22/信号处理原理4/","content":"信号处理原理 笔记 4 信号的采样 采样：每隔一段时间（等距）在模拟信号波形上抽取一个幅度值，称之为采样 抽样的时间间隔称为采样周期TsT_{s}Ts​，倒数称为采样频率fs=1/Tsf_{s} = 1/T_{s}fs​=1/Ts​，采样角频率（不会混淆的情况下也可称为采样频率）为ωs=2π/Ts\\omega_{s} = 2\\pi / T_{s}ωs​=2π/Ts​ 采样的数学模型 在时域： xp(t)=x(t)p(t)x_{p}(t) = x(t)p(t) xp​(t)=x(t)p(t) 在频域： Xp(ω)=12πX(ω)∗P(ω)X_{p}(\\omega) = \\frac{1}{2\\pi}X(\\omega)*P(\\omega) Xp​(ω)=2π1​X(ω)∗P(ω) 其中p(t)p(t)p(t)为我们的采样公式，理想采样为冲激串采样： p(t)=∑n=−∞∞δ(t−nTs)p(t) = \\sum\\limits_{n=-\\infty}^{\\infty}\\delta(t-nT_{s}) p(t)=n=−∞∑∞​δ(t−nTs​) 这种情况下： xp(t)=∑n=−∞∞x(nTs)δ(t−nTs)x_{p}(t) = \\sum\\limits_{n=-\\infty}^{\\infty}x(nT_{s})\\delta(t-nT_{s}) xp​(t)=n=−∞∑∞​x(nTs​)δ(t−nTs​) 此时p(t)p(t)p(t)的FT为： p(t)⇔P(ω)=2πTs∑n=−∞∞δ(ω−nωs)p(t)\\Leftrightarrow P(\\omega) = \\frac{2\\pi}{T_{s}}\\sum\\limits_{n=-\\infty}^{\\infty}\\delta(\\omega-n\\omega_{s}) p(t)⇔P(ω)=Ts​2π​n=−∞∑∞​δ(ω−nωs​) 于是我们有： Xp(ω)=1Ts∑n=−∞∞X(ω−kωs)X_{p}(\\omega) = \\frac{1}{T_{s}}\\sum\\limits_{n=-\\infty}^{\\infty}X(\\omega - k\\omega_{s}) Xp​(ω)=Ts​1​n=−∞∑∞​X(ω−kωs​) 也即在时域上对时间信号进行理想采样相当于在频域上对连续时间信号的频谱以ωs\\boldsymbol{\\omega_{s}}ωs​为周期做周期延拓 我们来证明上述公式中最重要的一步：p(t)⇔P(ω)=2πTs∑n=−∞∞δ(ω−nωs)p(t)\\Leftrightarrow P(\\omega) = \\frac{2\\pi}{T_{s}}\\sum\\limits_{n=-\\infty}^{\\infty}\\delta(\\omega-n\\omega_{s})p(t)⇔P(ω)=Ts​2π​n=−∞∑∞​δ(ω−nωs​)首先直接套用定义：P(ω)=F[p(t)]=∫−∞∞p(t)e−jωtdt=∑n=−∞∞e−jωnT\\begin{align*} P(\\omega) = \\mathscr{F}[p(t)] &amp;= \\int_{-\\infty}^{\\infty}p(t)e^{-j\\omega t}dt \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}e^{-j\\omega nT}\\end{align*}P(ω)=F[p(t)]​=∫−∞∞​p(t)e−jωtdt=n=−∞∑∞​e−jωnT​这个式子并不友好，而且在后续卷积的过程中也没法进行化简，于是我们采用另外一种方法，首先对p(t)p(t)p(t)进行FS，系数：Fn=1Ts∫−Ts/2Ts/2p(t)e−jnωstdt=1Ts∑k=−∞∞∫−Ts/2Ts/2δ(t−kTs)e−jnωstdt=1Ts∫−Ts/2Ts/2δ(t)e−jnωstdt=1Ts\\begin{align*} F_{n} &amp;= \\frac{1}{T_{s}}\\int_{-T_{s}/2}^{T_{s}/2}p(t)e^{-jn\\omega_{s}t}dt \\\\ &amp;= \\frac{1}{T_{s}}\\sum\\limits_{k=-\\infty}^{\\infty}\\int_{-T_{s}/2}^{T_{s}/2}\\delta(t-kT_{s})e^{-jn\\omega_{s}t}dt \\\\ &amp;= \\frac{1}{T_{s}}\\int_{-T_{s}/2}^{T_{s}/2}\\delta(t)e^{-jn\\omega_{s}t}dt \\\\ &amp;= \\frac{1}{T_{s}}\\end{align*}Fn​​=Ts​1​∫−Ts​/2Ts​/2​p(t)e−jnωs​tdt=Ts​1​k=−∞∑∞​∫−Ts​/2Ts​/2​δ(t−kTs​)e−jnωs​tdt=Ts​1​∫−Ts​/2Ts​/2​δ(t)e−jnωs​tdt=Ts​1​​因此有：p(t)=1Ts∑n=−∞∞ejnωstp(t) = \\frac{1}{T_{s}}\\sum\\limits_{n=-\\infty}^{\\infty}e^{jn\\omega_{s}t}p(t)=Ts​1​n=−∞∑∞​ejnωs​t此时我们再利用FT的线性可得：P(ω)=F[p(t)]=1Ts∑n=−∞∞F[ejnωst]=2πTs∑n=−∞∞δ(ω−nωs)\\begin{align*} P(\\omega) = \\mathscr{F}[p(t)] &amp;= \\frac{1}{T_{s}}\\sum\\limits_{n=-\\infty}^{\\infty}\\mathscr{F}[e^{jn\\omega_{s}t}] \\\\ &amp;= \\frac{2\\pi}{T_{s}}\\sum\\limits_{n=-\\infty}^{\\infty}\\delta(\\omega - n\\omega_{s})\\end{align*}P(ω)=F[p(t)]​=Ts​1​n=−∞∑∞​F[ejnωs​t]=Ts​2π​n=−∞∑∞​δ(ω−nωs​)​ 采样示意图 时钟域上的采样代表频率域上的周期延拓 当上图中，ωm&gt;ωs\\omega_{m} &gt; \\omega_{s}ωm​&gt;ωs​时会发生混叠，也即延拓之后会发生相互重叠的现象，因此我们要想从Xp(jω)X_{p}(j\\omega)Xp​(jω)中不失真地分离出XjωX_{j\\omega}Xjω​，则需要满足： x(t)x(t)x(t)是带限的，最高频率分量为ωM\\omega_{M}ωM​ 采样周期不能是任意的，必须保证采样频率ωs≥2ωM\\omega_{s} \\geq 2\\omega_{M}ωs​≥2ωM​ 此时可以用理想低通滤波器从Xp(jω)X_{p}(j\\omega)Xp​(jω)中不失真地分理处X(jω)X(j\\omega)X(jω) 我们有Nyquist采样定理： 对带限于最高频率ωM\\omega_{M}ωM​对连续时间信号x(t)x(t)x(t)，如果以ωs&gt;2ωM\\omega_{s} &gt; 2\\omega_{M}ωs​&gt;2ωM​的频率进行理想采样，则x(t)x(t)x(t)可以唯一由其样本x(nT)x(nT)x(nT)来确定 而在实际应用中，由于理想滤波器是不可实现的，因此要求ωs&gt;2ωM\\omega_{s} &gt; 2\\omega_{M}ωs​&gt;2ωM​ 内插 内插是由样本值重建某一函数的过程 理想内插 以理想低通滤波器（频域矩形脉冲）的单位冲激响应（定义为函数的IFT）作为内插函数 设内插函数为h(t)=F−1[Gωs(ω)]=1TsSa(πtTs)h(t) = \\mathscr{F}^{-1}[G_{\\omega_{s}}(\\omega)] = \\dfrac{1}{T_{s}}\\mathrm{Sa}(\\dfrac{\\pi t}{T_{s}})h(t)=F−1[Gωs​​(ω)]=Ts​1​Sa(Ts​πt​) 则有： x(t)=xp(t)∗h(t)=∑n=−∞∞x(nT)δ(t−nT)∗h(t)=∑n=−∞∞x(nT)h(t−nT)\\begin{align*} x(t) = x_{p}(t) * h(t) &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(nT)\\delta(t-nT)*h(t) \\\\ &amp;= \\sum\\limits_{n=-\\infty}^{\\infty}x(nT)h(t-nT) \\end{align*} x(t)=xp​(t)∗h(t)​=n=−∞∑∞​x(nT)δ(t−nT)∗h(t)=n=−∞∑∞​x(nT)h(t−nT)​ 也即可以通过采样值xp(t)x_{p}(t)xp​(t)和Sa函数来恢复原有信号，并且恢复效果是很理想的 时域为Sa函数，频域为矩形脉冲，也即单位冲激响应为其IFT 理想内插函数的单位冲激响应 零阶保持内插 内插函数为矩形脉冲，对应的频域为Sa函数 所得的结果是采样后的信号在x轴上向正方向扩展，到达下一个采样信号的时候直接跳变 零阶保持内插 一阶保持内插 内插函数为脉高为1的三角脉冲 所得到的结果是通过一次函数连接相邻的采样点 一阶保持内插 欠采样信号的恢复 当不满足采样定理的时候，就会出现频谱混叠的情况，此时有一些结论： 频谱混叠的情况下，时域信号变了，但是抽样点处取值不变，也即通过理想内插也得不到原信号，但抽样点处的取值不变 发生频谱混叠的时候理想内插也得不到正确值 工程应用时，如果采样频率ωs=2ωM\\omega_{s} = 2\\omega_{M}ωs​=2ωM​将不足以恢复原信号，例如x(t)=cos⁡(ω0t+φ)x(t) = \\cos(\\omega_{0}t+\\varphi)x(t)=cos(ω0​t+φ)在ωs=2ω0\\omega_{s} = 2\\omega_{0}ωs​=2ω0​的时候可能被恢复成x1(t)=cos⁡φcos⁡(ω0t)x_{1}(t) = \\cos\\varphi\\cos(\\omega_{0}t)x1​(t)=cosφcos(ω0​t) 频域采样 在频域利用P(ω)=∑k=−∞∞δ(ω−kω0)P(\\omega) = \\sum\\limits_{k=-\\infty}^{\\infty}\\delta(\\omega - k\\omega_{0})P(ω)=k=−∞∑∞​δ(ω−kω0​)进行采样得到Xp(ω)X_{p}(\\omega)Xp​(ω) 在频域采样的结果 可以看出，在频域以ω0\\omega_{0}ω0​采样相当于在时域将信号以2πω0\\boldsymbol{\\frac{2\\pi}{\\omega_{0}}}ω0​2π​为周期无限延拓 如果想在时域截取原信号，只需要利用矩形窗信号进行读取 w(t)={ω0∣t∣≤πω00∣t∣&gt;πω0\\begin{align*} w(t) = \\begin{cases} \\omega_{0} &amp; |t| \\leq \\frac{\\pi}{\\omega_{0}} \\\\ 0 &amp; |t| &gt; \\frac{\\pi}{\\omega_{0}} \\end{cases} \\end{align*} w(t)={ω0​0​∣t∣≤ω0​π​∣t∣&gt;ω0​π​​​ 而在频域截取原信号需要内插，内插函数为W(jω)=2πsinc(ω/ω0)W(j\\omega) = 2\\pi\\mathrm{sinc}(\\omega/\\omega_{0})W(jω)=2πsinc(ω/ω0​) 即： X(jω)=12πXp(jω)∗W(jω)=∑k=−∞∞X(kω0)sinc(ω−kω0ω0)\\begin{align*} X(j\\omega) &amp;= \\frac{1}{2\\pi}X_{p}(j\\omega)*W(j\\omega) \\\\ &amp;= \\sum\\limits_{k=-\\infty}^{\\infty}X(k\\omega_{0})\\mathrm{sinc}(\\frac{\\omega-k\\omega_{0}}{\\omega_{0}}) \\end{align*} X(jω)​=2π1​Xp​(jω)∗W(jω)=k=−∞∑∞​X(kω0​)sinc(ω0​ω−kω0​​)​","tags":["笔记","信原","数学基础"],"categories":["信号处理原理"]},{"title":"Swin Transformer","path":"/2024/10/20/Swin-T/","content":"Swin Transformer 一种基于移位窗口的层次化Transformer 基本架构 基本架构 首先将输入的RGB图片划分为不重叠的patch（图中大小为443），之后将每一个patch扁平化位移位向量，这样初始化图像转化成了H4×W4×48\\frac{H}{4}\\times \\frac{W}{4}\\times 484H​×4W​×48，之后将patch进一个MLP将其维度变为CCC 之后进行patch的合并，每次将2*2单位内的patch进行合并，之后进一个MLP保证patch的大小只扩张两倍 这种架构可以方便的替换其他一些网络中的backbone 而Swin Transformer块是将传统的Multi-head Self Attention替换成了(Shift-)Window MSA，每一Stage中的层数可能不同，由超参数决定 (S)W-MSA 用于解决常规Attention平方复杂度的问题，将图片划分为不重叠的窗口，窗口的大小为M×MM\\times MM×M，而其中MMM为常数，这样可以将计算的复杂度从O(N2)O(N^{2})O(N2)降到O(N)O(N)O(N)，但是这样的话失去了窗口之间的联系，可能导致信息的丢失，于是引入窗口的移位，即SW-MSA 基础版移动窗口 第l个模块是W-MSA的话，第l+1个就会是SW-MSA，会将窗口向左上循环移动(⌊M2⌋,⌊M2⌋)(\\lfloor\\frac{M}{2}\\rfloor, \\lfloor\\frac{M}{2}\\rfloor)(⌊2M​⌋,⌊2M​⌋)位，确保了跨窗口信息可以被保留下来 但是如果按照Fig2中的移动方式，窗口数会从(⌈hM⌉,⌈wM⌉)(\\lceil\\frac{h}{M}\\rceil, \\lceil\\frac{w}{M}\\rceil)(⌈Mh​⌉,⌈Mw​⌉)变成(⌈hM⌉+1,⌈wM⌉+1)(\\lceil\\frac{h}{M}\\rceil + 1, \\lceil\\frac{w}{M}\\rceil + 1)(⌈Mh​⌉+1,⌈Mw​⌉+1)，因此采用下面这种方式： 移动窗口 这种方式不会增加窗口数量，但是每一个窗口可能是由许多子窗口拼接而成的，因此引入masked限制attention的计算范围，也即给每一个窗口一个index，在attention的过程中只留下相同id窗口的计算结果，忽略其他值，这一步需要依靠根据当前窗口的排布来确定mask 在计算attention的过程中，引入相对偏移矩阵BBB： 相对位置偏移 其中QKV∈RM2×dQKV\\in\\mathbb{R}^{M^{2}\\times d}QKV∈RM2×d，因此B∈RM2×M2B\\in\\mathbb{R}^{M^{2}\\times M^{2}}B∈RM2×M2，其是从B^∈R(2M−1)2\\hat{B}\\in\\mathbb{R}^{(2M - 1)^{2}}B^∈R(2M−1)2中取值得到的 BBB的每一行分别代表了以窗口的第iii个patch为原点时，其他patch针对原点的相对偏移量，并通过偏移数组B^\\hat{B}B^来确定偏移量，详细计算过程参考博客 原版参数设置 相对位置偏移","tags":["Transformer","Shift Windows"],"categories":["科研"]},{"title":"DiT","path":"/2024/10/20/DiT/","content":"Diffusion Transformer相关 LDM简介 LDM架构 架构如上，ε\\mathcal{\\varepsilon}ε代表编码器，负责将像素空间的输入映射到潜在空间，D\\mathcal{D}D代表解码器，负责将潜在空间的输出映射到像素空间，中间利用U-Net进行去噪，同时τθ\\tau_\\thetaτθ​是Condition编码后的参数，通过CrossAttention与潜在变量结合起来 DiT 基于LDM的一个模型，将其中原本用于Denoise的U-Net修改为Transfomer架构，称为DiT Block，而VAE则是由已有的部分： DiT Block Patchify 对输入的latent noise做扁平化： 将图像切分成多个大小为 p×pp\\times pp×p 的补丁，然后将其转换为长度为 TTT 的序列作为 Transformer 的输入。这使得 DiT 能够处理不同分辨率、持续时间和长宽比的视频和图像。 其中ppp的大小会影响GFLOP，ppp减半GFLOP乘4，但是对下游参数数量没有影响。 DiT Block 按照对条件信息的处理方式分为三种： In-Context Conditioning 直接把Condition和Image拼接在一起，最终一次迭代不拼接，对GFLOP影响很小 Cross Attention Image做一次Self-Attention之后，再和Condition做一次Cross Attention，会提升15%左右的GFLOP，这个和LDM中使用的方法是很相似的 Adaptive Layer Norm(adaLN) 传统的LayerNorm，在做完归一化之后的线性映射参数是直接训练得到，与输入无关，和adaLN中线性映射的采纳数是输入经过一次MLP部分得到 用Adaptive Layer Norm替换Transfomer中的Layer Norm部分，并且这其中的参数与Image无关得到，而通过Condition经过MLP之后得到，对GFLOP影响最小 adaLN-Zero 先把α\\alphaα输出都初始化为0，这样每一个残差块的初始输出都为恒等映射，训练成本更低 Decoder 用一个线性decoder来做解码，输出一个噪声预测与一个对角协方差预测","tags":["Transformer","Diffusion"],"categories":["科研"]},{"title":"科研内容记录","path":"/2024/10/14/科研内容记录/","content":"记录科研过程中学习的一些内容 Self-Attention &amp; Transformer 李宏毅老师的课程： https://www.bilibili.com/video/BV1Wv411h7kN/https://www.bilibili.com/video/BV1Wv411h7kN/ 一些博客： https://nlp.seas.harvard.edu/annotated-transformer/https://nlp.seas.harvard.edu/annotated-transformer/ https://blog.csdn.net/weixin_42475060/article/details/121101749https://blog.csdn.net/weixin_42475060/article/details/121101749 模型参数量分析https://zhuanlan.zhihu.com/p/624740065 Four kinds of parallelism data-parallelismhttps://juejin.cn/post/7254001262646738981 pipeline-parallelismhttps://juejin.cn/post/7262274383287484476 tensor-parallelismhttps://juejin.cn/post/7269698032655728640 sequence-parallelismhttps://juejin.cn/post/7273680143658287156 Flash Attention 原论文，按照版本升序 https://arxiv.org/pdf/2205.14135https://arxiv.org/pdf/2205.14135 https://arxiv.org/pdf/2307.08691https://arxiv.org/pdf/2307.08691 https://arxiv.org/pdf/2407.08608https://arxiv.org/pdf/2407.08608 博客： https://blog.csdn.net/v_JULY_v/article/details/133619540https://blog.csdn.net/v_JULY_v/article/details/133619540 Sequence Parallelism 一些论文： Sequence Parallelismhttps://arxiv.org/pdf/2105.13120 DeepSpeed-Ulysseshttps://arxiv.org/pdf/2309.14509 Ring-Attentionhttps://arxiv.org/pdf/2310.01889 Unified-Sequence-Parallelismhttps://arxiv.org/pdf/2405.07719 博客： https://zhuanlan.zhihu.com/p/689067888https://zhuanlan.zhihu.com/p/689067888 https://zhuanlan.zhihu.com/p/683714620https://zhuanlan.zhihu.com/p/683714620 Distributed System 通信原语Pytorchhttps://zhuanlan.zhihu.com/p/478953028 通信原语MMEngine库https://mmengine.readthedocs.io/zh-cn/v0.7.2/advanced_tutorials/distributed.html Swin Transformer 论文： Swin Transformerhttps://arxiv.org/pdf/2103.14030 博客： Swin Transformerhttps://zhuanlan.zhihu.com/p/367111046 Relative Positionhttps://blog.csdn.net/weixin_40723264/article/details/127632545 Diffusion 论文： DDPMhttps://arxiv.org/pdf/2006.11239 Diffusion Mathhttps://arxiv.org/pdf/2208.11970 Diffusion Transformerhttps://arxiv.org/pdf/2212.09748 Diffusion Overviewhttps://arxiv.org/pdf/2404.07771 博客： Diffusion Modelhttps://zhuanlan.zhihu.com/p/624221952 Diffusion Transformerhttps://zhuanlan.zhihu.com/p/684125968 Sora技术报告： Sorahttps://openai.com/index/video-generation-models-as-world-simulators/ DiTFastAttn 论文： DiT Fast Attentionhttps://arxiv.org/pdf/2406.08552 xFusion 论文： DistriFusionhttps://arxiv.org/pdf/2402.19481 PipeFusionhttps://arxiv.org/pdf/2405.14430 一些问题 不同的Normalization BN是在一个batch内，针对每一个特征做归一化；LN是指对于每个样本做归一化；IN是指对于通道内的每个特征做归一化；GN是指对通道内的一组特征做归一化 对于一个y = [batch_size, channel_num, width, height] = [N, C, W, H]的张量： BN对于批次中的每一个通道作归一化，即：μc=1NHW∑n=1N∑h=1H∑w=1Wynchwσc2=1NHW∑n=1N∑h=1H∑w=1W(ynchw−μc)2ynchw′=γ(ynchw−μcσc)+β\\begin{align*} \\mu_{c} &amp;= \\frac{1}{NHW}\\sum\\limits_{n=1}^{N}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}y_{nchw} \\\\ \\sigma_{c}^{2} &amp;= \\frac{1}{NHW}\\sum\\limits_{n=1}^{N}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}(y_{nchw} - \\mu_{c})^{2} \\\\ y&#x27;_{nchw} &amp;= \\gamma(\\frac{y_{nchw} - \\mu_{c}}{\\sigma_{c}}) + \\beta \\end{align*} μc​σc2​ynchw′​​=NHW1​n=1∑N​h=1∑H​w=1∑W​ynchw​=NHW1​n=1∑N​h=1∑H​w=1∑W​(ynchw​−μc​)2=γ(σc​ynchw​−μc​​)+β​ LN为对批次中每一个样本做归一化：μn=1CHW∑c=1C∑h=1H∑w=1Wynchwσn2=1CHW∑c=1C∑h=1H∑w=1W(ynchw−μn)2ynchw′=γ(ynchw−μnσn)+β\\begin{align*} \\mu_{n} &amp;= \\frac{1}{CHW}\\sum\\limits_{c=1}^{C}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}y_{nchw} \\\\ \\sigma_{n}^{2} &amp;= \\frac{1}{CHW}\\sum\\limits_{c=1}^{C}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}(y_{nchw} - \\mu_{n})^{2} \\\\ y&#x27;_{nchw} &amp;= \\gamma(\\frac{y_{nchw} - \\mu_{n}}{\\sigma_{n}}) + \\beta \\end{align*} μn​σn2​ynchw′​​=CHW1​c=1∑C​h=1∑H​w=1∑W​ynchw​=CHW1​c=1∑C​h=1∑H​w=1∑W​(ynchw​−μn​)2=γ(σn​ynchw​−μn​​)+β​ IN为对样本的特征做归一化：μnc=1HW∑h=1H∑w=1Wynchwσnc2=1HW∑h=1H∑w=1W(ynchw−μnc)2ynchw′=γ(ynchw−μncσnc)+β\\begin{align*} \\mu_{nc} &amp;= \\frac{1}{HW}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}y_{nchw} \\\\ \\sigma_{nc}^{2} &amp;= \\frac{1}{HW}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}(y_{nchw} - \\mu_{nc})^{2} \\\\ y&#x27;_{nchw} &amp;= \\gamma(\\frac{y_{nchw} - \\mu_{nc}}{\\sigma_{nc}}) + \\beta \\end{align*} μnc​σnc2​ynchw′​​=HW1​h=1∑H​w=1∑W​ynchw​=HW1​h=1∑H​w=1∑W​(ynchw​−μnc​)2=γ(σnc​ynchw​−μnc​​)+β​ GN是对样本的一组特征做归一化： 首先将[N, C, H, W]划分为[N, G, S, H, W]，之后：μng=1SHW∑s=1S∑h=1H∑w=1Wyngshwσng2=1SHW∑s=1S∑h=1H∑w=1W(yngshw−μng)2yngshw′=γ(yngshw−μngσng)+β\\begin{align*} \\mu_{ng} &amp;= \\frac{1}{SHW}\\sum\\limits_{s=1}^{S}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}y_{ngshw} \\\\ \\sigma_{ng}^{2} &amp;= \\frac{1}{SHW}\\sum\\limits_{s=1}^{S}\\sum\\limits_{h=1}^{H}\\sum\\limits_{w=1}^{W}(y_{ngshw} - \\mu_{ng})^{2} \\\\ y&#x27;_{ngshw} &amp;= \\gamma(\\frac{y_{ngshw} - \\mu_{ng}}{\\sigma_{ng}}) + \\beta \\end{align*} μng​σng2​yngshw′​​=SHW1​s=1∑S​h=1∑H​w=1∑W​yngshw​=SHW1​s=1∑S​h=1∑H​w=1∑W​(yngshw​−μng​)2=γ(σng​yngshw​−μng​​)+β​ 当G=1G = 1G=1的时候，GN退化为LN；当G=CG = CG=C的时候，GN退化为IN 一般来说，LN更适用于数据量小、批大小更小的时候","tags":["Attention","Transformer","Speed up","Diffusion"],"categories":["科研"]},{"title":"自顶向下语法分析","path":"/2024/10/12/自顶向下语法分析/","content":"编译原理 笔记 1 自顶向下语法分析 自顶向下的核心思想是从文法的开始符号出发，每一步推导得到一个句型，最终产生一个句子即为期待的终结符串 带回溯的自顶向下分析 由于每一步推导的终结符串和使用的文法都是不确定的，因此复杂度很高，只能进行不断回溯，因此我们进行改进：每步推导总是用最左边的非终结符，产生最左推导，如果想进一步确认使用的文法，则需改进为： 确定的自顶向下分析 在从左向右扫描的过程中，向前查看常数个的单词，以确定每一步使用的文法 例如对于文法GGG： S→ABA→aA∣ϵB→b∣bB\\begin{align*} S &amp;\\to AB \\\\ A &amp;\\to aA | \\epsilon \\\\ B &amp;\\to b | bB \\end{align*} SAB​→AB→aA∣ϵ→b∣bB​ 则在自顶向下分析anbm(n≥0,m&gt;0)a^{n}b^{m}(n\\geq 0, m &gt; 0)anbm(n≥0,m&gt;0)时，只需要向前查看2个单词，即可预测每步所应该使用的文法 如果想要实现这种分析，需要保证文法不含左递归与左公因子，否则： 考虑文法GGG： S→Sa∣b\\begin{align*} S &amp;\\to Sa | b \\end{align*} S​→Sa∣b​ 则在分析banba^{n}ban时需要向前看的单词数为n+2n + 2n+2个 考虑文法GGG： S→aAb∣aAcA→a∣aA\\begin{align*} S &amp;\\to aAb | aAc \\\\ A &amp;\\to a | aA \\end{align*} SA​→aAb∣aAc→a∣aA​ 则在分析an(b+c)a^{n}(b + c)an(b+c)时需要向前看的单词数为n+2n + 2n+2个 都不为常数，无法确定地分析 LL(1) 分析 最常用的预测分析方法，要求文法是LL(1)文法 从左向右扫描单词 每步产生最左推导 向前看一个单词 重要的集合 First集合 First集合定义为： 设G=(VT,VN,P,S)G = (V_{T}, V_{N}, P, S)G=(VT​,VN​,P,S)是上下文无关文法，则对α∈(VT∪VN)∗\\alpha \\in (V_{T}\\cup V_{N})^{*}α∈(VT​∪VN​)∗：First(α)={a ∣ α⇒∗aβ, a∈VT,β∈(VT∪VN)∗ or a=β=ϵ}\\mathrm{First}(\\alpha) = \\{a\\,|\\, \\alpha \\Rightarrow^{*} a\\beta,\\, a\\in V_{T}, \\beta\\in(V_{T}\\cup V_{N})^{*}\\text{ or } a = \\beta = \\epsilon\\}First(α)={a∣α⇒∗aβ,a∈VT​,β∈(VT​∪VN​)∗ or a=β=ϵ} 即任意句型或句子的First是指这个句型或句子能推导出的串中首个单词的集合 First(α)\\mathrm{First}(\\alpha)First(α)计算过程为： 先置所有First(α)=∅\\mathrm{First}(\\alpha) = \\varnothingFirst(α)=∅ 若α∈VT∪{ϵ}\\alpha \\in V_{T}\\cup \\{\\epsilon\\}α∈VT​∪{ϵ}，则First(α)={X}\\mathrm{First}(\\alpha) = \\{X\\}First(α)={X} 若α=X1X2…Xk∈(VT∪VN)∗\\alpha = X_{1}X_{2}\\dots X_{k} \\in (V_{T} \\cup V_{N})^{*}α=X1​X2​…Xk​∈(VT​∪VN​)∗，则先置First(α)=First(X1)\\mathrm{First}(\\alpha) = \\mathrm{First}(X_{1})First(α)=First(X1​) 遍历XiX_{i}Xi​，若X1…Xi⇒∗ϵX_{1}\\dots X_{i} \\Rightarrow^{*} \\epsilonX1​…Xi​⇒∗ϵ 则First(α)∪=First(Xi+1)\\mathrm{First}(\\alpha) \\cup= \\mathrm{First}(X_{i + 1})First(α)∪=First(Xi+1​)，其中Xk+1=ϵX_{k + 1} = \\epsilonXk+1​=ϵ 若α∈VN\\alpha \\in V_{N}α∈VN​，且α→X1X2…Xk\\alpha \\to X_{1}X_{2}\\dots X_{k}α→X1​X2​…Xk​， 则First(α)∪=First(X1X2…Xk)\\mathrm{First}(\\alpha) \\cup= \\mathrm{First}(X_{1}X_{2}\\dots X_{k})First(α)∪=First(X1​X2​…Xk​) Follow集合 Follow集合定义为： 设G=(VT,VN,P,S)G = (V_{T}, V_{N}, P, S)G=(VT​,VN​,P,S)是上下文无关文法，则对A∈VNA\\in V_{N}A∈VN​：Follow(A)={a ∣ S#⇒∗αAβ#, a∈First(β#),α,β∈(VT∪VN)∗}\\mathrm{Follow}(A) = \\{a\\,|\\, S\\# \\Rightarrow^{*} \\alpha A\\beta\\#,\\, a\\in \\mathrm{First}(\\beta\\#), \\alpha, \\beta\\in(V_{T}\\cup V_{N})^{*}\\}Follow(A)={a∣S#⇒∗αAβ#,a∈First(β#),α,β∈(VT​∪VN​)∗} 也即Follow集合被定义为可能在某些举行中紧跟在AAA右边的终结符集合 Follow(A)\\mathrm{Follow}(A)Follow(A)的计算方法为： 置Follow(S)={#}\\mathrm{Follow}(S) = \\{\\#\\}Follow(S)={#}，其他均为∅\\varnothing∅ 循环直到所有集合不变： 对于A→αBβA\\to \\alpha B \\betaA→αBβ，Follow(B)∪=First(β)−{ϵ}\\mathrm{Follow}(B) \\cup= \\mathrm{First}(\\beta) - \\{\\epsilon\\}Follow(B)∪=First(β)−{ϵ} 若ϵ∈First(B)\\epsilon \\in \\mathrm{First}(B)ϵ∈First(B)，则Follow(β)∪=Follow(A)\\mathrm{Follow}(\\beta) \\cup= \\mathrm{Follow}(A)Follow(β)∪=Follow(A) 预测集合 预测集合的定义为： 设G=(VT,VN,P,S)G = (V_{T}, V_{N}, P, S)G=(VT​,VN​,P,S)是上下文无关文法，则对A→α∈PA\\to \\alpha \\in PA→α∈P：PS(A→α)={First(α)ϵ∉First(α)(First(α)−{ϵ})∪Follow(A)ϵ∈First(α)\\mathrm{PS}(A\\to \\alpha) = \\begin{cases} \\mathrm{First}(\\alpha) &amp; \\epsilon otin \\mathrm{First}(\\alpha) \\\\ (\\mathrm{First}(\\alpha) - \\{\\epsilon\\}) \\cup \\mathrm{Follow}(A) &amp; \\epsilon \\in \\mathrm{First}(\\alpha)\\end{cases}PS(A→α)={First(α)(First(α)−{ϵ})∪Follow(A)​ϵ∈/First(α)ϵ∈First(α)​ 预测集合给出了读入了什么字符的时候需要采用产生式A→αA\\to \\alphaA→α LL(1)文法 文法GGG是LL(1)文法当且仅当对于GGG的每个非终结符AAA的任何两个不同产生式A→α∣βA\\to \\alpha | \\betaA→α∣β，满足：PS(A→α)∩PS(A→β)=∅\\mathrm{PS}(A\\to\\alpha) \\cap \\mathrm{PS}(A\\to\\beta) = \\varnothingPS(A→α)∩PS(A→β)=∅ 递归下降LL(1)分析程序 每个非终结符对应一个子程序，每个子程序的行为根据语法描述来明确： 根据当前非终结符的PS集合与下一个输入符号选择产生式 如果产生式右端遇到非终结符，则调用相应的子程序 如果产生式右端遇到终结符，判断当前读入的单词是否与该终结符相匹配，匹配则继续读取，反之报错 递归下降分析 实际应用中，可以将产生式的右端添加新运算，使之更加简洁，例如将S→XSS→ϵS\\to XS\\quad S \\to \\epsilonS→XSS→ϵ替换为S→{X}S\\to \\{X\\}S→{X}等，具体来说： {X}=X∗\\{X\\} = X^{*}{X}=X∗ [X]=X ∣ ϵ[X] = X \\,|\\, \\epsilon[X]=X∣ϵ (X)(X)(X)代表XXX优先 表驱动LL(1)分析程序 由PS集合形成一个预测分析表，即根据非终结符和下一个单词决定产生式的表，并利用该表和一个下推栈实现： 将#\\##入栈 若栈顶为终结符，则判断读入的单词和终结符是否匹配，匹配则出栈并继续读取，反之报错 若栈顶为非终结符，则查表找到产生式，若为None则报错，反之非终结符出栈，产生式右端从右到左依次入栈，无需继续读入 直到栈顶和下一位输入都是#\\## 可以证明，预测分析表的每一项都只包含一个产生式，当且仅当文法是LL(1)的 文法变换 主要包含消除左递归与提取左公因子两种，通常用于将一些文法转换成LL(1)文法 消除左递归 消除直接左递归 对P→Pα ∣ βP \\to P\\alpha\\,|\\,\\betaP→Pα∣β，α≠ϵ\\alpha eq\\epsilonα=ϵ且β\\betaβ首字符不是PPP，则消除方法为引入新终结符QQQ使得： P→βQQ→αQ ∣ ϵ\\begin{align*} P &amp;\\to \\beta Q \\\\ Q &amp;\\to \\alpha Q \\,|\\, \\epsilon \\end{align*} PQ​→βQ→αQ∣ϵ​ 对于一般形式P→Pα1 ∣ … ∣ Pαm ∣ β1 ∣ … ∣ βnP \\to P\\alpha_{1}\\,|\\,\\dots\\,|\\,P\\alpha_{m}\\,|\\,\\beta_{1}\\,|\\,\\dots\\,|\\,\\beta_{n}P→Pα1​∣…∣Pαm​∣β1​∣…∣βn​，则： P→β1Q ∣ … ∣ βmQQ→α1Q ∣ … ∣ αnQ ∣ ϵ\\begin{align*} P &amp;\\to \\beta_{1}Q\\,|\\,\\dots\\,|\\, \\beta_{m}Q \\\\ Q &amp;\\to \\alpha_{1}Q \\,|\\,\\dots\\,|\\,\\alpha_{n}Q\\,|\\, \\epsilon \\end{align*} PQ​→β1​Q∣…∣βm​Q→α1​Q∣…∣αn​Q∣ϵ​ 消除一般左递归 对于无环无ϵ\\epsilonϵ产生式的文法，消除一般左递归的方法为： 排列非终结符A1,A2,…,AnA_{1}, A_{2}, \\dots, A_{n}A1​,A2​,…,An​ for i in 1..n: for j in 1..(i-1): 对于形如Ai→AjrA_{i} \\to A_{j}rAi​→Aj​r的规则 其中AjA_{j}Aj​的全部产生式为Aj→α1 ∣ α2 ∣ … ∣ αkA_{j} \\to \\alpha_{1}\\,|\\,\\alpha_{2}\\,|\\,\\dots\\,|\\,\\alpha_{k}Aj​→α1​∣α2​∣…∣αk​ 将AiA_{i}Ai​产生式替换为Ai→α1r ∣ α2r ∣ … ∣ αkrA_{i} \\to \\alpha_{1}r\\,|\\,\\alpha_{2}r\\,|\\,\\dots\\,|\\,\\alpha_{k}rAi​→α1​r∣α2​r∣…∣αk​r 再消除AiA_{i}Ai​的直接左递归 化简文法 提取左公因子 对于形如P→αβ ∣ αγP\\to \\alpha\\beta \\,|\\, \\alpha\\gammaP→αβ∣αγ的产生式，增加新终结符使得： P→αQQ→β ∣ γ\\begin{align*} P &amp;\\to \\alpha Q \\\\ Q &amp;\\to \\beta \\,|\\, \\gamma \\end{align*} PQ​→αQ→β∣γ​ 一般化为P→αβ1 ∣ … ∣ αβm ∣ γ1 ∣ … ∣ γnP\\to \\alpha\\beta_{1}\\,|\\,\\dots\\,|\\,\\alpha\\beta_{m}\\,|\\,\\gamma_{1}\\,|\\,\\dots\\,|\\,\\gamma_{n}P→αβ1​∣…∣αβm​∣γ1​∣…∣γn​： P→αQ ∣ γ1 ∣ … ∣ γnQ→β1 ∣ … ∣ βm\\begin{align*} P &amp;\\to \\alpha Q\\,|\\,\\gamma_{1}\\,|\\,\\dots\\,|\\,\\gamma_{n} \\\\ Q &amp;\\to \\beta_{1} \\,|\\,\\dots\\,|\\, \\beta_{m} \\end{align*} PQ​→αQ∣γ1​∣…∣γn​→β1​∣…∣βm​​ 预测分析中的出错处理 处理原则： 尽可能准确地给出错误位置与属性 尽可能校正 表驱动LL(1)分析中的错误处理 对于栈顶终结符与输入不匹配，直接弹出终结符 对于栈顶非终结符与输入符号在表中找不到产生式，我们采用恐慌模式，即跳过一些符号以找到同步符号 同步符号集合的构建为： Follow(A)\\mathrm{Follow}(A)Follow(A)中的所有符号都是AAA的同步符号 将First(B)\\mathrm{First}(B)First(B)中的符号加入AAA同步符号，代表AAA遇到错误的时候可以从BBB开始继续分析 递归下降分析的错误处理 当递归进入某个语法单位的时候，检查当前符号是否属于该单位的开始符号，离开该语法单位的时候检查符号是否属于该单位的结束符号 若不属于则不断滤去直到到达补救集合，也即开始符号与结束符号的并集，中的符号并重新判断 递归下降分析错误处理 LL(k)的结论 LL(k)文法的定义是LL(1)的推广，有关的结论有： 给定k&gt;0k&gt;0k&gt;0，一个CFG是否为LL(k)是可判定的 给定CFG，是否存在kkk使得该文法是LL(k)是不可判定的 给定CFG，是否存在与之等价的LL(k)是不可判定的 两个LL(k)是否相等时可判定的 LL(k)无二义 LL(k)中不存在左递归 给定k&gt;0k&gt;0k&gt;0，不含ϵ\\epsilonϵ产生式的LL(k)的集合真包含于不含ϵ\\epsilonϵ产生式的LL(k+1)的集合","tags":["笔记","编原","自顶向下","语法分析"],"categories":["编译原理"]},{"title":"计算机组成原理2","path":"/2024/10/10/计算机组成原理2/","content":"计算机组成原理 笔记 2 #TODO","tags":["笔记","计组"],"categories":["计算机组成原理"]},{"title":"信号处理原理 3","path":"/2024/10/08/信号处理原理3/","content":"信号处理原理 笔记 3 信号的分解 信号的分解方法 直流与交流分解 fDC(t)=lim⁡T→∞∫−T/2T/2f(t)dtfAC(t)=f(t)−fDC(t)\\begin{align*} f_{DC}(t) &amp;= \\lim\\limits_{T\\to\\infty}\\int_{-T/2}^{T/2}f(t)dt \\\\ f_{AC}(t) &amp;= f(t) - f_{DC}(t) \\end{align*} fDC​(t)fAC​(t)​=T→∞lim​∫−T/2T/2​f(t)dt=f(t)−fDC​(t)​ 奇偶分解 fe(t)=f(t)+f(−t)2fo(t)=f(t)−f(−t)2\\begin{align*} f_{e}(t) &amp;= \\frac{f(t) + f(-t)}{2} \\\\ f_{o}(t) &amp;= \\frac{f(t) - f(-t)}{2} \\end{align*} fe​(t)fo​(t)​=2f(t)+f(−t)​=2f(t)−f(−t)​​ 复分解 Re(f(t))=f(t)+f(t)‾2Im(f(t))=f(t)−f(t)‾2j\\begin{align*} \\mathrm{Re}(f(t)) &amp;= \\frac{f(t) + \\overline{f(t)}}{2} \\\\ \\mathrm{Im}(f(t)) &amp;= \\frac{f(t) - \\overline{f(t)}}{2j} \\end{align*} Re(f(t))Im(f(t))​=2f(t)+f(t)​​=2jf(t)−f(t)​​​ 脉冲分解 TODO 信号的正交分解 当f(t)f(t)f(t)在[t1,t2][t_{1}, t_{2}][t1​,t2​]区间内具有连续一阶导数和逐段连续的二阶导数时，f(t)f(t)f(t)可以用完备的正交函数集{φ)i(t)}\\{\\varphi){i}(t)\\}{φ)i(t)}来表示，即： f(t)=∑i=1∞ciφi(t)f(t) = \\sum\\limits_{i=1}^{\\infty}c_{i}\\varphi_{i}(t) f(t)=i=1∑∞​ci​φi​(t) 定义ki=⟨φi(t),φi(t)⟩k_{i} = \\langle\\varphi_{i}(t), \\varphi_{i}(t)\\rangleki​=⟨φi​(t),φi​(t)⟩ 则常数cic_{i}ci​的定义为： ci=1ki⟨f(t),φi(t)⟩ c_{i} = \\frac{1}{k_{i}}\\langle f(t), \\varphi_{i}(t)\\rangle ci​=ki​1​⟨f(t),φi​(t)⟩ 我们有帕斯瓦尔定理： ∫t1t2∣∣f(t)∣∣2dt=∑i=1∞∣∣ci∣∣2ki\\int_{t_{1}}^{t_{2}}||f(t)||^{2}dt = \\sum\\limits_{i=1}^{\\infty}||c_{i}||^{2}k_{i} ∫t1​t2​​∣∣f(t)∣∣2dt=i=1∑∞​∣∣ci​∣∣2ki​ 周期信号的正交分解 满足Dirchlet条件的周期函数都可以在一组完备正交基函数上展开为无穷级数 Dirchlet条件为： 间断点个数有限 极值点个数有限 绝对积分数值有限 当完备正交基函数为三角函数集或指数函数集的时候，展成的级数称为Fourier级数 Fourier级数 三角Fourier 设f(t)f(t)f(t)周期为T1T_{1}T1​，令ω1=2π/T1\\omega_{1} = 2\\pi/T_{1}ω1​=2π/T1​，则Fourier级数为： f(t)=a0+∑n=1∞(ancos⁡nω1t+bnsin⁡nω1t)f(t) = a_{0} + \\sum\\limits_{n=1}^{\\infty}(a_{n}\\cos n\\omega_{1}t + b_{n}\\sin n\\omega_{1}t) f(t)=a0​+n=1∑∞​(an​cosnω1​t+bn​sinnω1​t) 积分变换为： a0=1T1∫t0t0+T1f(t)dtan=2T1∫t0t0+T1f(t)cos⁡(nω1t)dtbn=2T1∫t0t0+T1f(t)sin⁡(nω1t)dt\\begin{align*} a_{0} &amp;= \\frac{1}{T_{1}}\\int_{t_{0}}^{t_{0} + T_{1}}f(t)dt \\\\ a_{n} &amp;= \\frac{2}{T_{1}}\\int_{t_{0}}^{t_{0} + T_{1}}f(t)\\cos(n\\omega_{1}t)dt \\\\ b_{n} &amp;= \\frac{2}{T_{1}}\\int_{t_{0}}^{t_{0} + T_{1}}f(t)\\sin(n\\omega_{1}t)dt \\end{align*} a0​an​bn​​=T1​1​∫t0​t0​+T1​​f(t)dt=T1​2​∫t0​t0​+T1​​f(t)cos(nω1​t)dt=T1​2​∫t0​t0​+T1​​f(t)sin(nω1​t)dt​ 复指数Fourier 对三角Fouier利用欧拉函数进行转换，可以得到复指数形式的Fourier级数： f(t)=a0+∑n=1∞[an−jbn2ejnω1t+an+jbn2e−jnω1t]f(t) = a_{0} + \\sum\\limits_{n=1}^{\\infty}\\bigl[ \\frac{a_{n}-jb_{n}}{2}e^{jn\\omega_{1}t} + \\frac{a_{n}+jb_{n}}{2}e^{-jn\\omega_{1}t} \\bigr] f(t)=a0​+n=1∑∞​[2an​−jbn​​ejnω1​t+2an​+jbn​​e−jnω1​t] 定义： Fn={a0n=0F(nω1)=an−jbn2n∈Z/{0}F_{n} = \\begin{cases} a_{0} &amp; n = 0\\\\ F(n\\omega_{1}) = \\dfrac{a_{n} - jb_{n}}{2} &amp; n \\in \\mathbb{Z}/\\{0\\} \\end{cases} Fn​=⎩⎨⎧​a0​F(nω1​)=2an​−jbn​​​n=0n∈Z/{0}​ 则有： f(t)=∑n=−∞∞Fnejnω1tf(t) = \\sum\\limits_{n = -\\infty}^{\\infty}F_{n}e^{jn\\omega_{1}t} f(t)=n=−∞∑∞​Fn​ejnω1​t 其中： Fn=1T1∫T1f(t)e−jnω1tdtF_{n} = \\frac{1}{T_{1}}\\int_{T_{1}}f(t)e^{-jn\\omega_{1}t}dt Fn​=T1​1​∫T1​​f(t)e−jnω1​tdt Fouier频谱 考虑Fouier复系数{Fn}\\{F_{n}\\}{Fn​}，则为了表示这个复系数序列可以得到两张频谱： 幅度谱 ∣Fn∣|F_{n}|∣Fn​∣ 相位谱 Arg(Fn)\\mathrm{Arg}(F_{n})Arg(Fn​) 周期信号的Fouier频谱特点为： 仅在离散点{nω1}\\{n\\omega_{1}\\}{nω1​}处有值，为谐波 FnF_{n}Fn​是双边谱，也即正负频率的频率幅度相加才是实际幅度 信号的功率为∑n=−∞∞∣Fn∣2\\sum\\limits_{n=-\\infty}^{\\infty}|F_{n}|^{2}n=−∞∑∞​∣Fn​∣2 周期矩形脉冲信号 脉宽为τ\\tauτ，幅度为EEE，周期为T1T_{1}T1​ 周期矩形脉冲信号的FS 包络线为EτT1Sa(ωτ2)\\dfrac{E\\tau}{T_{1}}Sa(\\dfrac{\\omega\\tau}{2})T1​Eτ​Sa(2ωτ​) 可以看出，周期信号的能量主要集中在第一个零点以内，即∣ω∣≤2πτ|\\omega| \\leq \\dfrac{2\\pi}{\\tau}∣ω∣≤τ2π​内，因此这段频率范围被称为矩形信号的频带宽度，在允许失真的情况下可以只用这一段进行通信 信号的正交分解 信号的级数展开 用一组函数φi(t)\\varphi_{i}(t)φi​(t)将信号x(t)∈L2(R)x(t)\\in L^{2}(R)x(t)∈L2(R)展开成级数： x(t)=∑i=−∞∞ciφi(t)x(t) = \\sum\\limits_{i=-\\infty}^{\\infty}c_{i}\\varphi_{i}(t) x(t)=i=−∞∑∞​ci​φi​(t) 求出cic_{i}ci​的过程称为信号变换 正交变换 若基函数φi(t)\\varphi_{i}(t)φi​(t)为标准完备正交基，则积分变换为： ci=∫t1t2x(t)φi(t)‾dtc_{i} = \\int_{t_{1}}^{t_{2}}x(t)\\overline{\\varphi_{i}(t)}dt ci​=∫t1​t2​​x(t)φi​(t)​dt 称为x(t)x(t)x(t)的正交变换，亦称为Karhunen-Loeve变换 非周期信号的Fouier变换 非周期信号可以看成T→∞T\\to\\inftyT→∞的周期信号，于是其频谱会变化为连续频谱，也即ω1→0Fn→0\\omega_{1}\\to 0\\quad F_{n}\\to 0ω1​→0Fn​→0 我们将非周期信号的FT定义为： F(ω)=∫Rf(t)e−jωtdtF(\\omega) = \\int_{\\mathbb{R}}f(t)e^{-j\\omega t}dt F(ω)=∫R​f(t)e−jωtdt 逆Fourier变换定义为IFT: f(t)=12π∫RF(ω)ejωtdωf(t) = \\frac{1}{2\\pi}\\int_{\\mathbb{R}}F(\\omega)e^{j\\omega t}d\\omega f(t)=2π1​∫R​F(ω)ejωtdω 上式可写成，F(ω)=∣F(ω)∣ejφ(ω)F(\\omega) = |F(\\omega)|e^{j\\varphi(\\omega)}F(ω)=∣F(ω)∣ejφ(ω)，其中∣F(ω)∣|F(\\omega)|∣F(ω)∣为幅度频谱密度函数，φ(ω)\\varphi(\\omega)φ(ω)为相位频谱密度函数 FT性质 唯一性：FT与IFT可以分别确定唯一的函数 可逆性： F[f(t)]=F(ω)⇔F−1[F(ω)]=f(t)\\mathscr{F}[f(t)] = F(\\omega) \\Leftrightarrow \\mathscr{F}^{-1}[F(\\omega)] = f(t)F[f(t)]=F(ω)⇔F−1[F(ω)]=f(t) FT与FS的关系 将非周期信号f(t)f(t)f(t)做周期延拓，即时移并叠加，可得到一个周期信号f~(t)\\tilde{f}(t)f~​(t)，令其周期为T1T_{1}T1​，则我们有： Fn=1T1∫−T1/2T1/2f~(t)e−jnω1tdt=1T1∫Rf(t)e−jnω1tdtF(ω)=∫Rf(t)e−jωtdt\\begin{align*} F_{n} &amp;= \\frac{1}{T_{1}}\\int_{-T_{1}/2}^{T_{1}/2}\\tilde{f}(t)e^{-jn\\omega_{1}t}dt \\\\ &amp;= \\frac{1}{T_{1}}\\int_{\\mathbb{R}}f(t)e^{-jn\\omega_{1}t}dt \\\\ F(\\omega) &amp;= \\int_{\\mathbb{R}}f(t)e^{-j\\omega t}dt \\end{align*} Fn​F(ω)​=T1​1​∫−T1​/2T1​/2​f~​(t)e−jnω1​tdt=T1​1​∫R​f(t)e−jnω1​tdt=∫R​f(t)e−jωtdt​ 于是可以得到： Fn=1T1F(nω1)F_{n} = \\frac{1}{T_{1}}F(n\\omega_{1})Fn​=T1​1​F(nω1​) 这代表着我们可以从波形图上较为简单的在周期信号的FS与非周期信号的FT之间进行计算 周期信号至非周期信号：连接包络线，之后整体扩展T1T_{1}T1​倍 非周期信号至周期信号：离散化，只取nω1n\\omega_{1}nω1​处的点，并缩小1T1\\frac{1}{T_{1}}T1​1​ 典型信号的FT 矩形脉冲信号 信号为： f(t)=EGτ(t)f(t) = EG_{\\tau}(t) f(t)=EGτ​(t) 其FT为： F(ω)=Eτ⋅Sa(τ2ω)F(\\omega) = E\\tau\\cdot Sa(\\frac{\\tau}{2}\\omega) F(ω)=Eτ⋅Sa(2τ​ω) 冲激信号 F[Eδ(t)]=∫REδ(t)e−jωtdt=E\\mathscr{F}[E\\delta(t)] = \\int_{\\mathbb{R}}E\\delta(t)e^{-j\\omega t}dt = E F[Eδ(t)]=∫R​Eδ(t)e−jωtdt=E 也即频谱为常数，被称为白色谱 三角信号 F[cos⁡ω0t]=F[ejω0t+e−jω0t2]=πδ(ω−ω0)+πδ(ω+ω0)\\begin{align*} \\mathscr{F}[\\cos \\omega_{0}t] &amp;= \\mathscr{F}[\\frac{e^{j\\omega_{0}t} + e^{-j\\omega_{0}t}}{2}] \\\\ &amp;= \\pi\\delta(\\omega - \\omega_{0}) + \\pi\\delta(\\omega + \\omega_{0}) \\end{align*} F[cosω0​t]​=F[2ejω0​t+e−jω0​t​]=πδ(ω−ω0​)+πδ(ω+ω0​)​ 一些性质 常数的频谱 F[ejω0t]=2πδ(ω−ω0)F[12π]=δ(ω)\\begin{align*} \\mathscr{F}[e^{j\\omega_{0}t}] &amp;= 2\\pi\\delta(\\omega - \\omega_{0}) \\\\ \\mathscr{F}[\\frac{1}{2\\pi}] &amp;= \\delta(\\omega) \\end{align*} F[ejω0​t]F[2π1​]​=2πδ(ω−ω0​)=δ(ω)​ 证明的关键点为： F−1(δ(ω))=12π∫Rδ(ω)ejωtdω=12π\\mathscr{F}^{-1}(\\delta(\\omega)) = \\frac{1}{2\\pi}\\int_{\\mathbb{R}}\\delta(\\omega)e^{j\\omega t}d\\omega = \\frac{1}{2\\pi} F−1(δ(ω))=2π1​∫R​δ(ω)ejωtdω=2π1​ 线性 F\\mathscr{F}F是线性运算 反褶与共轭 F(f(−t))=F(−ω)F(f∗(t))=F∗(−ω)F(f∗(−t))=F∗(ω)\\begin{align*} \\mathscr{F}(f(-t)) &amp;= F(-\\omega) \\\\ \\mathscr{F}(f^{*}(t)) &amp;= F^{*}(-\\omega) \\\\ \\mathscr{F}(f^{*}(-t)) &amp;= F^{*}(\\omega) \\end{align*} F(f(−t))F(f∗(t))F(f∗(−t))​=F(−ω)=F∗(−ω)=F∗(ω)​ 对偶性 F−1[F(ω)]=12πFω∗[F∗(ω)]\\mathscr{F}^{-1}[F(\\omega)] = \\frac{1}{2\\pi}\\mathscr{F}_{\\omega}^{*}[F^{*}(\\omega)] F−1[F(ω)]=2π1​Fω∗​[F∗(ω)] 而FT和IFT的对偶性可以表示为： F(t)⇔2πf(−ω)F(t) \\Leftrightarrow 2\\pi f(-\\omega) F(t)⇔2πf(−ω) 尺度变换特性 F[f(at)]=1∣a∣F(ωa)\\mathscr{F}[f(at)] = \\frac{1}{|a|}F(\\frac{\\omega}{a}) F[f(at)]=∣a∣1​F(aω​) 等效性 对任意信号f(t)⇔F(ω)f(t)\\Leftrightarrow F(\\omega)f(t)⇔F(ω)，设f(0)f(0)f(0)与F(0)F(0)F(0)分别为最大值，则可定义： 等效脉宽τ=F(0)/f(0)\\tau = F(0) / f(0)τ=F(0)/f(0) 等效带宽Bf=f(0)/F(0)B_{f} = f(0) / F(0)Bf​=f(0)/F(0) 波形运算特性 F[f(t−t0)]=F(ω)e−jωt0F[f(at−t0)]=1∣a∣F(ωa)e−jωt0aF[f(t)ejω0t]=F(ω−ω0)F[1∣a∣f(ta)ejω0at]=F(aω−ω0)\\begin{align*} \\mathscr{F}[f(t-t_{0})] &amp;= F(\\omega)e^{-j\\omega t_{0}} \\\\ \\mathscr{F}[f(at-t_{0})] &amp;= \\frac{1}{|a|}F(\\frac{\\omega}{a})e^{-j\\omega \\frac{t_{0}}{a}} \\\\ \\mathscr{F}[f(t)e^{j\\omega_{0}t}] &amp;= F(\\omega - \\omega_{0}) \\\\ \\mathscr{F}[\\frac{1}{|a|}f(\\frac{t}{a})e^{j\\frac{\\omega_{0}}{a}t}] &amp;= F(a\\omega - \\omega_{0}) \\end{align*} F[f(t−t0​)]F[f(at−t0​)]F[f(t)ejω0​t]F[∣a∣1​f(at​)ejaω0​​t]​=F(ω)e−jωt0​=∣a∣1​F(aω​)e−jωat0​​=F(ω−ω0​)=F(aω−ω0​)​ 微积分特性 df(t)dt⇔jωF(ω)dF(ω)sω⇔−jtf(t)∫−∞tf(τ)dτ⇔(jω)−1F(ω)+πF(0)δ(ω)∫−∞ωF(λ)dλ⇔πf(0)δ(t)−(jt)−1f(t)\\begin{align*} \\frac{df(t)}{dt} &amp;\\Leftrightarrow j\\omega F(\\omega) \\\\ \\frac{dF(\\omega)}{s\\omega} &amp;\\Leftrightarrow -jtf(t) \\\\ \\int_{-\\infty}^{t}f(\\tau)d\\tau &amp;\\Leftrightarrow (j\\omega)^{-1}F(\\omega) + \\pi F(0)\\delta(\\omega) \\\\ \\int_{-\\infty}^{\\omega}F(\\lambda)d\\lambda &amp;\\Leftrightarrow \\pi f(0)\\delta(t) - (jt)^{-1}f(t) \\end{align*} dtdf(t)​sωdF(ω)​∫−∞t​f(τ)dτ∫−∞ω​F(λ)dλ​⇔jωF(ω)⇔−jtf(t)⇔(jω)−1F(ω)+πF(0)δ(ω)⇔πf(0)δ(t)−(jt)−1f(t)​ 卷积特性 F[f1(t)∗f2(t)]=F[f1(t)]⋅F[f2(t)]F[f1(t)⋅f2(t)]=12πF[f1(t)]∗F[f2(t)]\\begin{align*} \\mathscr{F}[f_{1}(t)*f_{2}(t)] &amp;= \\mathscr{F}[f_{1}(t)] \\cdot \\mathscr{F}[f_{2}(t)] \\\\ \\mathscr{F}[f_{1}(t)\\cdot f_{2}(t)] &amp;= \\frac{1}{2\\pi} \\mathscr{F}[f_{1}(t)] * \\mathscr{F}[f_{2}(t)] \\\\ \\end{align*} F[f1​(t)∗f2​(t)]F[f1​(t)⋅f2​(t)]​=F[f1​(t)]⋅F[f2​(t)]=2π1​F[f1​(t)]∗F[f2​(t)]​","tags":["笔记","信原","数学基础"],"categories":["信号处理原理"]},{"title":"计算机组成原理1","path":"/2024/10/07/计算机组成原理1/","content":"计算机组成原理 笔记 1 计算机组成原理 指令与数据 计算机指令系统 指令是计算机运行的最小的功能单元，计算机的指令系统是该计算机提供的全部指令 指令的功能与分类 算数与逻辑运算指令 加、减、乘、除等 与、或、非、异或等 移位操作指令 算数右移（补符号位）、逻辑右移、逻辑左移、循环左移 数据传送指令 寄存器之间 寄存器与内存间 不同内存之间 输入输出 计算器与外设之间 跳转指令 无条件与条件 调用与返回指令 堆栈操作指令 其它 指令表示 指令 = 操作码 + 操作数或操作数地址（寄存器、内存或外设） 即对哪些数据做什么操作 操作码可以是定长或变长，定长操作码的译码速度更快，变长操作码可以表示的指令更多 指令是定长的，和机器有关 操作码的扩展 显然，操作码的个数会限制指令系统的长度，因此可以对操作码进行扩展，从而支持更多样化的指令 扩展的方式为：充分利用空闲的位置，例如32位机器上，如果有2个8位地址段作为操作数，则有至多16位可以用于表示操作码，而如果只有1个8为地址段作为操作数，则有至多24位可以用于表示操作码 寻址方式 由于操作数可能需要在某一个地址中进行读取，因此需要指定指令系统的寻址方式： 立即数寻址：操作数直接给出，无需寻址 直接寻址：给出内存中的地址 寄存器寻址：给出操作数所存放在的寄存器编号，则addr = (base) 变址寻址：给出寄存器编号和立即数，则addr = (base) + offset 相对寻址：给出地址相对程序计数器PC的偏移 间接寻址：通过上述的某种方式，给出指向操作数的指针的地址 基址寻址：在计算机中设置一个寄存器用于存放固定的基址，指令中给出偏移量 堆栈寻址：利用sp进行寻址（变址寻址的一种） Riscv 指令 Riscv为定长操作码，如下图，从上到下依次为：寄存器型、立即数型、存储型、分支指令、跳转指令、大立即数 RISCV指令格式 由于指令长度为32位，因此U型指令是操作一些长立即数，例如向x1中写入0x12345678等 寄存器 RISCV中的寄存器 算数、位运算、移位指令 用后缀指明操作数的类型，无u代表有符号数，有u代表无符号数，i代表第二个操作数是立即数没有`subi` op dst, src1, src2: R型 opi dst, src, imm: I型 opu dst, src1, src2: R型 操作有： add sub mul div rem neg and or xor not sll srl sra lui reg, imm: U型，加载立即数到指定寄存器的高20位 其中，mul的结果是64位，但是dst指向的是32位，因此乘法指令有如下变体： mul rdl, rs1, rs2 &lt;==&gt; rdl = (rs1 * rs2)[31:0] mulh rdh, rs1, rs2 &lt;==&gt; rdh = (rs1 * rs2)[63:32] mulhu rdh, rs1, rs2 &lt;==&gt; rdh = (unsigned(rs1) * (unsigned)rs2)[63:32] mulhsu rdh, rs1, rs2 &lt;==&gt; rdh = (rs1 * (unsigned)rs2)[63:32] 访存指令 Riscv中有专属的Load/Store指令用于操作内存，其他指令只能操作寄存器，采用变址寻址的方式 l/s reg, offset(base): Load为I型，Store为S型 允许以字节为基本单位进行读写： 一字节：lb, sb 双字节：lh, sh 四字节：lw, sw 注： Riscv是小端 lb lh会做符号扩展，而lbu lhu会做0扩展 分支与跳转 比较指令有： slt rd, rs1, rs2: rd = 1 if rs1 &lt; rs2 else 0: R型 sgt rd, rs1, rs2: rd = 1 if rs1 &gt; rs2 else 0: R型 同理还有snez, seqz与0进行比较 分支跳转指令有： beq rs1, rs2, label: goto label if rs1 == rs2: B型 bne rs1, rs2, label: goto label if rs1 != rs2: B型 同理也有blt bgt等 无条件跳转指令有 jal rd, label: goto label and rd = pc + 4: J型 jalr rd, base, offset: goto [(base) + offset] and rd = pc + 4: J型 伪指令 一些更加直观的指令，会被翻译成上述指令 mv dst, src = addi dst, src, 0 li dst, imm la dst, label ret = jalr x0, x1, 0 函数调用 参数：x10 - x17(a0 - a7)为前8个参数，后面的参数倒序入栈，x1(ra)为返回值 注意需要维护好栈帧指针，并且调用者和被调用者需要维护好对应寄存器 数据表示与纠错 逻辑数据 True: 1 False: 0 字符数据 ASCII编码 占用1字节，表示128个西文字符 ASCII字符集 UNICODE字符集 为每种语言的每个字符设定了统一且唯一的二进制编码，兼容ASCII UTF-8编码 UTF-8编码 变长字符编码，长度由首字节确定 字符除首字节外均以10开头 字符显示 点阵字体：将字符展示在GUI界面中的方式，点阵字体是其中最简单的一种，即利用黑白位图来表示 矢量字体：用多条曲线来表示字符，每条曲线存储一部分关键点，显示的时候平滑连接关键点并填充 数值数据 整数 第一位为符号位，后续为数值位 正数的原=补=反 0有两个原码与反码(±0\\pm 0±0)，仅有1个补码 负数：反码为数值位取反，补码为反码+1 浮点数 浮点数组成为[s, exp, frac]，分别为符号位、阶码（表示方式为移码）与尾码（表示方式为原码） bias = 2^(len(exp) - 1) - 1 exp != 0, exp != 11..11时为规格化数，此时： E = exp - bias M = 1.frac num = (s ? -1 : 1) * M * 2^E exp == 0时为非规格化数，此时： E = 1 - bias M = 0.frac num = (s ? -1 : 1) * M * 2^E exp == 11..11, frac == 0时为inf 其他情况为NaN 浮点数加减： 操作阶码，阶码较小的右移，阶码大的为结果的阶码 尾数加减 规格化处理 舍入操作 检查阶码是否溢出 浮点数乘除： 阶码直接加减 尾数乘除法 规格化处理 舍入操作 检查阶码溢出 检错纠错码 码距指任意两个合法编码之间不同的二进制位数目的最小值，当码距为1的时候无检错能力 常见的检错纠错码有：奇偶校验码、海明校验码与循环冗余校验码 奇偶校验码 在k位数据码之外增加1位校验位，使得其中取值为1的位数总保持为奇数或偶数，码距为2 如偶校验：1101 -&gt; 11101，奇校验1101 -&gt; 01101 电路上来看是取异或即可 海明校验码 并行数据，为k位数据设定r位校验码，使其能够： 能够发现并改正k+r中任意一位出错 能够发现k+r中任意两位同时出错 码距为4 则k与r之间应该满足的关系是： 2r≥k+r+12^{r} \\geq k + r + 12r≥k+r+1：此时即为用2r2^{r}2r个编码分别表示出错的位数或都不出错 2r−1≥k+r2^{r - 1} \\geq k + r2r−1≥k+r：用r−1r - 1r−1为校验码为出错位编码，单独设置一位用于区分是1位还是2位出错（总校验位） 编码方案： 将校验位依次安排在2的幂次位，将数据位依次排布在剩下的位置 将数据位的编号拆分成2的幂的和，则所出现的幂对应位置上校验位负责对该数据位进行校验 校验位的值即为其所校验的数据的异或 总校验位的值为所有数据的异或 译码方案：对收到的数据的数据位再次编码并比较","tags":["笔记","计组","指令系统","数据表示","ALU"],"categories":["计算机组成原理"]},{"title":"从平板到辅助显示器","path":"/2024/10/07/从平板到辅助显示器/","content":"爆改平板ing 某日，某小伙痛定思痛，希望自己在撰写《计算机组成原理》的课程笔记时markdown预览页面不再只占电脑屏幕的25%，于是打开某东某宝某夕夕，搜索“便携式显示屏”并浏览，眼花缭乱，仔细抉择性价比后决定，不买了！ 高质量的价钱和大显示器差不多，但是暂时还不想入大显示器 原本决定利用平板文件传输助手进行文档分屏阅读，但是这有些许堂食，于是乎开始思考能否爆改平板，最终找到了一款免费的平板-&gt;显示器软件 —— Spacedesk Spacedesk 是一款投屏软件，可以将 Windows 系统投屏到 iOS 手机、安卓手机、iPad、iMac 上。（文案来自中文官网） 使用方法： 在你需要显示的设备上下载Spacedesk 在电脑上下载Spacedesk，官网需要科学上网，也可以在Microsoft应用商店下载 将两个设备置于同一局域网下 在PC上运行spacedeskConsole.exe，这里会显示连接的状况与连接方式，左上角显示的是连接接口，如下图 在显示器设备上运行spacedesk，可以看到其是作为Viewer，点击Connect to Primary Machine(Server)右边的+，输入PC上显示的IP地址即可 当然笔者只尝试过这一种连接方式，看上去它还提供了USB等多种方式；并且有的wifi可能会禁止这种内部设备相互通信的行为，此时可以试一下连接接口处显示的各个IP"},{"title":"密码学","path":"/2024/09/25/密码学/","content":"计算机网络安全技术 笔记 1 密码学基础 基本概念 传统加密 = 对称加密 = 单钥加密 安全性的来源是算法本身的保密性 现代加密 = 非对称加密 = 公钥加密 分离了算法与密钥 密码编码学的特征： 加密运算的运算类型： 置换：打乱输入数据的顺序 代换：将输入数据进行映射 所用的密钥数，即发送方与接收方使用的密钥是否相同 处理明文的方法： 块密码 / 分组密码：按组处理数据 流密码 / 序列密码：按比特处理数据 古典密码 代换密码 Caeser密码 对每个字母使用其后方的第3个字母代换 单表代换密码 单表代换代码：只有一张代换表，如Caeser与密钥词代码：设置一个密钥词放在前面，其余按照字母顺序 密钥词代码 Playfair密码 多表代换密码，同样有一个密钥词，按照行优先的方式填充在一个5×55\\times 55×5的矩阵内，剩余部分按顺序填充，i与j当成一个字母 加密方法： 每次加密两个字母，如果两个字母相同则更换后者为填充字，如果只剩一个字母同样填充，如： balloon -&gt; ba lx lo on 字母对落在同一行时，分别循环右移一位 字母对落在同一列时，分别循环下移一位 反之，对换两个字母的列 Hill密码 将字母指定为特定的数字，之后对原文序列做线性变换，因此密钥即为变换矩阵KKK 优势为：玩去哪隐藏了单字母的频率特性，并且矩阵越大，隐藏的信息就越多，例如3×33\\times 33×3矩阵隐藏了双字母的频率特性 Vigenere密码 多表代换密码的一种 Vigenere密码表 对于密钥中的每一个字母，在上表中取出其所对应的加密表，之后循环使用密钥中的加密表来为明文进行加密 Vernam密码和一次一密 利用与明文相同长度的密钥进行加密，基于二进制数据而非字节： ci=pi⊕kic_{i} = p_{i} \\oplus k_{i} ci​=pi​⊕ki​ 当kkk完全随机产生的时候，即为一次一密OTP，在理论上是牢不可破的，但是其在实际生活中几乎不可能实现 置换密码 常规的置换方法是按照密钥长度生成一个矩阵，并且按照行优先写入、列优先读出的规则，密钥决定了读出列的顺序 置换密码举例 ENIGMA TODO 破译 基本方法有： 穷举法：暴力，但是不能用于OTP 频率分析法：利用字符的统计特性，例如在英语中最常出现的字母是e，q后方总是跟着u 对称密码 加密与解密使用同样的密钥，因此密钥需要使用私密信道进行分配 对称密钥算法 常见的对称密钥算法简介： DES: Data Encryption Standard IDEA: International Data Encryption Algorithm RC 2/4/5 CAST-128 Blowfish 简化DES 加密算法：输入为一个8位明文组和10位密钥，输出8位密文组 解密算法：输出为一个8位密文组和10位密钥，输出8位明文组 S-DES算法的整体结构 密钥的生成 S-DES算法的整体结构 P10置换定义为[1..10] -&gt; [3, 5, 2, 7, 4, 10, 1, 9, 8, 6] LS-k指前5位与后5位分别循环左移k位 P8定义为[1..10] -&gt; [6, 3, 7, 4, 8, 5, 10, 9] 加密过程 S-DES算法加密过程 初始置换IP(1…8) = (2, 6, 3, 1, 4, 8, 5, 7) 扩展置换E/P(1…4) = (4, 1, 2, 3, 2, 3, 4, 1) ⊕\\oplus⊕代表异或操作 Lk/Rk分别代表左k位与右k为 S盒操作4输入2输出，S盒为4×44\\times 44×4矩阵，输出为123row = 2 * input[0] + input[3]col = 2 * input[1] + input[2]output = &#x27;&#123;0:b&#125;&#x27;.format(S[row][col]) S盒输出置换P4(1…4) = (2, 4, 3, 1) SW为交换左右4位 末尾置换IP−1^{-1}−1(1…8) = (4, 1, 3, 5, 7, 2, 8, 6) Feistel密码 建议使用乘积密码，即依次使用两个或以上的基本密码 交替使用代换和置换 Shannon引入了混淆和扩散来刻画密码系统 扩散是指将明文的统计信息消散在密文中 混淆是指尽量使密文与密钥之间的关系更复杂 Feistel密码结构为： 明文组等分成两部分，经过nnn轮迭代合成密文组 第iii轮输入为前一轮的输出 子密钥由密钥经过相应算法推导出 Feistel密码结构 DES密码 将明文分成64bits大小的块mmm，执行如下操作 DES(m)=IP−1⋅T16⋯T1⋅IP(m)\\mathrm{DES}(m) = \\mathrm{IP}^{-1}\\cdot T_{16} \\cdots T_{1}\\cdot \\mathrm{IP}(m) DES(m)=IP−1⋅T16​⋯T1​⋅IP(m) 密钥的生成步骤为： 拆分为两部分 左移一定位数 进行压缩置换 56bits→48bits\\quad56\\text{bits}\\to 48 \\text{bits}56bits→48bits 每一轮的迭代步骤（即F函数）为： E盒扩展置换32bits→48bits\\quad32\\text{bits}\\to 48\\text{bits}32bits→48bits 与对应轮次密钥异或 S盒代换选择，使用8个6进4出的S盒组成48bits→32bits\\quad48\\text{bits}\\to 32\\text{bits}48bits→32bits P盒置换 与另一半输入异或 三重DES加密 密钥长度为112比特，即两个常规DES算法的密钥拼接得到 加密算法为： C=Enc(k1,Dec(k2,Enc(k1,p)))C = \\mathrm{Enc}(k_{1}, \\mathrm{Dec}(k_{2}, \\mathrm{Enc}(k_{1}, p))) C=Enc(k1​,Dec(k2​,Enc(k1​,p))) Blowfish算法 分组长度64位，密钥长度在32~448位之间 Blowfish加密过程 F函数包含了4个S盒运算，子密钥与S和都是由算法本身生成的，并且对数据左右同时执行运算，复杂性更强 RC5 分组长度为32/64/128位，密钥长度0~2040位，参数有： www：分组长度 rrr：迭代次数 bbb：密钥的字节数 RC5加密过程 非对称密码 公钥密码体制： 明文 加密算法 公钥 / 私钥 密文 解密算法 利用Alice的公钥加密的密文只能被Alice的私钥解密，因此只要Alice保证了私钥的私有性，即可保证密码的安全，公钥是公开的 同样的，Alice可以用自己的私钥对数据进行签名，这样其他人可以利用Alice的公钥解签名，验证其专有性 这两种方法也可以同时使用，如下图 同时做签名与加密 数学原理 公钥密码基于陷门单向函数，其是指满足下列条件的函数fff： y=f(x)y = f(x)y=f(x)是可计算的 x=f−1(y)x = f^{-1}(y)x=f−1(y)是不可计算的 ∃δ\\exists\\delta∃δ，当δ\\deltaδ已知的时候，x=f−1(y)x = f^{-1}(y)x=f−1(y)是可计算的 数论基础 欧拉函数φ(n)=∑i=1n1gcd(m,n)=1\\varphi(n) = \\sum\\limits_{i=1}^{n}\\mathbf{1}_{\\mathrm{gcd}(m, n) = 1}φ(n)=i=1∑n​1gcd(m,n)=1​ 欧拉定理： gcd(m,n)=1⇔mφ(n)≡1 mod n\\mathrm{gcd}(m, n) = 1 \\Leftrightarrow m^{\\varphi(n)} \\equiv 1 \\text{ mod }n gcd(m,n)=1⇔mφ(n)≡1 mod n 推论：给定素数p≠qp eq qp=q，整数0&lt;m&lt;n=pq0 &lt; m &lt; n = pq0&lt;m&lt;n=pq，则∀k∈Z\\forall k \\in \\mathbb{Z}∀k∈Z mkφ(n)+1≡m mod nm^{k\\varphi(n) + 1} \\equiv m \\text{ mod }n mkφ(n)+1≡m mod n RSA算法 密钥的产生： 取大素数p,qp, qp,q，计算n=pqn = pqn=pq，公开nnn 计算φ(n)=(p−1)(q−1)\\varphi(n) = (p-1)(q-1)φ(n)=(p−1)(q−1) 取eee满足：gcd(e,φ(n))=1\\mathrm{gcd}(e, \\varphi(n)) = 1gcd(e,φ(n))=1且1&lt;e&lt;φ(n)1 &lt; e &lt; \\varphi(n)1&lt;e&lt;φ(n) 计算d&lt;φ(n)d &lt; \\varphi(n)d&lt;φ(n)使得de≡1 mod φ(n)de \\equiv 1 \\text{ mod }\\varphi(n)de≡1 mod φ(n) 公钥为(e,n)(e, n)(e,n)，私钥为(d,n)(d, n)(d,n) 加密过程：将待解密的内容分为k≤log⁡2nk\\leq \\log_{2}nk≤log2​n组，则密文为：C=Me mod nC = M^{e}\\text{ mod }nC=Me mod n 相对应的解密过程为：M=Cd mod nM = C^{d}\\text{ mod } nM=Cd mod n 大素数质因数分解的困难性保证了这个算法的安全性，因此密钥越大，安全性越高，但是性能会越低 DH密钥交换算法 我们定义本原根： aaa是素数ppp的本原根当且仅当{a,a2,…,ap−1} mod p={1,…,p−1}\\{a, a^{2},\\dots,a^{p-1}\\}\\text{ mod }p = \\{1, \\dots, p-1\\}{a,a2,…,ap−1} mod p={1,…,p−1} 对于素数ppp，设其一个本原根为aaa，取整数b&lt;pb&lt;pb&lt;p，则我们可以找到唯一的指数iii，使得b≡ai mod pb\\equiv a^{i}\\text{ mod }pb≡ai mod p，则iii成为bbb以aaa为底模ppp的离散对数，记为i=inda,p(b)i = \\mathrm{ind}_{a, p}(b)i=inda,p​(b) 离散对数满足：计算bbb是简单的，但是计算iii非常困难 DH算法可以且仅可以用于密钥交换 DH密钥交换算法 这样之后Alice与Bob同时知道了一个密钥KKK，可以用于作为对称密码的密钥 密钥分配 传统的对称密码分配 最朴素的方法有人工传送密钥，但是这样只适用于链路加密，并且被破译的概率可能更大，如果对于端对端加密，则需要KDC 密钥分配中心KDC 假定每个用户与KDC共享唯一的一个主密钥，则Alice和Bob获得会话密钥KsK_{s}Ks​的过程是： Alice向KDC请求会话密钥以保护与Bob的逻辑连接，提供Alice与Bob的标识和临时交互号N1N_{1}N1​ KDC用KaK_{a}Ka​加密响应并告知Alice，其中包括Ks,N1,EncKb(Ks,IdA)K_{s}, N_{1}, \\mathrm{Enc}_{K_{b}}(K_{s}, Id_{A})Ks​,N1​,EncKb​​(Ks​,IdA​) Alice解密，利用N1N_{1}N1​分割信息，前一段为KsK_{s}Ks​，后一段转发给Bob Bob使用KbK_{b}Kb​解密即可 当网络规模过大的时候，使用层次式的KDC，类比ISP 公钥的分配 公开发布 即通信方直接公布自己的公钥，非常简便，但是容易被伪造 公开可访问目录 由管理员维护一个dict[name, public_key]，定期发布或者更新该目录 通信方通过管理员来注册公钥，可以随时更新自己的公-私钥对，并且可以通过与管理员的安全认证通道来访问公钥目录 这种方法的弱点在于目录管理员本身 公钥授权 由管理员来控制 A发送带有时间戳的信息请求B的公钥 管理员发送签名","tags":["笔记","网安","密码学"]},{"title":"信号处理原理 2","path":"/2024/09/24/信号处理原理2/","content":"信号处理原理 笔记 2 信号的数学基础 信号运算 共有四种基本运算： 四则运算：线性、乘除 波形变换：时移、压扩、反褶 数学运算：微分、积分 相互运算：卷积、相关 四则运算 略 波形变换 时移运算：将原信号的波形沿横轴平移bbb个单位，即f(t)−&gt;f(t−b)f(t) -&gt; f(t - b)f(t)−&gt;f(t−b) 反褶运算: 将原信号沿着纵轴翻转 压扩运算：将原信号压缩/扩张为原来的∣a∣|a|∣a∣个单位，aaa的符号决定是否需要反褶 注意：画图细节，例如原点、横纵坐标轴注意原函数振幅为0的部分 数学运算 微分运算：图像边缘提取 积分运算：充电 举例： 能量信号：E(f(t))=∫−∞∞∣∣f(t)∣∣2dtE\\bigl(f(t)\\bigr) = \\int_{-\\infty}^{\\infty}||f(t)||^{2}dtE(f(t))=∫−∞∞​∣∣f(t)∣∣2dt 功率信号：P(f(t))=lim⁡T→∞1T∫−T/2T/2∣∣f(t)∣∣2dtP\\bigl(f(t)\\bigr) = \\lim\\limits_{T\\to \\infty}\\frac{1}{T}\\int_{-T/2}^{T/2}||f(t)||^{2}dtP(f(t))=T→∞lim​T1​∫−T/2T/2​∣∣f(t)∣∣2dt 离散信号类似，将 ∫\\int∫ 换为 ∑\\sum∑ 即可 相互运算 卷积运算 设f,gf, gf,g为两个连续时间信号，其卷积定义为： (f∗g)(t)=f(t)∗g(t)=∫−∞∞f(t−τ)g(τ)dτ(f*g)(t) = f(t)*g(t) = \\int_{-\\infty}^{\\infty}f(t - \\tau)g(\\tau) d\\tau (f∗g)(t)=f(t)∗g(t)=∫−∞∞​f(t−τ)g(τ)dτ 运算性质： 交换律：f1∗f2=f2∗f1f_{1}*f_{2} = f_{2}*f_{1}f1​∗f2​=f2​∗f1​ 分配率: f1∗(f2+f3)=f1∗f2+f1∗f3f_{1}*(f_{2} + f_{3}) = f_{1}*f_{2} + f_{1} * f_{3}f1​∗(f2​+f3​)=f1​∗f2​+f1​∗f3​ 结合律：f1∗(f2∗f3)=(f1∗f2)∗f3f_{1}*(f_{2} * f_{3}) = (f_{1} * f_{2}) * f_{3}f1​∗(f2​∗f3​)=(f1​∗f2​)∗f3​ 微分积分性质：(f1∗f2)(n)=f1(m)∗f2(n−m)(f_{1}*f_{2})^{(n)} = f_{1}^{(m)} * f_{2}^{(n-m)}(f1​∗f2​)(n)=f1(m)​∗f2(n−m)​ 上式中的n,m,n−mn, m, n-mn,m,n−m代表微分/积分的阶数，正数代表微分，负数代表积分 相关运算 定义为： Rf1f2(t)=∫−∞∞f1(τ)f2(τ−t)‾dτ=∫−∞∞f1(τ+t)f2(τ)‾dτ\\begin{align*} R_{f_{1}f_{2}}(t) &amp;= \\int_{-\\infty}^{\\infty}f_{1}(\\tau)\\overline{f_{2}(\\tau - t)}d\\tau \\\\ &amp;= \\int_{-\\infty}^{\\infty}f_{1}(\\tau + t)\\overline{f_{2}(\\tau)}d\\tau \\end{align*} Rf1​f2​​(t)​=∫−∞∞​f1​(τ)f2​(τ−t)​dτ=∫−∞∞​f1​(τ+t)f2​(τ)​dτ​ 性质： Rf1f2(t)=Rf2f1(−t)‾R_{f_{1}f_{2}}(t) = \\overline{R_{f_{2}f_{1}}(-t)}Rf1​f2​​(t)=Rf2​f1​​(−t)​ Rf2f1(t)=f1(−t)‾∗f2(t)R_{f_{2}f_{1}}(t) = \\overline{f_{1}(-t)}*f_{2}(t)Rf2​f1​​(t)=f1​(−t)​∗f2​(t) 奇异信号 Sa函数 略 单位斜变信号 定义为： R(t)={0t&lt;0tt≥0R(t) = \\begin{cases} 0 &amp; t &lt; 0 \\\\ t &amp; t \\geq 0 \\end{cases} R(t)={0t​t&lt;0t≥0​ 截顶的单位斜变信号定义为： R(t,τ)={0t&lt;0t0≤t&lt;ττt≥τR(t, \\tau) = \\begin{cases} 0 &amp; t &lt; 0 \\\\ t &amp; 0\\leq t &lt; \\tau \\\\ \\tau &amp; t \\geq \\tau \\end{cases} R(t,τ)=⎩⎨⎧​0tτ​t&lt;00≤t&lt;τt≥τ​ 单位阶变信号 定义为： u(t)={0t&lt;01t≥0u(t) = \\begin{cases} 0 &amp; t &lt; 0 \\\\ 1 &amp; t \\geq 0 \\end{cases} u(t)={01​t&lt;0t≥0​ 单位矩形脉冲信号 定义为： Gτ(t)={1∣t∣≤τ/20∣t∣&gt;τ/2G_{\\tau}(t) = \\begin{cases} 1 &amp; |t| \\leq \\tau / 2\\\\ 0 &amp; |t| &gt; \\tau / 2 \\end{cases} Gτ​(t)={10​∣t∣≤τ/2∣t∣&gt;τ/2​ 脉冲的定义 符号函数 sgn(t)={1t≥0−1t&lt;0\\text{sgn}(t) = \\begin{cases} 1 &amp; t \\geq 0 \\\\ -1 &amp; t &lt; 0 \\end{cases} sgn(t)={1−1​t≥0t&lt;0​ 单位冲激信号 狄拉克定义δ(t)\\delta(t)δ(t)为满足以下两式的信号： δ(t)=0(t≠0)∫−∞∞δ(t)=1\\begin{align*} \\delta(t) &amp;= 0\\quad (t eq 0) \\\\ \\int_{-\\infty}^{\\infty}\\delta(t) &amp;= 1 \\end{align*} δ(t)∫−∞∞​δ(t)​=0(t=0)=1​ 可一般化为：δE,t0(t)=Eδ(t−t0)\\delta_{E, t_{0}}(t) = E\\delta(t - t_{0})δE,t0​​(t)=Eδ(t−t0​) 也可以定义为： δ(t)=lim⁡τ→0Gτ(t)τ\\delta(t) = \\lim\\limits_{\\tau\\to 0}\\frac{G_{\\tau}(t)}{\\tau} δ(t)=τ→0lim​τGτ​(t)​ 性质： f(t)∗δ(t)=f(t)f(t)∗δ(t−t0)=f(t−t0)δ(at)=1∣a∣δ(t)(a≠0)∫−∞tδ(τ)dτ=u(t)∫−∞∞f(t)δ(t−t0)=f(t0)\\begin{align*} f(t) * \\delta(t) &amp;= f(t) \\\\ f(t) * \\delta(t - t_{0}) &amp;= f(t - t_{0}) \\\\ \\delta(at) &amp;= \\frac{1}{|a|}\\delta(t)\\qquad(a eq 0) \\\\ \\int_{-\\infty}^{t}\\delta(\\tau)d\\tau &amp;= u(t) \\\\ \\int_{-\\infty}^{\\infty}f(t)\\delta(t - t_{0}) &amp;= f(t_{0}) \\end{align*} f(t)∗δ(t)f(t)∗δ(t−t0​)δ(at)∫−∞t​δ(τ)dτ∫−∞∞​f(t)δ(t−t0​)​=f(t)=f(t−t0​)=∣a∣1​δ(t)(a=0)=u(t)=f(t0​)​ 当我们把很多个冲激点不同的冲激信号线性叠加时，可以得到冲激串信号，通常取冲激点为周期变化的序列，可以用于信号的抽样： ΔTs(t)=∑n=−∞∞δ(t−nTs)\\Delta_{T_{s}}(t) = \\sum\\limits_{n = -\\infty}^{\\infty}\\delta(t - nT_{s}) ΔTs​​(t)=n=−∞∑∞​δ(t−nTs​) 则我们可以对于信号f(t)f(t)f(t)抽样出其中的一个子信号序列： fs(t)=f(t)Δ˙Ts(t)=∑n=−∞∞f(nTs)δ(t−nTs)f_{s}(t) = f(t)\\dot\\Delta_{T_{s}}(t) = \\sum\\limits_{n = -\\infty}^{\\infty}f(nT_{s})\\delta(t - nT_{s}) fs​(t)=f(t)Δ˙Ts​​(t)=n=−∞∑∞​f(nTs​)δ(t−nTs​)","tags":["笔记","信原","数学基础"],"categories":["信号处理原理"]},{"title":"信号处理原理 1","path":"/2024/09/10/信号处理原理1/","content":"信号处理原理 笔记 1 信号的基本概念 信号的描述 数学描述： 使用具体的表达式描述为函数Sa(t)=sin⁡(t)tSa(t) = \\frac{\\sin(t)}{t} Sa(t)=tsin(t)​ 波形描述： 函数图像，横纵坐标要求标出，原点要求标出，零点要求标出 分类： 确定信号与随机信号 周期信号(f(t)=f(t+T)∀t∈R)(f(t) = f(t + T) \\forall t \\in \\mathbb{R})(f(t)=f(t+T)∀t∈R)与非周期信号 奇异信号举例： 正余弦信号：f(t)=Ksin⁡(ωt+θ)f(t)=Kcos⁡(ωt+θ)\\begin{align*} f(t) &amp;= K\\sin(\\omega t + \\theta) \\\\ f(t) &amp;= K\\cos(\\omega t + \\theta) \\\\ \\end{align*} f(t)f(t)​=Ksin(ωt+θ)=Kcos(ωt+θ)​ Sa函数：Sa(t)=sin⁡(t)tSa(t) = \\frac{\\sin(t)}{t} Sa(t)=tsin(t)​ 有结论：∫−∞∞Sa(t)=π\\int_{-\\infty}^{\\infty}Sa(t) = \\pi ∫−∞∞​Sa(t)=π 指数信号：f(t)=Keαtf(t) = Ke^{\\alpha t} f(t)=Keαt 欧拉公式 eix=cos⁡(x)+isin⁡(x)sin⁡(x)=eix−e−ix2icos⁡(x)=eix+e−ix2\\begin{align*} e^{ix} &amp;= \\cos(x) + i\\sin(x) \\\\ \\sin(x) &amp;= \\frac{e^{ix} - e^{-ix}}{2i} \\\\ \\cos(x) &amp;= \\frac{e^{ix} + e^{-ix}}{2} \\end{align*} eixsin(x)cos(x)​=cos(x)+isin(x)=2ieix−e−ix​=2eix+e−ix​​ 用于描述复指数信号： f(t)=Kest=Ke(σ+jω)t=Keσt(cos⁡(ωt)+jsin⁡(ωt))\\begin{align*} f(t) &amp;= Ke^{st} \\\\ &amp;= Ke^{(\\sigma + j\\omega)t} \\\\ &amp;= Ke^{\\sigma t}(\\cos(\\omega t) + j\\sin(\\omega t)) \\end{align*} f(t)​=Kest=Ke(σ+jω)t=Keσt(cos(ωt)+jsin(ωt))​ 函数分解 若非零函数φ1(t)\\varphi_{1}(t)φ1​(t)与φ2(t)\\varphi_{2}(t)φ2​(t)满足： ∫t1t2φ1(t)φ2∗(t)dt=0\\int_{t_{1}}^{t_{2}}\\varphi_{1}(t)\\varphi_{2}^{*}(t)dt = 0 ∫t1​t2​​φ1​(t)φ2∗​(t)dt=0 则称其在[t1,t2][t_{1}, t_{2}][t1​,t2​]上正交 若非零函数序列φ1(t),φ2(t),⋯ ,φn(t)\\varphi_{1}(t), \\varphi_{2}(t), \\cdots, \\varphi_{n}(t)φ1​(t),φ2​(t),⋯,φn​(t)满足： ∫t1t2φi(t)φj∗(t)dt={0i≠jki≠0i=j\\int_{t_{1}}^{t_{2}}\\varphi_{i}(t)\\varphi_{j}^{*}(t)dt = \\begin{cases} 0 &amp; i eq j \\\\ k_{i} eq 0 &amp;i = j \\end{cases} ∫t1​t2​​φi​(t)φj∗​(t)dt={0ki​=0​i=ji=j​ 则称这组函数为正交函数集 举例： {cos⁡(kω1t+φk) ∣ k=0,1,…,n,φ0=0}\\{\\cos(k\\omega_{1}t + \\varphi_{k})\\ | \\,k = 0, 1, \\dots, n, \\varphi_{0} = 0\\}{cos(kω1​t+φk​) ∣k=0,1,…,n,φ0​=0}，区间[0,2πω1][0, \\frac{2\\pi}{\\omega_{1}}][0,ω1​2π​] {ejnω0t ∣ n=0,±1,±2,…,ω∈R}\\{e^{jn\\omega_{0}t}\\,|\\,n = 0, \\plusmn 1, \\plusmn 2, \\dots, \\omega \\in \\mathbf{R}\\}{ejnω0​t∣n=0,±1,±2,…,ω∈R}，区间[−πω0,πω0][-\\frac{\\pi}{\\omega_{0}}, \\frac{\\pi}{\\omega_{0}}][−ω0​π​,ω0​π​] 若在[t1,t2][t_{1}, t_{2}][t1​,t2​]上，除正交函数集{φi(t)}\\{\\varphi_{i}(t)\\}{φi​(t)}外，不存在函数x(t)满足： 0&lt;∫t1t2x(t)x∗(t)dt&lt;∞∫t1t2x(t)φi∗(t)dt=0∀i\\begin{align*} 0 &lt; &amp;\\int_{t_{1}}^{t_{2}}x(t)x^{*}(t)dt &lt; \\infty \\\\ &amp;\\int_{t_{1}}^{t_{2}}x(t)\\varphi_{i}^{*}(t)dt = 0 \\quad \\forall i \\end{align*} 0&lt;​∫t1​t2​​x(t)x∗(t)dt&lt;∞∫t1​t2​​x(t)φi∗​(t)dt=0∀i​ 则称{φi(t)}\\{\\varphi_{i}(t)\\}{φi​(t)}是完备的","tags":["笔记","信原","数学基础"],"categories":["信号处理原理"]},{"title":"Rust入门","path":"/2024/07/26/Rust入门/","content":"Rust学习随记（暂时停更） Rust 入门随记 本文用来记录在学习Rust的过程中遇到的各种问题 所有权 函数参数写法是x: &amp;u32而不是&amp;x: u32（蠢了 裸指针不涉及所有权 切片 数组切片的下表索引一定是usize 字符串切片是以字节为单位，一些UTF-8字符（例如中文）会占用多个字节，因此一般不采用切片 项目结构 main.rs为二进制项目的包根，lib.rs为库项目的包根，包根也即crates::访问的地方 一个功能文件夹如果要作为一个模块导出，需要在文件夹下添加mod.rs，其中指出需要导出的模块 Cargo换源 使用清华源避免在拉去第三方库的时候由于超时导致失败，方法如下： 在$HOME/.cargo/config.toml中添加 12[source.tuna]registry = &quot;https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git&quot; 并将replace-with的值改为tuna即可","tags":["Rust"]},{"title":"IAI-神经网络与机器学习","path":"/2024/06/17/IAI-神经网络与机器学习/","content":"人智导 神经网络与机器学习 神经网络 多层神经网络示意图 神经元 接收输入x⃗\\vec{x}x，计算net=w⃗⋅x⃗+bnet = \\vec{w} \\cdot \\vec{x} + bnet=w⋅x+b，之后使用激活函数ggg对其进行激活（限制其值域范围） 常见的激活函数有： 符号函数sgn\\mathrm{sgn}sgn Sigmoid函数σ(z)=11+e−z\\sigma(z) = \\frac{1}{1 + e^{-z}}σ(z)=1+e−z1​ 双曲正切函数tanh⁡(z)=ez−e−zez+e−z\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}tanh(z)=ez+e−zez−e−z​ 线性整流函数ReLU(z)=max⁡{0,z}\\mathrm{ReLU}(z) = \\max\\{0, z\\}ReLU(z)=max{0,z} 输出归一化 采用Softmax函数将输出层netinet_{i}neti​归一化： oi=eneti∑k=1menetko_{i} = \\frac{e^{net_{i}}}{\\sum\\limits_{k=1}^{m}e^{net_{k}}} oi​=k=1∑m​enetk​eneti​​ 全连接网络 隐含层之间是全连接的神经网络 训练方式为： 构建数据集，划分为训练集与验证集（实际应用的数据被称为为测试集，我们在训练阶段理应不能得到这批数据） 选择损失函数，通常选用误差平方和或交叉熵，误差平方和为：E(w⃗)=12∑d=1n∑k=1m(tkd−okd)2E(\\vec{w}) = \\frac{1}{2}\\sum\\limits_{d=1}^{n}\\sum\\limits_{k=1}^{m}(t_{kd} - o_{kd})^{2} E(w)=21​d=1∑n​k=1∑m​(tkd​−okd​)2 训练：求损失函数最小值，一种算法为梯度下降：wi⇐wi−η∂E∂wiw_{i} \\Leftarrow w_{i} - \\eta \\frac{\\partial E}{\\partial w_{i}} wi​⇐wi​−η∂wi​∂E​ 也即：w⇐w−η(∇wE)w \\Leftarrow w - \\eta( abla_{w}E) w⇐w−η(∇w​E) 梯度下降分为批量、小批量、随机样本三种，差距在与每次处理的样本个数 梯度的计算 随机梯度下降中最麻烦的问题在于梯度的计算，主要思想是链式法则与反向传播，以激活函数为σ\\sigmaσ为例，具体来说，算法为： 随机初始化为权重为较小随机值给定样本，计算所有输出对于输出层第jjj个元素，有：∂E∂wji=∂E∂oj∂oj∂netj∂netj∂wji=−(tj−oj)oj(1−oj)xji=−δjxji\\begin{align*} \\frac{\\partial E}{\\partial w_{ji}} &amp;= \\frac{\\partial E}{\\partial o_{j}} \\frac{\\partial o_{j}}{\\partial net_{j}} \\frac{\\partial net_{j}}{\\partial w_{ji}} \\\\ &amp;= -(t_{j} - o_{j})o_{j}(1-o_{j})x_{ji} \\\\ &amp;= - \\delta_{j}x_{ji}\\end{align*}∂wji​∂E​​=∂oj​∂E​∂netj​∂oj​​∂wji​∂netj​​=−(tj​−oj​)oj​(1−oj​)xji​=−δj​xji​​更新权重对于隐含层第jjj个元素，有：∂E∂wji=∑k∈succ(j)(∂E∂netk∂netk∂oj∂oj∂netj)∂netj∂wji=−∑k∈succ(j)(δkwkjoj(1−oj))xji=−δjxji\\begin{align*} \\frac{\\partial E}{\\partial w_{ji}} &amp;= \\sum\\limits_{k \\in \\text{succ}(j)}\\biggl(\\frac{\\partial E}{\\partial net_{k}} \\frac{\\partial net_{k}}{\\partial o_{j}} \\frac{\\partial o_{j}}{\\partial net_{j}}\\biggr) \\frac{\\partial net_{j}}{\\partial w_{ji}} \\\\ &amp;= -\\sum\\limits_{k \\in \\text{succ}(j)}\\bigl(\\delta_{k}w_{kj}o_{j}(1-o_{j})\\bigr)x_{ji} \\\\ &amp;= -\\delta_{j}x_{ji}\\end{align*}∂wji​∂E​​=k∈succ(j)∑​(∂netk​∂E​∂oj​∂netk​​∂netj​∂oj​​)∂wji​∂netj​​=−k∈succ(j)∑​(δk​wkj​oj​(1−oj​))xji​=−δj​xji​​更新权重 交叉熵 交叉熵损失函数为： H(w)=−∑d=1N∑k=1Mtkdlog⁡(okd)H(w) = -\\sum\\limits_{d=1}^{N}\\sum\\limits_{k=1}^{M}t_{kd}\\log(o_{kd}) H(w)=−d=1∑N​k=1∑M​tkd​log(okd​) 其中okdo_{kd}okd​为实际值，要求是概率，因此其输入层需要经过一次softmax 平方和损失函数常用于输出是具体数值的问题，交叉熵损失函数用于分类问题 卷积神经网络 全连接的不足：参数过多，影响速度与效率 卷积示意图 因此卷积神经网络利用卷积核对于输入数据进行卷积，降低输出维数，卷积核通过训练得到，并且卷积核是共享的，也即对于同一组数据的不同部分其参数值不变 填充与步长 填充为在原输入的最外围填充若干圈000来增加维数，例如5×55\\times 55×5变为7×77 \\times 77×7 步长为卷积核每次滑动的距离，必须要保证卷积核被完整的包含在输入当中 多卷积核与多通道 多个卷积共同卷同一个数据，每个卷积产生一个通道，通道数等于卷积核数 当输入为多通道时，例如6×6×36\\times 6\\times 36×6×3，卷积核的深度一定要与之一致，即x×y×3x \\times y \\times 3x×y×3 池化 降维的手段，通常是将一定的区域压缩为一个值，通常包括最大池化、平均池化等，窗口的大小与步长都可以设置 最大池化示意图 实例 两个实例 LeNet神经网络 VGG-16神经网络 总结 卷积神经网络总结： 卷积核能够提取特征 参数较少 梯度消失问题 神经网络的主要问题之一 在BP中，我们有： δh=oh(1−oh)∑k∈succ(h)δkwkh≤14∑k∈succ(h)δkwkh\\delta_{h} = o_{h}(1 - o_{h})\\sum\\limits_{k\\in \\mathrm{succ}(h)}\\delta_{k}w_{kh} \\leq \\frac{1}{4}\\sum\\limits_{k\\in \\mathrm{succ}(h)}\\delta_{k}w_{kh} δh​=oh​(1−oh​)k∈succ(h)∑​δk​wkh​≤41​k∈succ(h)∑​δk​wkh​ 因此层数过多的时候，梯度将以指数级下降，一种解决思路是采用ReLU\\mathrm{ReLU}ReLU 两个实例 GoogLeNet中的Inception模块 利用1×11\\times 11×1等卷积核改变通道数之后拼接起来，相当于在参数尽可能少的情况下减弱梯度消失的问题 残差网络中的残差模块 在残差网络中，在一层的常规计算结束之后，将计算结果与输入取加和得到下一层的输入，这样可以一定程度上避免神经网络发生退化（层数过多导致再增加层数的时候效果提升不显著） 注意要求F(X)F(X)F(X)与XXX的维数、通道数必须相同，因此需要对二者进行相应的填充 过拟合问题 对于训练集数据过拟合，训练出的模型不具有普适性，解决方法有： 使用验证集，个人理解是利用验证集来模拟测试集，在训练集上训练，在验证集上防止过拟合 正则化项法：将损失函数加上正则化项∣∣w⃗∣∣2||\\vec{w}||^{2}∣∣w∣∣2其中w⃗\\vec{w}w为所有的参数组成的向量，从而降低模型参数个数，降低模型复杂性 Dropout：随机临时舍弃一些神经元，使之不参与计算，减少参数量，舍弃率可调 数据增强：增加数据量，将同一份数据进行多种变换，例如图像的缩放、旋转等等，也有非线性化等高级的做法 词向量 第一个问题：如何来表示词与文本 独热编码(One-hot) 用于词表等长的向量来表示词，第iii个词的第iii位为111，其余全000 优点： 简单方便 缺点： 编码太长 无法度量相似性 分布式表示 稠密向量表示，向量的每一位代表一个特征，具体的数值代表这个词该种特征的强弱 克服了独热的两种问题，但是编码很困难，具体的值不容易找出，需要在训练过程中调整 语言模型 nnn元语言模型是指，通过前n−1n - 1n−1个词来推断下一个词，在神经网络中实现方式如下： n元模型神经网络 其中第一步为词嵌入得到词向量，具体的向量需要通过训练得到 参数的估计采用最大似然估计的方式，即： max⁡θ∏w∈CP(w=k ∣ context(w),θ)\\max\\limits_{\\theta}\\prod\\limits_{w\\in C}P(w = k\\,|\\, \\text{context}(w), \\theta) θmax​w∈C∏​P(w=k∣context(w),θ) word2vec 训练词向量的模型，有连续词袋模型和跳词模型 连续词袋模型(CBOW) 我们有一个词表WWW，训练集为大量的句子，对于某个句子w1…wmw_{1}\\dots w_{m}w1​…wm​，我们采用如下的方式进行训练： 在句中任选一位置合适的词wtw_{t}wt​ 取其前后各ccc个词的词向量进行求和并激活：xw=g(∑i=1c(wt−i+wt+i))x_{w} = g(\\sum\\limits_{i=1}^{c}(w_{t-i} + w_{t + i}))xw​=g(i=1∑c​(wt−i​+wt+i​)) 将xwx_{w}xw​作为输入传给一个霍夫曼树，其中每一个内部节点是一个神经元，叶节点为所有的词 霍夫曼树的内部节点，输入为一个词，输出为选择左边或者右边的概率，最终我们要极大到达原来的词wtw_{t}wt​的概率，也即神经网络需要训练每个节点的参数使得root→wt\\mathrm{root}\\rightarrow w_{t}root→wt​这条路径的概率尽可能大 跳词模型 PPT没说，略 循环神经网络RNN 循环神经网络 如上图，我们利用输入来更新状态向量h(k)=[h1(k),…,hm(k)]h^{(k)} = [h_{1}^{(k)}, \\dots, h_{m}^{(k)}]h(k)=[h1(k)​,…,hm(k)​] 最终输出的处理方式根据相应的实际问题变化而变化，例如情感分类问题可以接一个全连接层与一个softmax 并且我们可以将每一次循环的结果都输出出来进行相应的处理，例如看图说话的过程可以将每一次循环的结果分别做一次全连接与softmax来作为一个字，最终组成一段话 双向循环神经网络 由于一些情况下序列不仅是有正向的关系，利于一句话的某个词需要根据上下文而不是上文才能确定，而采用传统的RNN会导致下文信息的丢失，因此采用一个双向的形式 双向循环神经网络 如上图，这样可以同时考虑上下文的信息 序列到序列 考虑在实际中，许多问题的输入也是一个序列而不是一个简单的向量或矩阵，例如问答、翻译等，因此采用序列到序列的RNN模式，即采用编码器-解码器模块，现将输入序列编码为矩阵或向量，再将其放入解码器中 序列到序列循环神经网络 长短期记忆网络LSTM 简单RNN的问题： 长期依赖：如果输入序列具有距离较远的依赖关系，那RNN很容易丢失这层关系，例如&quot;bei jing shi yi ge mei li de&quot;这句话，&quot;shi&quot;具体的字的确认就很困难，需要根据后续的输入来确定，而这层关系被短期的RNN丢弃 重点选择问题：序列中不同部分的重要性不同 梯度消失问题 因此改进为LSTM LSTM循环结构 在上图的循环结构中，维护两个状态sss与hhh，循环结构内部是由“门”组成的 门是指一层神经网络，例如σ\\sigmaσ门就指的是将输入接一个全连接层和一个σ\\sigmaσ激活函数作为输出，全连接层的作用是将输入维数与状态的维数进行匹配 结构中×\\times×等算数运算符代表的是按位操作，相当于是对原有数据进行重新加权与筛选 下面依次介绍上图中的门 遗忘门 左数第一个，为σ\\sigmaσ门，将原有状态h(t−1)h^{(t-1)}h(t−1)和输入共同作为输入，经过全连接与σ\\sigmaσ之后直接与状态向量s(t−1)s^{(t-1)}s(t−1)按位相乘 表示遗忘掉状态中的一些信息，防止过拟合 输入部分 输入门：左数第二个，也为σ\\sigmaσ门 输入处理单元：左数第三个，为tanh⁡\\tanhtanh门 将这两个门的处理结果按位乘，表示这一轮学到的东西，之后与遗忘后的状态向量s′(t−1)s^{&#x27;(t-1)}s′(t−1)按位加，完成学习的过程得到s(t)s^{(t)}s(t) 输出部分 输出门：左数第四个，为σ\\sigmaσ门 输出处理单元：不是门！是一个单独的tanh⁡\\tanhtanh函数 将这两个的处理结果按位乘得到新一轮的输出状态h(t)h^{(t)}h(t) 实例 利用LSTM解决序列到序列的问题 编码器-解码器模式的LSTM","tags":["笔记","IAI","神经网络","机器学习"],"categories":["人工智能导论"]},{"title":"概统复习笔记","path":"/2024/06/15/概统复习笔记/","content":"概统 复习随笔 概统复习笔记 两两独立但不相互独立 例1 四张卡牌，分别写有2,3,5,302, 3, 5, 302,3,5,30，随机抽取一张，定义事件AAA为取出的数字是222的倍数，事件BBB为取出的数字是333的倍数，事件CCC为取出的数字是555的倍数，则有 P(A)=P(B)=P(C)=12P(AB)=P(BC)=P(CA)=14=12×12P(ABC)=14≠P(A)P(B)P(C)\\begin{align*} P(A) = P(B) &amp;= P(C) = \\frac{1}{2} \\\\ P(AB) = P(BC) &amp;= P(CA) = \\frac{1}{4} = \\frac{1}{2}\\times \\frac{1}{2} \\\\ P(ABC) = \\frac{1}{4} &amp; eq P(A)P(B)P(C) \\end{align*} P(A)=P(B)P(AB)=P(BC)P(ABC)=41​​=P(C)=21​=P(CA)=41​=21​×21​=P(A)P(B)P(C)​ 例2 连续独立抛一枚质地均匀的硬币两次，AAA代表第一次正面向上，BBB代表第二次正面向上，CCC代表一正一反，则 P(A)=P(B)=P(C)=12P(AB)=P(BC)=P(CA)=14=12×12P(ABC)=0≠P(A)P(B)P(C)\\begin{align*} P(A) = P(B) &amp;= P(C) = \\frac{1}{2} \\\\ P(AB) = P(BC) &amp;= P(CA) = \\frac{1}{4} = \\frac{1}{2}\\times \\frac{1}{2} \\\\ P(ABC) = 0 &amp; eq P(A)P(B)P(C) \\end{align*} P(A)=P(B)P(AB)=P(BC)P(ABC)=0​=P(C)=21​=P(CA)=41​=21​×21​=P(A)P(B)P(C)​ 条件独立与独立无关 条件独立不蕴含独立 对于任意非空事件A,BA, BA,B有： P(AB ∣ B)=P(AB)P(B)=P(A ∣ B)≡P(A ∣ B)P(B ∣ B)P(AB\\,|\\,B) = \\frac{P(AB)}{P(B)} = P(A\\,|\\,B) \\equiv P(A\\,|\\,B)P(B\\,|\\,B) P(AB∣B)=P(B)P(AB)​=P(A∣B)≡P(A∣B)P(B∣B) 因此它们都在条件BBB下独立，显然不一定A,BA, BA,B独立 独立不蕴含条件独立 对于独立事件A,BA, BA,B，满足C=A∪B⊊ΩC = A\\cup B \\subsetneq \\OmegaC=A∪B⊊Ω，则有： P(AB ∣ C)=P(ABC)P(C)=P(AC)P(BC)P(C)≠P(AC)P(BC)P(C)2=P(A ∣ C)P(B ∣ C)P(AB\\,|\\,C) = \\frac{P(ABC)}{P(C)} = \\frac{P(AC)P(BC)}{P(C)} eq \\frac{P(AC)P(BC)}{P(C)^{2}} = P(A\\,|\\,C)P(B\\,|\\,C) P(AB∣C)=P(C)P(ABC)​=P(C)P(AC)P(BC)​=P(C)2P(AC)P(BC)​=P(A∣C)P(B∣C) 泊松分布与指数分布 泊松分布P(λ)P(\\lambda)P(λ)为离散型分布，其PMF为： P(X=k)=λkk!e−λP(X = k) = \\frac{\\lambda^{k}}{k!}e^{-\\lambda} P(X=k)=k!λk​e−λ 其数字特征为： E(X)=Var(X)=λE(X) = Var(X) = \\lambda E(X)=Var(X)=λ 而指数分布Exp(λ)Exp(\\lambda)Exp(λ)为连续型分布，其PDF为： f(x)=λe−λxf(x) = \\lambda e^{-\\lambda x} f(x)=λe−λx 其数字特征为： E(X)=1λVar(X)=1λ2E(X) = \\frac{1}{\\lambda}\\quad Var(X) = \\frac{1}{\\lambda^{2}} E(X)=λ1​Var(X)=λ21​ 尾概率为： P(X&gt;x)=e−λxP(X &gt; x) = e^{-\\lambda x} P(X&gt;x)=e−λx 指数分布与泊松分布为对同一事件的不同描述，指数分布为两次发生这一事件之间的时间间隔（连续），泊松分布为固定时间段内发生事件的次数（离散） 全期望公式 E(Y)=E(E(Y ∣ X))E(E(g(X)Y ∣ X))=E(g(X)E(Y∣X))\\begin{align*} E(Y) &amp;= E(E(Y\\,|\\, X)) \\\\ E(E(g(X)Y\\,|\\, X)) &amp;= E(g(X)E(Y|X)) \\end{align*} E(Y)E(E(g(X)Y∣X))​=E(E(Y∣X))=E(g(X)E(Y∣X))​ 条件期望是均方误差意义下的最优预测，即∀ g\\forall\\, g∀g： E((Y−g(X))2)≥E((Y−E(Y∣X))2)E((Y - g(X))^{2})\\geq E((Y - E(Y|X))^{2}) E((Y−g(X))2)≥E((Y−E(Y∣X))2) 矩母函数与矩 nnn阶原点矩为矩母函数的nnn阶导数 E(Xn)=MX(n)(0)E(X^{n}) = M_{X}^{(n)}(0) E(Xn)=MX(n)​(0) 相对应的标准矩为： E((X−μ)n)=∑k=0nCnkE(Xk)μn−k=∑k=0nCnkMX(n)(0)μn−k\\begin{align*} E((X - \\mu)^{n}) &amp;= \\sum\\limits_{k=0}^{n}C_{n}^{k}E(X^{k})\\mu^{n-k} \\\\ &amp;= \\sum\\limits_{k=0}^{n}C_{n}^{k}M^{(n)}_{X}(0)\\mu^{n-k} \\end{align*} E((X−μ)n)​=k=0∑n​Cnk​E(Xk)μn−k=k=0∑n​Cnk​MX(n)​(0)μn−k​ 概率不等式 Markov 若随机变量X≥0X\\geq 0X≥0，则∀ a&gt;0\\forall\\, a &gt; 0∀a&gt;0 P(X≥a)≤E(X)aP(X \\geq a) \\leq \\frac{E(X)}{a} P(X≥a)≤aE(X)​ Chebyshev 若随机变量XXX方差存在，则： P(∣X−E(X)∣≥a)≤Var(X)a2P(|X - E(X)| \\geq a) \\leq \\frac{Var(X)}{a^{2}} P(∣X−E(X)∣≥a)≤a2Var(X)​ Chernoff XXX任意，则∀a&gt;0,t&gt;0\\forall a&gt;0, t&gt;0∀a&gt;0,t&gt;0 P(X≥a)≥E(etX)etaP(X \\geq a) \\geq \\frac{E(e^{tX})}{e^{ta}} P(X≥a)≥etaE(etX)​ Hoeffding bound 随机变量列Xi∈[ai,bi]X_{i} \\in [a_{i}, b_{i}]Xi​∈[ai​,bi​]，记X=∑i=1nXiX = \\sum\\limits_{i=1}^{n} X_{i}X=i=1∑n​Xi​，并记μ=E(X)\\mu = E(X)μ=E(X)，则： P(X≤μ−t)≤exp⁡(−2t2∑i=1n(ai−bi)2)P(X≥μ+t)≤exp⁡(−2t2∑i=1n(ai−bi)2)\\begin{align*} P(X \\leq \\mu - t) &amp;\\leq \\exp\\biggl(-\\frac{2t^{2}}{\\sum\\limits_{i=1}^{n}(a_{i} - b_{i})^{2}}\\biggr) \\\\ P(X \\geq \\mu + t) &amp;\\leq \\exp\\biggl(-\\frac{2t^{2}}{\\sum\\limits_{i=1}^{n}(a_{i} - b_{i})^{2}}\\biggr) \\end{align*} P(X≤μ−t)P(X≥μ+t)​≤exp(−i=1∑n​(ai​−bi​)22t2​)≤exp(−i=1∑n​(ai​−bi​)22t2​)​ Multiplicative-form Chernoff Bound 随机变量列Xi∈[0,1]X_{i} \\in [0, 1]Xi​∈[0,1]，记X=∑i=1nXiX = \\sum\\limits_{i=1}^{n} X_{i}X=i=1∑n​Xi​，并记μ=E(X)\\mu = E(X)μ=E(X)，则： P(X≤(1−ε)μ)≤exp⁡(−ε22μ)P(X≥(1+ε)μ)≤exp⁡(−ε22+εμ)\\begin{align*} P(X \\leq (1 - \\varepsilon)\\mu) &amp;\\leq \\exp\\bigl(-\\frac{\\varepsilon^{2}}{2}\\mu\\bigr) \\\\ P(X \\geq (1 + \\varepsilon)\\mu) &amp;\\leq \\exp\\bigl(-\\frac{\\varepsilon^{2}}{2 + \\varepsilon}\\mu\\bigr) \\end{align*} P(X≤(1−ε)μ)P(X≥(1+ε)μ)​≤exp(−2ε2​μ)≤exp(−2+εε2​μ)​ 收敛性的差异 Ω∼U(0,1)\\Omega \\sim U(0, 1)Ω∼U(0,1)，则考虑如下随机变量列： Y0(ω)=ω+1[0,1](ω)Y1(ω)=ω+1[0,12](ω)Y2(ω)=ω+1[12,1](ω)Y3(ω)=ω+1[0,13](ω)Y4(ω)=ω+1[13,23](ω)Y5(ω)=ω+1[23,1](ω)…\\begin{align*} Y_{0}(\\omega) &amp;= \\omega + \\mathrm{1}_{[0, 1]}(\\omega) \\\\ Y_{1}(\\omega) &amp;= \\omega + \\mathrm{1}_{[0, \\frac{1}{2}]}(\\omega) \\\\ Y_{2}(\\omega) &amp;= \\omega + \\mathrm{1}_{[\\frac{1}{2}, 1]}(\\omega) \\\\ Y_{3}(\\omega) &amp;= \\omega + \\mathrm{1}_{[0, \\frac{1}{3}]}(\\omega) \\\\ Y_{4}(\\omega) &amp;= \\omega + \\mathrm{1}_{[\\frac{1}{3}, \\frac{2}{3}]}(\\omega) \\\\ Y_{5}(\\omega) &amp;= \\omega + \\mathrm{1}_{[\\frac{2}{3}, 1]}(\\omega) \\\\ \\dots \\end{align*} Y0​(ω)Y1​(ω)Y2​(ω)Y3​(ω)Y4​(ω)Y5​(ω)…​=ω+1[0,1]​(ω)=ω+1[0,21​]​(ω)=ω+1[21​,1]​(ω)=ω+1[0,31​]​(ω)=ω+1[31​,32​]​(ω)=ω+1[32​,1]​(ω)​ 记Y(ω)=ωY(\\omega) = \\omegaY(ω)=ω，则Yn(ω)Y_{n}(\\omega)Yn​(ω)依概率收敛至Y(ω)Y(\\omega)Y(ω)，但是不以概率111收敛 中心极限定理连续性修正 由于常规的中心极限定理是： X‾−μσ/n→N(0,1)\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}\\rightarrow N(0, 1) σ/n​X−μ​→N(0,1) 因此一些离散分布使用该定理之后反而会失去其单点的概率（连续分布单点恒为0），因此进行连续性修正，以二项分布X∼B(n,p)X \\sim B(n, p)X∼B(n,p)为例： P(t1≤X≤t2)≈Φ(y2)−Φ(y1)P\\bigl(t_{1}\\leq X \\leq t_{2}\\bigr) \\approx \\Phi(y_{2}) - \\Phi(y_{1}) P(t1​≤X≤t2​)≈Φ(y2​)−Φ(y1​) 其中： Φ(yi)=ti−np+(12)inp(1−p)\\Phi(y_{i}) = \\frac{t_{i} - np + (\\frac{1}{2})^{i}}{\\sqrt{np(1-p)}} Φ(yi​)=np(1−p)​ti​−np+(21​)i​ 极大似然估计可能有偏 均匀分布U(0,θ)U(0, \\theta)U(0,θ)，样本值为{Xi}i=1n\\{X_{i}\\}_{i=1}^{n}{Xi​}i=1n​，则其MLE为θ∗=max⁡{Xi}\\theta^{*} = \\max\\{X_{i}\\}θ∗=max{Xi​}，下面我们证明这个不是无偏估计： Y=max⁡{Xi}Y = \\max\\{X_{i}\\}Y=max{Xi​}的CDF为： FY(y)=P(max⁡{Xi}≤y)=(FX(y))n=(yθ)n\\begin{align*} F_{Y}(y) &amp;= P(\\max\\{X_{i}\\} \\leq y) \\\\ &amp;= (F_{X}(y))^{n} \\\\ &amp;= \\bigl(\\frac{y}{\\theta}\\bigr)^{n} \\end{align*} FY​(y)​=P(max{Xi​}≤y)=(FX​(y))n=(θy​)n​ 因此其PDF为f(y)=FY′(y)=nθ(yθ)n−1f(y) = F&#x27;_{Y}(y) = \\frac{n}{\\theta}(\\frac{y}{\\theta})^{n-1}f(y)=FY′​(y)=θn​(θy​)n−1 因此我们有： E(θ∗)=∫0θyf(y)dy=nθn∫0θyndy=nn+1θE(\\theta^{*}) = \\int_{0}^{\\theta}yf(y)dy = \\frac{n}{\\theta^{n}}\\int_{0}^{\\theta}y^{n}dy = \\frac{n}{n+1}\\theta E(θ∗)=∫0θ​yf(y)dy=θnn​∫0θ​yndy=n+1n​θ 也即θ∗\\theta^{*}θ∗并不是θ\\thetaθ的无偏估计 无偏MSE不一定优于有偏 X∼N(μ,σ2)X \\sim N(\\mu, \\sigma^{2})X∼N(μ,σ2)，分别用二阶矩m2m_{2}m2​和样本方差S2S^{2}S2来估计σ2\\sigma^{2}σ2，有： E(m2)=n−1nσ2E(S2)=σ2\\begin{align*} E(m_{2}) &amp;= \\frac{n-1}{n}\\sigma^{2} \\\\ E(S^{2}) &amp;= \\sigma^{2} \\\\ \\end{align*} E(m2​)E(S2)​=nn−1​σ2=σ2​ 但是： E((m2−σ2)2)&lt;E((S2−σ2)2)E((m_{2} - \\sigma^{2})^{2}) &lt; E((S^{2} - \\sigma^{2})^{2}) E((m2​−σ2)2)&lt;E((S2−σ2)2) 区间估计 标准正态 如X∼N(μ,σ2)X \\sim N(\\mu, \\sigma^{2})X∼N(μ,σ2)，其中μ\\muμ未知而σ2\\sigma^{2}σ2已知，则： X‾−μσ/n∼N(0,1)\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1) σ/n​X−μ​∼N(0,1) 因此(1−α)(1 - \\alpha)(1−α)置信区间为： (X‾−zα/2σn,X‾+zα/2σn)(\\overline{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\overline{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}) (X−zα/2​n​σ​,X+zα/2​n​σ​) 其中zα2z_{\\frac{\\alpha}{2}}z2α​​为标准正态分布上α2\\frac{\\alpha}{2}2α​分位数 ttt分布 在上述例子中，如果σ2\\sigma^{2}σ2未知，则应利用ttt分布来进行区间估计，具体来说： X‾−μσ/n∼N(0,1)(n−1)S2σ2∼χ2(n−1)\\begin{align*} \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} &amp;\\sim N(0, 1) \\\\ \\frac{(n-1)S^{2}}{\\sigma^{2}} &amp;\\sim \\chi^{2}(n-1) \\end{align*} σ/n​X−μ​σ2(n−1)S2​​∼N(0,1)∼χ2(n−1)​ 因此有： X‾−μS/n∼t(n−1)\\frac{\\overline{X} - \\mu}{S / \\sqrt{n}} \\sim t(n-1) S/n​X−μ​∼t(n−1) (1−α)(1-\\alpha)(1−α)置信区间为： (X‾−tα/2(n−1)Sn,X‾+tα/2(1−α)Sn)(\\overline{X} - t_{\\alpha/2}(n-1)\\frac{S}{\\sqrt{n}}, \\overline{X} + t_{\\alpha/2}(1-\\alpha)\\frac{S}{\\sqrt{n}}) (X−tα/2​(n−1)n​S​,X+tα/2​(1−α)n​S​) 同样估计均值差也可以使用ttt分布：X∼N(μ1,σ12)X\\sim N(\\mu_{1}, \\sigma_{1}^{2})X∼N(μ1​,σ12​)，Y∼N(μ2,σ22)Y\\sim N(\\mu_{2}, \\sigma_{2}^{2})Y∼N(μ2​,σ22​)，则μ1−μ2\\mu_{1} - \\mu_{2}μ1​−μ2​的估计方法为： (X‾−Y‾)−(μ1−μ2)(1n+1m)(n−1)S12+(m−1)S22n+m−2∼t(n+m−2)\\frac{(\\overline{X} - \\overline{Y}) - (\\mu_{1} - \\mu_{2})}{\\sqrt{(\\frac{1}{n} + \\frac{1}{m})\\frac{(n-1)S_{1}^{2} + (m-1)S_{2}^{2}}{n+m-2}}} \\sim t(n + m - 2) (n1​+m1​)n+m−2(n−1)S12​+(m−1)S22​​​(X−Y)−(μ1​−μ2​)​∼t(n+m−2) F分布 常用在估计两正态总体方差之比上，所依赖的分布为： S12/σ12S22/σ22∼F(n−1,m−1)\\frac{S_{1}^{2}/\\sigma_{1}^{2}}{S_{2}^{2}/\\sigma_{2}^{2}} \\sim F(n - 1, m - 1) S22​/σ22​S12​/σ12​​∼F(n−1,m−1) 渐进估计 利用中心极限定理得到标准正态分布利用S2S^{2}S2或m2m_{2}m2​等方式来估计σ2\\sigma^{2}σ2 极大似然与Fisher θ∗−θ1nI(θ∗)→N(0,1)\\frac{\\theta^{*} - \\theta}{\\sqrt{\\frac{1}{nI(\\theta^{*})}}} \\rightarrow N(0, 1) nI(θ∗)1​​θ∗−θ​→N(0,1) 其中I(θ)I(\\theta)I(θ)为Fisher信息量，具体来说： I(θ)=E((∂ log⁡ f∂θ)2)I(\\theta) = E\\biggl(\\bigl(\\frac{\\partial\\,\\log\\,f}{\\partial \\theta}\\bigr)^{2}\\biggr) I(θ)=E((∂θ∂logf​)2)","tags":["笔记","概统","复习"],"categories":["概率论与数理统计"]},{"title":"IAI-搜索","path":"/2024/06/09/IAI-搜索/","content":"人智导 搜索 搜索问题 盲目搜索过于困难的时候，采用一些方法来帮助我们加快搜索进程 深度优先 优先扩展深度更深的节点，略 性质： 一般不能保证最优解 深度限制不合理时可能找不到解，并且可能退化为穷举 是一个通用的算法，并且相对节省内存 宽度优先 优先扩展深度更浅的节点，略 性质： 问题有解的时候一定能找到，如果问题是单位耗散值则一定能找到最优解 通用算法，但是效率较低，存储量较大 Dijkstra 略 优劣： 有解一定可以找到最佳解 只考虑了距离起点的具体 启发式图搜索 引入启发知识，用于评估节点到达目标的距离，尽可能减少搜索范围 A算法 评价函数为： f(n)=g(n)+h(n)f(n) = g(n) + h(n) f(n)=g(n)+h(n) 其中g(n)g(n)g(n)是点nnn到起点sss的耗散值估计值，h(n)h(n)h(n)是点nnn到目标ttt的耗散值估计值，真实值分别为g∗(n)g^{*}(n)g∗(n)与h∗(n)h^{*}(n)h∗(n) AAA算法伪代码如下，主要为维护CLOSE\\text{CLOSE}CLOSE表与OPEN\\text{OPEN}OPEN表： CLOSE=(), OPEN=(s)\\text{CLOSE} = (),\\, \\text{OPEN} = (s)CLOSE=(),OPEN=(s)123456789101112131415161718192021while OPEN is not empty, do: n = OPEN.first() if n == g return n OPEN.pop(n) CLOSE.push(n) expand(n) for child in n.childs() if child in OPEN: # 标记为m_&#123;k&#125; f(child) = min&#123;f(child), f(n) + d(n, child) + h(child)&#125; else if child in CLOSE: # 标记为m_&#123;l&#125; f(child) = min&#123;f(child), f(n) + d(n, child) + h(child)&#125; ### important! OPEN.push(child) if f(child) change else: # 标记为m_&#123;j&#125; f(child) = f(n) + d(n, child) + h(child) OPEN.push(child) sort(OPEN, f) 最终从目标开始顺次访问父节点即可 A*算法 若AAA算法中的启发函数满足条件h(n)≤h∗(n)h(n)\\leq h^{*}(n)h(n)≤h∗(n)，则得到A∗A^{*}A∗算法 放宽了约束条件得到估计值，A∗A^{*}A∗算法的优势在与： 一定能找到最优解 启发信息越少的算法，所拓展的节点越多（不少于信息更多的算法） 对启发函数的评价方式为平均分叉数b∗b^{*}b∗，探索ddd层后其与总搜索节点数NNN的关系为： N=∑i=0db∗d=1−b∗(d+1)1−b∗N = \\sum\\limits_{i=0}^{d}b^{*d} = \\frac{1 - b^{*(d+1)}}{1 - b^{*}} N=i=0∑d​b∗d=1−b∗1−b∗(d+1)​ 一般情况下，b∗b^{*}b∗是与问题相关，而与问题的规模关系不大 改进的A*算法 对于A∗A^{*}A∗算法，其最大的问题就是CLOSE表中的节点可能再次进入OPEN表中，也即没有在第一次探索到节点的时候就找到其最短路径，会造成重复探索，导致资源的浪费。因此我们需要对启发函数做出如下限制： 若启发函数hhh满足，对于节点vvv与其父节点uuu有：h(u)−h(v)≤d(u,v)h(u) - h(v) \\leq d(u, v) h(u)−h(v)≤d(u,v)并且有h(t)=0h(t) = 0h(t)=0，则称hhh是单调的若hhh是单调的，那么扩展了节点nnn之后就已经找到了到达nnn的最优路径，并且单调的hhh一定满足A∗A^{*}A∗条件 改进后有： 扩展的节点一定满足f(n)≤f∗(s)f(n) \\leq f^{*}(s)f(n)≤f∗(s) OPEN表中满足f(n)&lt;f∗(s)f(n) &lt; f^{*}(s)f(n)&lt;f∗(s)的一定会被扩展 再次改进，使用当前被扩展的最大节点来估计f∗(s)f^{*}(s)f∗(s)，即max⁡(f(n))∼f∗(s)\\max(f(n)) \\sim f^{*}(s)max(f(n))∼f∗(s)，具体来说： 维护一个NEST表，满足NEST={ni ∣ f(ni)&lt;fm,ni∈OPEN}\\mathrm{NEST} = \\{n_{i}\\,|\\,f(n_{i}) &lt; f_{m}, n_{i}\\in \\text{OPEN}\\}NEST={ni​∣f(ni​)&lt;fm​,ni​∈OPEN}，并且在选择的时候优先选择NEST表中ggg最小的节点，如果NEST空了再去OPEN表里面选择，并更新fmf_{m}fm​ 这种情况下仍可能会导致重复探索的出现！ 其他搜索算法 例如爬山法，随机搜索算法，动态规划等，h(n)=0h(n) = 0h(n)=0的时候A∗A^{*}A∗算法就变成了动态规划（？ Viterbi算法 Viterbi算法实例 转移方程为： Q(Wij)={min⁡k(Q(W(i−1)k)+D(W(i−1)k,Wij))i≠00i=0\\begin{align*} Q(W_{ij}) = \\begin{cases} \\min\\limits_{k}(Q(W_{(i-1)k}) + D(W_{(i-1)k, W_{ij}})) &amp; i eq 0 \\\\ 0 &amp; i = 0 \\end{cases} \\end{align*} Q(Wij​)={kmin​(Q(W(i−1)k​)+D(W(i−1)k,Wij​​))0​i=0i=0​​","tags":["笔记","IAI","对抗搜索"],"categories":["人工智能导论"]},{"title":"TCS-Lecture-E","path":"/2024/06/05/TCS-Lecture-E/","content":"TCS: Propositions as Types 本文含有大量复杂的LaTeX\\LaTeXLATE​X公式，因此md文档效果不好，仅供参考！ Propositions as Types Intuitionistic Logic and Natural Deduction Intuitionistic logic insists upon a constructionist notion of truth. In particular, a proof of A∨BA\\lor BA∨B must show which of AAA or BBB holds (different from classic logic). Natural Deduction Proof rules should come in pairs: Introduction rules which are like definitions of the logic operators. Elimination rules which are the consequences of theirdefinitions. Both of them should be in harmony. We will define the four kinds of rules, conjunction, implication, disjunction and falsehood in order. Gentzen’s contribution: Subformula principle: Proofs can be normalized so that no concepts enter the proof other than those contained in the final result (subformula). This means we can use only the known proposition and the subformulas of the conclusion. Guiding Question for Defining Logic Martin-Löf: The meaning of a proposition is determined by what counts as a verification (or proof) of it. Proposition: AAA Justification: A trueA \\text{ true}A true, A is true at time tA \\text{ is true at time } tA is true at time t, …\\dots… So that we need to determin a proposition by a verification(not only itself). In this lecture, we only consider justification A trueA \\text{ true}A true so we omit thet suffix true\\text{true}true Conjunction We use ‘→\\to→’ to represents the prooftree in PDF. (∧I)(A,B)→(A∧B)(∧E1)(A∧B)→A(∧E2)(A∧B)→B\\begin{align*} (\\land I)\\qquad &amp;(A, B) \\to (A\\land B) \\\\ (\\land E_{1})\\qquad &amp;(A\\land B) \\to A \\\\ (\\land E_{2})\\qquad &amp;(A\\land B) \\to B \\\\ \\end{align*} (∧I)(∧E1​)(∧E2​)​(A,B)→(A∧B)(A∧B)→A(A∧B)→B​ We define the local soundness and local completeness as follows: Local Soundness: The elimination cannot give us new information.((D→A), (E→B))→(A∧B)→A⇒R(D→A)\\begin{align*}&amp;\\quad((\\mathcal{D} \\to A),\\, (\\mathcal{E} \\to B)) \\to (A\\land B) \\to A \\\\&amp;\\Rightarrow_{R} (\\mathcal{D}\\to A)\\end{align*}​((D→A),(E→B))→(A∧B)→A⇒R​(D→A)​Local Completeness: The elimination can reconstitute the proof by introduction(D→A)⇒E(((D→(A∧B))→A), ((D→(A∧B))→B))→(A∧B)→A\\begin{align*} &amp;\\quad (\\mathcal{D}\\to A)\\\\ &amp;\\Rightarrow_{E}(((\\mathcal{D} \\to (A\\land B)) \\to A),\\, ((\\mathcal{D} \\to (A\\land B)) \\to B)) \\to (A\\land B) \\to A\\end{align*}​(D→A)⇒E​(((D→(A∧B))→A),((D→(A∧B))→B))→(A∧B)→A​ Implication We define the hypothetical judgment at first: (A∧(B∧C))⊢B(A\\land (B\\land C)) \\vdash B (A∧(B∧C))⊢B This is not an inference as it consists of two inferences! We have conjunctions inferences at now. Then we can define implication introduction and elimination rules: (⊃Ix)(A‾x→∗B)→(A⊃B)(⊃E)(A⊃B, A)→B\\begin{align*} (\\supset I^{x})\\qquad &amp;(\\overline{A}^{x} \\to^{*} B) \\to (A\\supset B) (\\supset E)\\qquad &amp; (A\\supset B,\\, A) \\to B \\end{align*} (⊃Ix)​(Ax→∗B)→(A⊃B)(⊃E)​(A⊃B,A)→B​ In which that the x^{x}x means the local scope. Disjunction Define the disjunction as follows: (∨I1)A→(A∨B)(∨I2)B→(A∨B)(∨Ex,y)(A∨B,A‾x→∗C,B‾y→∗C)→C\\begin{align*} (\\lor I_{1})\\qquad &amp;A \\to (A\\lor B) \\\\ (\\lor I_{2})\\qquad &amp;B \\to (A\\lor B) \\\\ (\\lor E^{x, y})\\qquad &amp;(A\\lor B, \\overline{A}^{x}\\to^{*}C, \\overline{B}^{y}\\to^{*}C) \\to C \\end{align*} (∨I1​)(∨I2​)(∨Ex,y)​A→(A∨B)B→(A∨B)(A∨B,Ax→∗C,By→∗C)→C​ Falsehood Falsehood is written as ⊥\\bot⊥. It has no introduction as we shouldn’t prove it. (⊥E)⊥→C(\\bot E) \\bot \\to C (⊥E)⊥→C Intuitionistic Natural Deduction So, we can use these four to define some other operates such as ¬ eg¬, ⊤\\top⊤ and ↔\\leftrightarrow↔ ¬A=defA⊃⊥⊤=def⊥⊃⊥A↔B=def(A⊃B)∧(B⊃A)\\begin{align*} eg A &amp;\\mathop{=}\\limits^{\\text{def}} A \\supset \\bot \\\\ \\top &amp;\\mathop{=}\\limits^{\\text{def}} \\bot \\supset \\bot \\\\ A \\leftrightarrow B &amp;\\mathop{=}\\limits^{\\text{def}} (A \\supset B) \\land (B \\supset A) \\end{align*} ¬A⊤A↔B​=defA⊃⊥=def⊥⊃⊥=def(A⊃B)∧(B⊃A)​ Now we can begin prove! The follows is some interesting conclusion: A⊃B⊃AA⊃¬¬A(A⊃B)⊃(¬B⊃¬A)¬(A∨B)↔(¬A∧¬B)\\begin{align} A \\supset &amp;B \\supset A \\\\ A \\supset &amp; eg eg A \\\\ (A \\supset B) \\supset &amp;( eg B \\supset eg A) \\\\ eg (A \\lor B) \\leftrightarrow &amp;( eg A \\land eg B) \\end{align} A⊃A⊃(A⊃B)⊃¬(A∨B)↔​B⊃A¬¬A(¬B⊃¬A)(¬A∧¬B)​​ But some weird things hapend as the following is not provable: ((A⊃B)⊃A)⊃A¬¬A⊃A(¬A⊃¬B)⊃(B⊃A)¬(A∧B)↔(¬A∨¬B)\\begin{align} ((A \\supset B) \\supset A) &amp;\\supset A \\\\ eg eg A &amp;\\supset A \\\\ ( eg A \\supset eg B) &amp;\\supset (B \\supset A) \\\\ eg (A \\land B) \\leftrightarrow &amp;( eg A \\lor eg B) \\end{align} ((A⊃B)⊃A)¬¬A(¬A⊃¬B)¬(A∧B)↔​⊃A⊃A⊃(B⊃A)(¬A∨¬B)​​ What the f***! The most interesting is that: (Gilvenko’s Theorem):, AAA is valid in classical logic if and only if ¬¬A eg eg A¬¬A is valid in intuitionistic logic! And intuitionistic logic is PSPACE-complete! Propositions as Types We write M:AM: AM:A to represent that MMM is a proof of AAA and alternatively MMM is a program of type AAA. So we can rewrite the four kinds of roles by λ\\lambdaλ-calculus! Implication (⊃Ix)((x:A‾x)→(M:B))→(λx.M):(A⊃B)(⊃E)((M:(A⊃B)),(N:A))→(MN:B)\\begin{align*} (\\supset I^{x})\\qquad &amp;((\\overline{x:A}^{x})\\to (M:B)) \\to (\\lambda x.M):(A\\supset B) \\\\ (\\supset E)\\qquad &amp;((M:(A\\supset B)), (N:A)) \\to (MN: B) \\end{align*} (⊃Ix)(⊃E)​((x:Ax)→(M:B))→(λx.M):(A⊃B)((M:(A⊃B)),(N:A))→(MN:B)​ From its local soundness we can get β\\betaβ reduction, and from local completeness we can get η\\etaη rule! Turing has proved that typed λ\\lambdaλ-calculus always terminates. Conjunction (∧I)((M:A),(N:B))→(⟨M,N⟩:A∧B)(∧E1)(M:(A∧B))→((fstM):A)(∧E2)(M:(A∧B))→((sndM):B)\\begin{align*} (\\land I)\\qquad((M:A), (N:B)) &amp;\\to (\\langle M, N\\rangle: A\\land B) \\\\ (\\land E_{1})\\qquad (M:(A\\land B)) &amp;\\to ((\\mathbf{fst}M):A) \\\\ (\\land E_{2})\\qquad (M:(A\\land B)) &amp;\\to ((\\mathbf{snd}M):B) \\end{align*} (∧I)((M:A),(N:B))(∧E1​)(M:(A∧B))(∧E2​)(M:(A∧B))​→(⟨M,N⟩:A∧B)→((fstM):A)→((sndM):B)​ The local reduction and expansion can give out the meaning of ⟨⋅,⋅⟩\\langle \\cdot, \\cdot\\rangle⟨⋅,⋅⟩, fst\\mathbf{fst}fst and snd\\mathbf{snd}snd Disjunction (∨I1)(M:A)→((inlM):A∨B)(∨I2)(M:B)→((inrM):A∨B)(∨Ex,y)(M:(A∨B),(x:A‾x→(N:C)),(y:B‾y→(P:C)))→case(M,x.N,y.P):C\\begin{align*} (\\lor I_{1})\\qquad (M:A) &amp;\\to ((\\mathbf{inl}M): A\\lor B) \\\\ (\\lor I_{2})\\qquad (M:B) &amp;\\to ((\\mathbf{inr}M): A\\lor B) \\\\ (\\lor E^{x,y}) \\qquad (M:(A\\lor B), &amp;(\\overline{x:A}^{x} \\to (N:C)), (\\overline{y:B}^{y} \\to (P:C))) \\\\ &amp;\\to \\mathbf{case}(M, x.N, y.P):C \\end{align*} (∨I1​)(M:A)(∨I2​)(M:B)(∨Ex,y)(M:(A∨B),​→((inlM):A∨B)→((inrM):A∨B)(x:Ax→(N:C)),(y:B​y→(P:C)))→case(M,x.N,y.P):C​ Falsehood No introduction rule. (⊥E)(M:⊥)→(abortM:C)(\\bot E)\\qquad(M:\\bot) \\to (\\mathbf{abort}M:C) (⊥E)(M:⊥)→(abortM:C)","tags":["命题","类型","笔记","TCS"],"categories":["理论计算机科学导引"]},{"title":"网原笔记7","path":"/2024/06/05/网原笔记7/","content":"计算机网络原理 笔记 7 无线网络和移动网络 概述 无线网络概述 在无线网络中，体系结构如下： 无线主机：端系统 无线链路：主机通过无线通信链路连接到基站或主机，不同无线链路具有不同的传输速率与覆盖距离 基站：负责协调与之相关联的多个主机之间的数据传输 网络基础设施：无线设备希望与之通信的网络 与基站相关联的主机称为以基础设施模式运行，所有网络服务有基站向主机提供，而自组织网络，主机向自身提供各种服务 由于无线主机可以移动，因此会出现切换现象，即改变与之相关联的基站 无线网络的分类一般是按照无线跳的数目和是否有基础设施（如基站）来共同决定的 基于基础设施 无基础设施 单跳 具有一个与较大有线网络连接的基站如802.11网络、4G LTE等 常用于协调其他节点的传输如蓝牙、自组织 多跳 无线节点为了与更大网络通信需要经过多个无线节点例如无线网状网络 没有基站，每个节点为了通信需要多个节点中继例如MANET或VANET 无线网络特征 相比于有线网络，无线网络有很多不同，例如： 信号强度递减：使用电磁波传输会导致信号的减弱，成为路径损耗 不同源之间会有干扰：在同一个频段中发送信号的源、环境噪声等都会相互干扰 多径传播：电磁波的不同部分在传播途中经过了不同的路径，导致信号模糊（例如经过了多次反射） 相干时间 在多径传播中，相干时间之信道中两次接收到预想信号的时间差，会影响到最大传输速率 相干时间示意图 取决于发送频率与接受者的速度 如果接受者不动则在一定时间后影响会被消除 噪声 由于无线链路非常容易出现差错，因此采用了CRC与ARQ结合的方式 对于主机，其接受到的信号事实上是退化后的初始信号与环境噪声的结合，我们使用信噪比来测量相对污染程度，信噪比越大，更容易提取出有效的信息，同时我们使用比特差错率来衡量接收方收到错误比特概率 不同的调制编码方式对于信号的传输也有影响，如下图 SNR、BER与调制编码方式 主要趋势为： 调制方案给定，BER与SNR成负相关（发送方增加传输速率） SNR给定：BER与调制方案的比特传输率成正相关 物理层调制技术可以动态选择，用于适配信道条件 其它问题 隐藏终端与衰减同样是无线传输中的两个重要问题，隐藏终端是是指两个终端相互之间不可见（被物理阻隔），但是其发送的信号在另一个终端有干扰；衰减很好理解 隐藏终端与衰减 CDMA 在无线领域使用非常广泛的协议，用于避免信号的相互干扰，举例来说如下 码分多址实例 CDMA将每个要发送的bit与编码bit想成来进行编码，编码bit以一个相当高的速率（码片速率）在变化，相当于将一个bit的信息编码为MMMbits的信息序列 理想状态下，编码为： Zi={Zim}∣1≤m≤M={di⋅cm}∣1≤m≤MZ_{i} = \\{Z_{im}\\}|_{1\\leq m \\leq M} = \\{d_{i}\\cdot c_{m}\\}|_{1\\leq m \\leq M} Zi​={Zim​}∣1≤m≤M​={di​⋅cm​}∣1≤m≤M​ 解码为： di=1M∑i=1MZim⋅cmd_{i} = \\frac{1}{M}\\sum\\limits_{i=1}^{M}Z_{im}\\cdot c_{m} di​=M1​i=1∑M​Zim​⋅cm​ 然而当有N&gt;1N &gt; 1N&gt;1个发送方时，接收方作加性处理，即： Zim∗=∑j=1NZimjZ^{*}_{im} = \\sum\\limits_{j=1}^{N}Z^{j}_{im} Zim∗​=j=1∑N​Zimj​ 而解码过程无需变化，在合适的CDMA编码的情况下，仍能辨别出所需要提取的信息 WiFi: 802.11无线LAN 多路访问使用CSMA/CA，有基站版和自组织版 体系结构 体系结构与自组织网络版 基本单位为基本服务集，一个BSS中包含若干个无线站点和一个接入点（中央基站），该种模式的WLAN称之为基础设施WLAN，其中基础设施指的是AP与其连接的有线网 每一个802.11无线站点和AP有一个6字节MAC地址，由IEEE\\text{IEEE}IEEE管理 同样可能存在自组织网络，相当于不需要与外部通信的情况下交换数据 信道与关联 802.11运行在2.4∼2.4835GHz2.4\\sim 2.4835\\text{GHz}2.4∼2.4835GHz的频段中，并将这个频段划分为了11个部分重叠的信道，当且仅当两个新岛之间距离超过4的时候才无重叠 每个AP会被分配一个单字或双字的服务集标识与一个信道号 WiFi丛林指的是一个物理位置，在此处无线站点可以收到多个AP的信号，因此我们需要和某一个AP进行关联，即在其间建立一条虚拟线路 为了了解到丛林中的AP，每个AP会周期性的发送信标帧，包括其SSID与MAC地址，站点通过扫描信道得知当前的AP，称之为被动扫描 指定关联AP没有相关准则，由软件方处理 同样的，站点可以主动向AP发送探测请求帧并期待AP回复探测响应帧，称之为主动扫描 主动扫描与被动扫描 确定了想要关联的AP后，站点发送关联请求帧，AP回复关联响应帧，之后将其加入AP子网中 802.11 MAC协议 采用CDMA/CA，由于无线设备的特殊性，信道的碰撞难以被检测（甚至有一部分无法被检测），因此采用避免碰撞的方式，一旦站点开始发送帧，就完整的发送 接收到信号的强度远小于发送的信号，制造检测碰撞设备代价高 不一定检测到所有碰撞，如隐藏终端和衰减 CSMA/CA协议如下： 站点监听到信道空闲，等待DIFS时间并完整发送帧 否则，选取一个随机回退值，在侦听到空闲时递减，在侦听到忙时不变 回退值减到0时，完整发送帧 等待ACK，若收到，则发送下一帧时从第二步开始 反之重新进入第二步并增大回退值 其与CSMA/CD最大的不同点在于，在侦听到空闲的时候不立即发送帧，而是等待回退值降到0，因此需要期待不同站点之间的回退值有一定距离 链路层确认 处理隐藏终端 采用RTS与CTS，即一个站点发送数据之前首先发送一个RTS控制帧，指示其所需要的总时间，AP接收到后广播CTS控制帧，告诉发送方可以发送，抑制其他的站点发送 处理隐藏终端 优势为： 有效解决隐藏终端 RTS和CTS的碰撞很短，对性能影响不大 用作点对点链路 略 IEEE 802.11帧 802.11帧示例 重要字段为： 有效载荷与CRC：通常包括IP数据报或者ARP分组 地址字段：共有四个，包括 地址1：目的地站点的MAC地址 地址2：源站点的MAC地址 地址3：该BSS所在子网对应的路由器接口MAC地址 地址4：用于自组织网络中互联，略 序号：用于区分同一个帧的不同副本（区分新帧与重传的帧） 持续期：预约享有信道的时间（RTS与CTS） 帧控制：很复杂，具体来说 类型：区分RTS、CTS、ACK与数据帧 到和从：定义地址字段含义 WEP：加密指示 相同IP子网中的移动性 在同一个子网下的多个BSS，TCP会话如何在其间丝滑切换？ 由于BSS在同一IP子网下，因此其IP地址不会发生变化，并且交换机可以通过自学习来改变与站点相连接的AP的端口MAC 高级特色 速率适应：根据信道特点来选择物理层调制技术，如果连续多帧没有收到ACK则降速，反之（或降速达到一定时间后）增速 功率管理：节点可以向AP发送信号表明自己进入睡眠状态，AP缓存发送到对应站点的帧。 节点在信标帧到来之前醒来，通过检测信标帧（包括所有被缓存帧的目标站点列表）确认是否有信号发过来，如果有则发送缓存请求报文，反之继续睡 个人域网络：蓝牙 自组织为一个皮可网，分为主设备、从设备与寄放设备，主设备统筹全局通信，所有通信必须经由主设备，寄放设备是不可活动的 皮可网 其采用TDM，每个时隙625μs625\\mu s625μs，工作在2.4GHz2.4\\text{GHz}2.4GHz，在每个时隙内，发送方利用79个信道中的一个进行传输，并且以不同时隙使用的信道是伪随机的，称之为跳频扩展频谱 蜂窝因特网接入 蜂窝网络与有线网络的同： 设备之间距离很远，但是属于同一个Carrier 全球蜂窝网络是一种网络的网络 广泛使用各种协议 与有线网络互联 蜂窝网络与有线网络的异： 有不同的无线链路层 移动性是第一要求 用户是可区分的（SIM卡） 用户需要向提供商订阅，权威机构提供基础设施 4G：LTE 4G网络架构 其中包括： 移动设备：将订阅者的身份信息ISMI存储在SIM卡中 基站：在服务范围边缘，管理范围内的无线通信资源；管理设备授权，与AP相似 归属订阅者服务：储存有关“归属网络”的信息，用于授权管理 S/P网关：位于数据传输的路径中，P网关是处理蜂窝网络与因特网的交流，提供NAT服务 移动管理实体：与HSS一起管理设备授权；管理设备的交接、位置跟踪等；设置设备到P网关的路径 4G无线网 将每个设备连接到一个基站 可以用很多频带，每个频带内有大量信道，可以区分上行与下行数据 通过OFDM共享信道，速率可以非常高 正交频分复用 图中，每一个PRB是一个传输单元，每一个单元内可以按照时隙与频段划分给不同用户，不同颜色代表不同用户 数据平面与控制平面分离 控制平面：管理移动性、安全与授权 数据平面：管理数据传输 控制平面 LTE中的控制平面有着不同链路层协议，如下： LTE协议1 新增的部分为： 数据收敛：压缩头部信息并加密 无线链路控制：信息的打碎与充足，执行可靠数据传输 中路访问：使用OFDM的需求 LTE协议2 传输规则为： 移动设备将数据传给S网关 S网关传给P网关 优势在与：用户移动的时候只有信道的末端（基站）在变 数据平面 主要是与基站通信，方法为： 基站全频段广播基础同步信号 移动设备找都一个同步信号，之后定位这个频段的第二同步信号（可能有多个基站），并拿到信道的相关信息 之后移动设备挑选基站连接 并且为了节省功率，同样会有睡眠模式，不同的是有两种模式： 浅睡眠：100ms100ms100ms不工作导致，并会时不时醒来检查下行传输 深睡眠：555-10s10s10s不动作导致，可能切换蜂窝 全球蜂窝网络 全IP连接，连接归属网络与被访网络，每个设备的SIM卡中存储了归属网络中的全球性身份信息，可以直接 与归属网络通信，也可以和被访网络进行漫游 5G 适用场景：大信息交流、高可信低延迟交流、增强移动宽带 新无线的新点： 两个频带：毫米波频率 不兼容4G 大量有线天线 毫米波有更高数据传输速率，但是传输距离更短 5G架构 用户直接向与数据中心相连的基站进行通信，数据中心会有分层，中心-边缘-远边缘 5G数据中心图示 移动性 移动性准则 移动性光谱 主要挑战是，如何知道发送的包要到哪，主要处理方式有两种： 路由器处理：路由器决定哪些设备跟它通信，通知这些设备给它发包。这种方式对路由器的变化较简单，可以直接使用之前的转发表和最长前缀匹配，但是当移动设备大量增加以后负担过重 端系统处理：在边缘进行处理： 间接路由：通过归属网络与移动设备进行交流 直接路由：直接与移动设备交流 其中“归属网络”非常重要： 有一个确定的信息源 别人可以与你进行通信 两种网络 4G/5G网络中有两种主要的网络： 归属网络：由蜂窝提供商提供服务，HSS储存了相关信息 被访网络：归属网络之外的所有网络，提供和远端的通信 而在ISP/WiFi中，不再有归属网络的概念： ISP的授权存储在设备或者用户上 ISP的影响力非常巨大 不同的网络有不同的授权，有为4G设计的结构但是未使用 ISP/WiFi网络 注册 移动设备和被访网络中MME相连接 被访网络中的MME向HSS注册移动设备的位置 最终被访MME知道设备存在，HSS知道设备位置 通信过程 通信者和移动设备通信的过程为： 将信息发给归属网关 归属网关将其发送给被访网关 被访网关与移动设备通信 之后被访网关间接路由（经过归属网关转移）或直接路由（直接发回） 但是这种情况下，如果通信者希望与同一网络下的移动设备通信，则效率会有很大降低；但是其优势在与连接的稳定性，移动设备的移动只会带来新的注册，同新方还是只用给HSS发信息即可 另一种通信过程为： 将信息发给归属网关 归属网关回复被访网络有关信息 直接与被访网关通信 克服了上述三角路由的效率问题，但是通信者必须保证自己发送信息到了正确的位置，并且被访网络的变化处理更复杂 实际中的移动性 4G 移动设备与被访网络中的基站通信，向基站提供IMSI 被访MME使用IMSI联系其HSS，并建立控制平面状态（相互知道移动设备在被访网络中） 数据平面建立：归属P网关和被访MME建立联系，被访MME通过基站与移动设备通信 移动设备可能改变其在被访网络中的接入点 移动设备的数据平面建立是通过S网关和基站共同完成的，其信道路径为：设备↔\\leftrightarrow↔基站↔\\leftrightarrow↔被访S网关↔\\leftrightarrow↔归属P网关↔\\leftrightarrow↔通信者，其中信道上通信的数据使用了GTP封装在UDP中 切换基站 源基站选择目的基站，向其发送请求 目的基站预先分配频段与时隙，回复ACK并告知相关信息 源基站告知移动设备新基站的信息（在移动设备看来切换已经完成） 源基站停止向移动设备发送信息，而是发送到目的基站 目的基站告知MME，MME通知S网关，S网关修改目的地 源基站收到ACK，释放资源，完成 基站切换 移动IP 大约20年前有了标准化架构，但是并没有广泛使用，其架构主要是： 间接路由 归属代理：HSS与归属P网关的结合 外部代理：MME与S网关结合 通过ICMP扩展注册，协议用于发现代理","tags":["笔记","网原","WiFi"],"categories":["计算机网络原理"]},{"title":"TCS-Lecture-D","path":"/2024/05/29/TCS-Lecture-D/","content":"TCS: Interactive Proofs and Zero-Knowledge Interactive Proofs and Zero-Knowledge Interactive Proofs Interactive Proof Verification Interactive proof system allows prover PPP and verifier VVV to exchange messages before stopping. We restrict VVV to be PPT while the probabilitic is necessary otherwise this system is NP\\text{NP}NP as PPP is stronger than VVV. Problem AAA is in IP\\text{IP}IP if there is a pair of interactive algorithms (P,V)(P, V)(P,V), with VVV running in probabilistic polynomial time in the length of input xxx, such that(Completeness). If x∈Ax\\in Ax∈A, then P(⟨P,V⟩(x)=1)=1P(\\langle P, V\\rangle(x) = 1) = 1P(⟨P,V⟩(x)=1)=1.(Soundness). If x∉Ax otin Ax∈/A, then for any possibly dishonest P∗P^{*}P∗, we have P(⟨P,V⟩(x)=1)≤12P(\\langle P, V\\rangle(x) = 1) \\leq \\frac{1}{2}P(⟨P,V⟩(x)=1)≤21​.(P,V)(P, V)(P,V) satisfying above is called a proof system for AAA. We say A∈IP[ℓ]A\\in\\text{IP}[\\ell]A∈IP[ℓ] if it has a proof system using ℓ=ℓ(∣x∣)\\ell = \\ell(|x|)ℓ=ℓ(∣x∣) times of message exchanges (number of messages, not rounds!). We have that: BPP⊆IP[1]\\text{BPP} \\subseteq \\text{IP}[1]BPP⊆IP[1] Concretely, we consider two problem GRAPH-ISO\\text{GRAPH-ISO}GRAPH-ISO(图同构问题) and GRAPH-NONISO\\text{GRAPH-NONISO}GRAPH-NONISO. Easily, GRAPH-ISO∈NP\\text{GRAPH-ISO}\\in \\text{NP}GRAPH-ISO∈NP since we can choose the isomorphic permutation π\\piπ as witness. Besides, we have GRAPH-NONISO∈IP\\text{GRAPH-NONISO}\\in \\text{IP}GRAPH-NONISO∈IP. The GNI protocol is: VVV samples a random b∈{0,1}b\\in\\{0, 1\\}b∈{0,1} and a random permutation π\\piπ, send π(Gb)\\pi(G_{b})π(Gb​) to PPPPPP returns an bit b′b&#x27;b′ to VVV and VVV accepts iff b′=bb&#x27; = bb′=b As PPP is all-know, it must have the ability to distinguish bbb if G0≁G1G_{0} ot\\sim G_{1}G0​∼G1​. But it can just guess arbitarily if they are isomorphic. Merlin-Arthur (Public-Coin) Protocols (Skip) A Merlin-Arthur protocol is an interactive proof system where the verifier (Arthur) messages are public random coin flips. A really simple VVV! Let AM[ℓ]\\text{AM}[\\ell]AM[ℓ] to be the Merlin-Arthur protocol with ℓ(∣x∣)\\ell(|x|)ℓ(∣x∣) messages. Consider MA=AM[1]\\text{MA} = \\text{AM}[1]MA=AM[1] and AM=AM[2]\\text{AM} = \\text{AM}[2]AM=AM[2] Though it seems week, but we have: Goldwasser-Sipser theorem: ∀ℓ\\forall \\ell∀ℓ, IP[ℓ]⊆AM[ℓ+2]\\text{IP}[\\ell]\\subseteq\\text{AM}[\\ell + 2]IP[ℓ]⊆AM[ℓ+2] Define a set: S={(H,π) ∣ π∈Aut(H),H∼G0 or H∼G1}S = \\{(H, \\pi) \\,|\\, \\pi\\in\\text{Aut}(H), H\\sim G_{0} \\text{ or } H\\sim G_{1} \\} S={(H,π)∣π∈Aut(H),H∼G0​ or H∼G1​} Aut(H)\\text{Aut}(H)Aut(H) represents the automorphism permutation multiplicity. So we have: ∣S∣={n!G0∼G12n!otherwise|S| = \\begin{cases} n! &amp; G_{0} \\sim G_{1} \\\\ 2n! &amp; \\text{otherwise} \\end{cases} ∣S∣={n!2n!​G0​∼G1​otherwise​ So the GRAPH-NONISO\\text{GRAPH-NONISO}GRAPH-NONISO is transformed into confirming ∣S∣|S|∣S∣ Let H\\mathcal{H}H be a pairwise independent hash family with U={0,1}mU = \\{0, 1\\}^{m}U={0,1}m and R={0,1}kR = \\{0, 1\\}^{k}R={0,1}k where k&lt;mk &lt; mk&lt;m. Then the protocol is: TODO More facts IP=PSPACE\\text{IP} = \\text{PSPACE}IP=PSPACE IP[ℓ]=AM[ℓ]=AM\\text{IP}[\\ell] = \\text{AM}[\\ell] = \\text{AM}IP[ℓ]=AM[ℓ]=AM for all constant ℓ≥2\\ell \\geq 2ℓ≥2 AM=MA=IP\\text{AM} = \\text{MA} = \\text{IP}AM=MA=IP if we allow completeness error Coin Flipping and Commitment We want a protocol where Alice and Bob talk to each other and then decide some important thing on the outcome R(a,b)R(a, b)R(a,b) of the coin flip and it makes sure that the result is not biased even if one of them is cheating (malicious). This means: P(R=0)≤12+negl(n)P(R=1)≤12+negl(n)\\begin{align*} P(R = 0) &amp;\\leq \\frac{1}{2} + \\text{negl}(n) \\\\ P(R = 1) &amp;\\leq \\frac{1}{2} + \\text{negl}(n) \\end{align*} P(R=0)P(R=1)​≤21​+negl(n)≤21​+negl(n)​ So we need something to hide aaa which filped by Alice to Bob to avoid bias. Bit Commitment The commitment scheme is a protocol consisting of the commit phase and reveal phase. A PPT protocol Com\\text{Com}Com is a (computationally hiding and statistically binding) commitment scheme if:Hiding: ∀ x0≠x1\\forall\\, x_{0} eq x_{1}∀x0​=x1​, Com(x0)≈cCom(x1)\\text{Com}(x_{0})\\approx_{c}\\text{Com}(x_{1})Com(x0​)≈c​Com(x1​).Binding: The probability that Alice can open Com(x0)\\text{Com}(x_{0})Com(x0​) to x1x_{1}x1​ with x1≠x0x_{1} eq x_{0}x1​=x0​ is negligible. In this ≈c\\approx_{c}≈c​ means that one cannot distinguish both is polynomial time. Intuitively, this definition means Bob(recevier) can hardly verify x0x_{0}x0​ or x1x_{1}x1​ and Alice(sender) can almost always verify them. Coin Flipping from Bit Commitment The safe protocol is as follows: Alice send Com(a)\\text{Com}(a)Com(a) to Bob with a random aaaBob sends bbb to AliceAlice opens Com(a)\\text{Com}(a)Com(a)Set R=a⊕bR = a \\oplus bR=a⊕b Bit Commitment From PRG (Naor’s Construction) Assume GGG is a PRG with ℓ(n)=3n\\ell(n) = 3nℓ(n)=3n In the commit phase:The receiver sends x∈{0,1}3nx\\in\\{0, 1\\}^{3n}x∈{0,1}3n;The sender samples m∈{0,1}m\\in\\{0, 1\\}m∈{0,1}, z∈{0,1}nz\\in\\{0, 1\\}^{n}z∈{0,1}n and sends y=G(z)⊕xmy = G(z)\\oplus x^{m}y=G(z)⊕xm;In the reveal phase:The sender sends zzz and mmm. Zero-Knowledge Proofs Definition at first is as follows, in which view\\text{view}view contains the transcript of the interaction(all messages and their probabilities) and local information. An interactive protocol (P,V)(P, V)(P,V) is perfect (statistical or computational) ZK for AAA if, ∀\\forall∀ PPT V∗V^{*}V∗, there exists a PPT simulator SSS such that ∀ x∈A\\forall\\, x\\in A∀x∈A, the following two distributions are the same:viewV∗⟨P,V∗⟩(x)\\text{view}_{V^{*}}\\langle P, V^{*}\\rangle(x)viewV∗​⟨P,V∗⟩(x)S(x)S(x)S(x) We have GRAPH-ISO∈PZK\\text{GRAPH-ISO} \\in \\text{PZK}GRAPH-ISO∈PZK. The protocol is as follows: PPP samples a random permutation π\\piπ and sends G=π(G0)G = \\pi(G_{0})G=π(G0​) to VVV VVV samples a random b∈{0,1}b\\in\\{0, 1\\}b∈{0,1} and sends it to PPP PPP responds with a proof that G∼GbG\\sim G_{b}G∼Gb​ with the permutation π∘σb\\pi\\circ\\sigma^{b}π∘σb simulator and rewind To prove this protocol is what we want, we need to prove its completeness, soundness and ZK. The first two are easy to argue, the ZK need a simulator and we generates it as follows: Choose aaa uniformly at random.Sample a random permutation π\\piπ and compute G=π(Ga)G = \\pi(G_{a})G=π(Ga​).Randomly sample rrr and simulate V∗V^{*}V∗ with random tape content rrr.If V∗V^{*}V∗ sends b=ab = ab=a, outputs (G,π)(G, \\pi)(G,π) as the message and the random tape rrr as the internal randomness of V∗V^{*}V∗.If V∗V^{*}V∗ sends b≠ab eq ab=a, rewind and start from the beginning. ZK Proofs for NP We have: (Goldreich-Micali-Wigderson). If statistically binding commitments exist, then there exists a zero-knowledge proof system for 3-COLORING\\text{3-COLORING}3-COLORING The protocol is: PPP samples random permutation π:{0,1,2}→{0,1,2}\\pi: \\{0, 1, 2\\}\\to \\{0, 1, 2\\}π:{0,1,2}→{0,1,2}. PPP sends {Com(π(ϕ(v))) ∣ v∈V}\\{\\text{Com}(\\pi(\\phi(v))) \\,|\\, v\\in V\\}{Com(π(ϕ(v)))∣v∈V} VVV samples a random edge (u,v)←E(u, v)\\gets E(u,v)←E and sends it to PPP PPP opens the commitment cuc_{u}cu​ and cvc_{v}cv​ VVV checks if π(ϕ(u)),π(ϕ(v))∈{0,1,2}\\pi(\\phi(u)), \\pi(\\phi(v))\\in\\{0, 1, 2\\}π(ϕ(u)),π(ϕ(v))∈{0,1,2} and π(ϕ(u))≠π(ϕ(v))\\pi(\\phi(u)) eq \\pi(\\phi(v))π(ϕ(u))=π(ϕ(v)) In this, ϕ\\phiϕ is a coloring scheme","tags":["笔记","TCS","交互式证明","零知识"],"categories":["理论计算机科学导引"]},{"title":"IAI-统计机器学习","path":"/2024/05/27/IAI-统计机器学习/","content":"人智导 统计机器学习 统计机器学习 对于数据集D={(xi,yi)}D = \\{(x_{i}, y_{i})\\}D={(xi​,yi​)}，输入输出之间存在关系fff，算法AAA根据训练集从假设空间H\\mathcal{H}H中选取一个合适的函数g≈fg\\approx fg≈f 主要决定因素：模型、策略、算法 分类方式： 监督式学习 半监督式学习 弱监督式学习 支持向量机(SVM) 按照模型的复杂程度，分为线性可分、线性与非线性 线性可分SVM 定义线性可分SVM相关概念如下： 线性可分训练集：T={(xi,yi) ∣ 1≤i≤N,xi∈Rn,yi∈{−1,1}}T = \\{ (x_{i}, y_{i}) \\,|\\, 1\\leq i\\leq N, x_{i}\\in \\mathbb{R}^{n}, y_{i}\\in\\{-1, 1\\} \\}T={(xi​,yi​)∣1≤i≤N,xi​∈Rn,yi​∈{−1,1}}给定超平面wx+b=0wx+b = 0wx+b=0，于是可以定义函数间隔为：γ^=min⁡1≤i≤N{(wxi+b)yi}\\hat{\\gamma} = \\mathop{\\min}\\limits_{1\\le i \\le N}\\{(wx_{i} + b)y_{i}\\}γ^​=1≤i≤Nmin​{(wxi​+b)yi​}相对应的，定义几何间隔为函数间隔的归一化，也即考虑超平面的缩放的影响γ=γ^∣∣w∣∣\\gamma = \\frac{\\hat{\\gamma}}{||w||}γ=∣∣w∣∣γ^​​通过间隔最大化得到分类超平面w∗x+b∗=0w^{*}x+b^{*} = 0w∗x+b∗=0，则得到决策函数：f(x)=sgn(w∗x+b∗)f(x) = \\mathrm{sgn}(w^{*}x+b^{*})f(x)=sgn(w∗x+b∗)此即线性可分SVM 于是关键问题即为间隔最大化： max⁡w,bγ=max⁡w,bγ^∣∣w∣∣\\max\\limits_{w, b}\\gamma = \\max\\limits_{w, b}{\\frac{\\hat{\\gamma}}{||w||}} w,bmax​γ=w,bmax​∣∣w∣∣γ^​​ 显然，函数间隔是可缩放的（函数缩放表示为$w, b$等比例缩放），因此不妨令γ^=1\\hat{\\gamma} = 1γ^​=1，则进一步转化为规划问题： max⁡w,b1∣∣w∣∣=min⁡w,b12∣∣w∣∣2\\max\\limits_{w, b}\\frac{1}{||w||} = \\min\\limits_{w, b} \\frac{1}{2}||w||^{2}w,bmax​∣∣w∣∣1​=w,bmin​21​∣∣w∣∣2使得：∀i,γi^=(wxi+b)yi≥1\\forall i,\\quad \\hat{\\gamma_{i}} = (wx_{i} + b)y_{i} \\geq 1∀i,γi​^​=(wxi​+b)yi​≥1 定义相对应的拉格朗日函数为： L(w,b,α)=12∣∣w∣∣2+∑i=1Nαi[1−(wxi+b)yi]L(w, b, \\alpha) = \\frac{1}{2}||w||^{2} + \\sum\\limits_{i=1}^{N}\\alpha_{i}[1 - (wx_{i} + b)y_{i}] L(w,b,α)=21​∣∣w∣∣2+i=1∑N​αi​[1−(wxi​+b)yi​] 则在满足优化条件的情况下，原问题转化为： min⁡w,bmax⁡αL(w,b,α)\\min\\limits_{w, b}\\max\\limits_{\\alpha}L(w, b, \\alpha) w,bmin​αmax​L(w,b,α) 当满足KKT条件时，该问题可转化为其对偶问题： max⁡αmin⁡w,bL(w,b,α)\\max\\limits_{\\alpha}\\min\\limits_{w, b}L(w, b, \\alpha) αmax​w,bmin​L(w,b,α) ∇w,bL(w,b,α)=0αi[1−(wxi+b)yi]=01−(wxi+b)yi≤0αi≥0\\begin{align*} abla_{w, b}L(w, b, \\alpha) &amp;= 0 \\\\\\alpha_{i}[1 - (wx_{i} + b)y_{i}] &amp;= 0 \\\\1 - (wx_{i} + b)y_{i} &amp;\\leq 0 \\\\\\alpha_{i} &amp;\\geq 0 \\end{align*}∇w,b​L(w,b,α)αi​[1−(wxi​+b)yi​]1−(wxi​+b)yi​αi​​=0=0≤0≥0​ 利用KKT条件转化为对偶问题之后，可以化简为： max⁡α(−12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)+∑i=1Nαi)=min⁡α(12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)−∑i=1Nαi)\\begin{align*}&amp;\\quad \\max\\limits_{\\alpha}\\bigl(-\\frac{1}{2}\\sum\\limits_{i = 1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}\\cdot x_{j}) + \\sum\\limits_{i=1}^{N}\\alpha_{i} \\bigr) \\\\&amp;= \\min\\limits_{\\alpha}\\bigl(\\frac{1}{2}\\sum\\limits_{i = 1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}\\cdot x_{j}) - \\sum\\limits_{i=1}^{N}\\alpha_{i} \\bigr) \\\\\\end{align*}​αmax​(−21​i=1∑N​j=1∑N​αi​αj​yi​yj​(xi​⋅xj​)+i=1∑N​αi​)=αmin​(21​i=1∑N​j=1∑N​αi​αj​yi​yj​(xi​⋅xj​)−i=1∑N​αi​)​使得：∑i=1Nαiyi=0αi≥0\\sum\\limits_{i=1}^{N}\\alpha_{i}y_{i} = 0\\quad \\alpha_{i} \\geq 0i=1∑N​αi​yi​=0αi​≥0 于是可以求出α∗\\alpha^{*}α∗，相对应的根据KKT条件求出w∗,b∗w^{*}, b^{*}w∗,b∗： w∗=∑i=1Nαi∗yixib∗=yj−∑i=1Nαi∗yi(xi⋅xj)αj≠0\\begin{align*}w^{*} &amp;= \\sum\\limits_{i=1}^{N}\\alpha_{i}^{*}y_{i}x_{i} \\\\b^{*} = y_{j} - &amp;\\sum\\limits_{i=1}^{N}\\alpha_{i}^{*}y_{i}(x_{i}\\cdot x_{j})\\quad \\alpha_{j} eq 0\\end{align*}w∗b∗=yj​−​=i=1∑N​αi∗​yi​xi​i=1∑N​αi∗​yi​(xi​⋅xj​)αj​=0​ 所得到的αi&gt;0\\alpha_{i} &gt; 0αi​&gt;0对应的实例xix_{i}xi​即为支持向量 线性SVM 在线性可分SVM中，约束条件要求每一个样本点的函数间隔都不小于111，这个条件事实上要求样本拥有很强的分类性，而对于一些分类性较弱的样本，很难全部满足这个条件，因此引入松弛变量ξi\\xi_{i}ξi​，将约束条件修改为 (wxi+b)yi≥1−ξi(wx_{i} + b)y_{i} \\geq 1 - \\xi_{i} (wxi​+b)yi​≥1−ξi​ 为了让分隔尽量好，因此要满足松弛变量尽量小，因此调整优化问题为： min⁡w,b,ξ(12∣∣w∣∣2+C∑i=1Nξi)\\min\\limits_{w, b, \\xi}\\bigl( \\frac{1}{2}||w||^{2} + C\\sum\\limits_{i=1}^{N}\\xi_{i} \\bigr) w,b,ξmin​(21​∣∣w∣∣2+Ci=1∑N​ξi​) 其中CCC为惩罚参数，用于调整间隔最大与误分类点数之间的矛盾 于是，可以转化为： min⁡α(12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)−∑i=1Nαi)\\min\\limits_{\\alpha}\\bigl(\\frac{1}{2}\\sum\\limits_{i = 1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}\\cdot x_{j}) - \\sum\\limits_{i=1}^{N}\\alpha_{i} \\bigr) αmin​(21​i=1∑N​j=1∑N​αi​αj​yi​yj​(xi​⋅xj​)−i=1∑N​αi​)使得：∑i=1Nαiyi=00≤αi≤C\\sum\\limits_{i=1}^{N}\\alpha_{i}y_{i} = 0\\quad 0\\leq \\alpha_{i} \\leq Ci=1∑N​αi​yi​=00≤αi​≤C 可以看出与线性可分的唯一区别在与αi\\alpha_{i}αi​的范围，因此最终求出w∗,b∗w^{*}, b^{*}w∗,b∗时同样和线性可分只有这个区别，具体来说： w∗=∑i=1Nαi∗yixib∗=yj−∑i=1Nαi∗yi(xi⋅xj)0&lt;αj&lt;C\\begin{align*}w^{*} &amp;= \\sum\\limits_{i=1}^{N}\\alpha_{i}^{*}y_{i}x_{i} \\\\b^{*} = y_{j} - &amp;\\sum\\limits_{i=1}^{N}\\alpha_{i}^{*}y_{i}(x_{i}\\cdot x_{j})\\quad 0 &lt; \\alpha_{j} &lt; C\\end{align*}w∗b∗=yj​−​=i=1∑N​αi∗​yi​xi​i=1∑N​αi∗​yi​(xi​⋅xj​)0&lt;αj​&lt;C​ 同样，αi∗&gt;0\\alpha_{i}^{*} &gt; 0αi∗​&gt;0所对应的样本成为软间隔的支持向量，具体来说： αi∗&lt;C\\alpha_{i}^{*} &lt; Cαi∗​&lt;C时，ξi=0\\xi_{i} = 0ξi​=0，xix_{i}xi​位于间隔边界 αi∗=C\\alpha_{i}^{*} = Cαi∗​=C时，ξi&gt;0\\xi_{i} &gt; 0ξi​&gt;0： 0&lt;ξi&lt;10 &lt; \\xi_{i} &lt; 10&lt;ξi​&lt;1时，分类正确 ξi=1\\xi_{i} = 1ξi​=1时，位于超平面上 ξi&gt;1\\xi_{i} &gt; 1ξi​&gt;1时，分类错误 非线性SVM 关键思想：利用变换ϕ:X→H\\phi: \\mathcal{X}\\to \\mathcal{H}ϕ:X→H将原空间的数据映射到特征空间，使之变为线性的，相对应的问题为： min⁡α(12∑i=1N∑j=1Nαiαjyiyj(ϕ(xi)⋅ϕ(xj))−∑i=1Nαi)\\min\\limits_{\\alpha}\\bigl(\\frac{1}{2}\\sum\\limits_{i = 1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\phi(x_{i})\\cdot \\phi(x_{j})) - \\sum\\limits_{i=1}^{N}\\alpha_{i} \\bigr) αmin​(21​i=1∑N​j=1∑N​αi​αj​yi​yj​(ϕ(xi​)⋅ϕ(xj​))−i=1∑N​αi​)使得：∑i=1Nαiyi=00≤αi≤C\\sum\\limits_{i=1}^{N}\\alpha_{i}y_{i} = 0\\quad 0\\leq \\alpha_{i} \\leq Ci=1∑N​αi​yi​=00≤αi​≤C 可以看出其中只有内积有变化，因此若存在函数KKK满足K(x,z)=ϕ(x)⋅ϕ(z)K(x, z) = \\phi(x)\\cdot \\phi(z)K(x,z)=ϕ(x)⋅ϕ(z)对任二元素都成立，则称K(x,z)K(x, z)K(x,z)为核函数 同一个核函数对应的映射不一定相同 映射到新空间之后，可以解得： w∗=∑i=1Nαi∗yiϕ(xi)b∗=yj−∑i=1Nαi∗yiK(xi,xj)0&lt;αj&lt;C\\begin{align*}w^{*} &amp;= \\sum\\limits_{i=1}^{N}\\alpha_{i}^{*}y_{i}\\phi(x_{i}) \\\\b^{*} = y_{j} - &amp;\\sum\\limits_{i=1}^{N}\\alpha_{i}^{*}y_{i}K(x_{i}, x_{j})\\quad 0 &lt; \\alpha_{j} &lt; C\\end{align*}w∗b∗=yj​−​=i=1∑N​αi∗​yi​ϕ(xi​)i=1∑N​αi∗​yi​K(xi​,xj​)0&lt;αj​&lt;C​ 常用的核函数包括： K(x,z)=(x⋅z+1)pK(x,z)=exp⁡(−∣∣x−z∣∣22σ2)(Gaussian)\\begin{align*} K(x, z) &amp;= (x\\cdot z + 1)^{p} \\\\ K(x, z) &amp;= \\exp(-\\frac{||x - z||^{2}}{2\\sigma^{2}})\\quad (\\text{Gaussian}) \\end{align*} K(x,z)K(x,z)​=(x⋅z+1)p=exp(−2σ2∣∣x−z∣∣2​)(Gaussian)​ 采用高斯核函数的时候，关键为σ\\sigmaσ的选取，不当会导致欠拟合与过拟合 σ过大导致欠拟合 σ过小导致过拟合 由于求解凸二次规划的算法在样本数很多的时候非常低效，因此有许多快速算法，例如序列最小最优化算法SMO 当样本中有多类怎么办？ 将一类视为正，其余所有视为负 任意两类构造一个SVM，分类时采取投票法 二分类别，依次二分整体直至单独类别 SVM应用举例 文本分类：文本表达为一个向量(w1j,…,wnj)T(w_{1j}, \\dots, w_{nj})^{T}(w1j​,…,wnj​)T，wijw_{ij}wij​表示词iii在文档jjj中的权重，具体计算有很多版本，在此介绍两种： wij=tfijw_{ij} = \\mathrm{tf}_{ij}wij​=tfij​即tf权重，用词频代表权重 wij=tfij∗idfiw_{ij} = \\mathrm{tf}_{ij}*\\mathrm{idf}_{i}wij​=tfij​∗idfi​即tf-idf权重，其中idf为逆文档频率，记cnt(i)\\mathrm{cnt}(i)cnt(i)为词iii所出现的文档数，则idfi=log⁡(Ncnt(i))\\mathrm{idf}_{i} = \\log(\\frac{N}{\\mathrm{cnt}(i)})idfi​=log(cnt(i)N​) 决策树 对样本进行分类，由节点和有向边组成，每个内部节点表示一个特征或者一个属性，每个叶节点表示一个类 决策树通过在训练集中对于分类规则进行训练，得到一组矛盾较小的特征，即KKK分类问题，而最优决策树的选择是一个NPC问题，因此通常采用启发式方法 学习过程包括： 特征选择 决策树生成与剪枝 特征选择 按照信息增益来选择特征，信息增益g(D,A)g(D, A)g(D,A)代表了特征AAA对于数据集DDD分类的不确定性的减少程度，计算如下： 记X,YX, YX,Y是两个随机变量，P(X=xi)=piP(X = x_{i}) = p_{i}P(X=xi​)=pi​，则：随机变量的熵：H(X)=−∑i=1npilog⁡(pi)H(X) = -\\sum\\limits_{i=1}^{n}p_{i}\\log(p_{i})H(X)=−i=1∑n​pi​log(pi​)条件熵：H(Y ∣ X)=∑i=1NpiH(Y ∣ X=xi)H(Y\\,|\\, X) = \\sum\\limits_{i=1}^{N}p_{i}H(Y \\,|\\, X = x_{i})H(Y∣X)=i=1∑N​pi​H(Y∣X=xi​)信息增益：g(D,A)=H(D)−H(D ∣ A)g(D, A) = H(D) - H(D \\,|\\, A)g(D,A)=H(D)−H(D∣A)信息增益越大的特征分类能力越强 具体到特征AAA对于数据集DDD的信息增益，设一共有KKK个类别(C1,…,CK)(C_{1}, \\dots, C_{K})(C1​,…,CK​)，AAA有nnn种不同值(a1,…,an)(a_{1},\\dots, a_{n})(a1​,…,an​)，对应了一个划分(D1,…,Dn)(D_{1}, \\dots, D_{n})(D1​,…,Dn​)，记DijD_{ij}Dij​为DiD_{i}Di​中属于类CjC_{j}Cj​的集合，则： H(D)=∑j=1K∣Cj∣∣D∣log⁡(∣Cj∣∣D∣)H(D ∣ A)=∑i=1n∣Di∣∣D∣H(Di)=−∑i=1n(∣Di∣∣D∣∑j=1K(∣Dij∣∣Di∣log⁡(∣Dij∣∣Di∣)))g(D,A)=H(D)−H(D ∣ A)\\begin{align*}H(D) &amp;= \\sum\\limits_{j=1}^{K}\\frac{|C_{j}|}{|D|}\\log(\\frac{|C_{j}|}{|D|}) \\\\H(D\\,|\\, A) &amp;= \\sum\\limits_{i=1}^{n}\\frac{|D_{i}|}{|D|}H(D_{i}) \\\\&amp;= -\\sum\\limits_{i=1}^{n}\\biggl(\\frac{|D_{i}|}{|D|}\\sum\\limits_{j=1}^{K}\\bigl(\\frac{|D_{ij}|}{|D_{i}|}\\log(\\frac{|D_{ij}|}{|D_{i}|})\\bigr)\\biggr) \\\\g(D, A) &amp;= H(D) - H(D\\,|\\, A)\\end{align*}H(D)H(D∣A)g(D,A)​=j=1∑K​∣D∣∣Cj​∣​log(∣D∣∣Cj​∣​)=i=1∑n​∣D∣∣Di​∣​H(Di​)=−i=1∑n​(∣D∣∣Di​∣​j=1∑K​(∣Di​∣∣Dij​∣​log(∣Di​∣∣Dij​∣​)))=H(D)−H(D∣A)​ 决策树的生成 两个常用的算法为ID3与C4.5 ID3算法 输入：训练集DDD，特征AAA，阈值ε\\varepsilonε 输出：决策树TTT 算法如下： 若DDD中所有实例属于同一类CCC，则TTT单节点，类标记为CCC，直接返回若A=∅A = \\varnothingA=∅，则TTT也为单节点，类标记为DDD中最多的类CjC_{j}Cj​，直接返回计算AAA中所有特征对DDD的信息增益，选择其中信息增益最大的特征AkA_{k}Ak​若g(D,Ak)≤εg(D, A_{k}) \\leq \\varepsilong(D,Ak​)≤ε，则将TTT置为单节点树，类标记为DDD中最多的类CjC_{j}Cj​，直接返回反之，遍历Ak=(ak1,…,aknk)A_{k} = (a_{k1}, \\dots, a_{kn_{k}})Ak​=(ak1​,…,aknk​​)，将DDD划分为nkn_{k}nk​个子集分别作为子节点遍历子节点DiD_{i}Di​，若为空，则将DDD中最多的类标记这个子节点反之以(Di,A−Ak,ε)(D_{i}, A - A_{k}, \\varepsilon)(Di​,A−Ak​,ε)递归返回TTT 也就是按照特征集合不断划分，当出现所有属于同一类或某一个划分为空集的时候进行特判即可 问题：信息增益倾向于选择分支较多的特征，但是有的分支是毫无意义的 C4.5算法 考虑信息增益比： gR(D,A)=g(D,A)HA(D)=g(D,A)(−∑i=1n∣Di∣∣D∣log⁡(∣Di∣∣D∣))−1g_{R}(D, A) = \\frac{g(D, A)}{H_{A}(D)} = g(D, A)\\bigl(-\\sum\\limits_{i=1}^{n}\\frac{|D_{i}|}{|D|}\\log(\\frac{|D_{i}|}{|D|})\\bigr)^{-1} gR​(D,A)=HA​(D)g(D,A)​=g(D,A)(−i=1∑n​∣D∣∣Di​∣​log(∣D∣∣Di​∣​))−1 因此C4.5引入信息增益比来选择特征，同时允许特征的取值为连续的而非离散的，相对于ID3的改进如下： 将选择信息增益最大的特征改为： 选择信息增益前kkk大的特征，再从其中选择信息增益比最大的特征 原因是信息增益比倾向于选择分割不均匀的特征 对于连续的特征，采用二分，找到中间值a0a_{0}a0​，将≤a0\\leq a_{0}≤a0​的划分到左子树，&gt;a0&gt;a_{0}&gt;a0​的划分到右子树 决策树的剪枝 由于决策树非常容易出现过拟合，因此需要对其进行剪枝先生成树再剪枝，这种方法称之为后剪枝 剪枝方式为： 对于某个内部节点，删除这棵子树，用这棵子树的根节点作为新的叶节点，其类别标记为其中最多的类 当数据集比较大的时候，剪枝方式为： 在训练集上训练，逐步剪枝 在验证集上验证直至性能下降 在测试集上测试 当数据集比较小的时候，直接利用训练集进行剪枝，方法如下： 设决策树TTT的叶节点为leaf(T)=(T1,…,Tt)\\mathrm{leaf}(T) = (T_{1}, \\dots, T_{t})leaf(T)=(T1​,…,Tt​)，TiT_{i}Ti​有NiN_{i}Ni​个样本，其中第kkk类的样本点有NikN_{ik}Nik​个，Hi(T)H_{i}(T)Hi​(T)为经验熵，aaa为参数，记损失函数为： Ca(T)=∑i=1tNiHi(T)+at=−∑i=1t∑j=1KNijlog⁡NijNi+at\\begin{align*} C_{a}(T) &amp;= \\sum\\limits_{i=1}^{t}N_{i}H_{i}(T) + at \\\\ &amp;= -\\sum\\limits_{i=1}^{t}\\sum\\limits_{j=1}^{K}N_{ij}\\log\\frac{N_{ij}}{N_{i}} + at \\end{align*} Ca​(T)​=i=1∑t​Ni​Hi​(T)+at=−i=1∑t​j=1∑K​Nij​logNi​Nij​​+at​ 其中两项分别代表预测误差与模型复杂程度 最终，剪枝算法为： 输入：整个树TTT，参数aaa输出：修剪后的TaT_{a}Ta​计算每个节点的经验熵从叶节点向上回溯，如果在某个点剪枝之后可以降低损失函数，则剪枝，反之跳过直到不能继续剪枝，结束 随机森林 由于决策树非常容易过拟合，因此使用多棵决策树组成一片随机森林，利用投票机制进行决策 单棵决策树的生成是通过有放回的数据采样，关键点为集外数据的使用（也即别的决策树怎么使用某棵决策树没使用的数据）","tags":["笔记","IAI","统计机器学习"],"categories":["人工智能导论"]},{"title":"TCS-Lecture-C","path":"/2024/05/22/TCS-Lecture-C/","content":"TCS: Pseudorandomness and Private-Key Encryption Pseudorandomness and Private-Key Encryption Private-Key Encryption This encryption(加密) problem is starting form the transmission question: two people need to transmit information while avoiding others to know the content. The resolution is to maintain a ‘key’ kkk by two people and one can use kkk to encrypt information while the other can use it to decrypt Validity A pair of polynomial-time computable functions (Enc,Dec)(\\text{Enc}, \\text{Dec})(Enc,Dec) is a valid private key encryption scheme if for every n∈Nn\\in \\mathbb{N}n∈N, k∈{0,1}nk\\in \\{0, 1\\}^{n}k∈{0,1}n,and xxx, we haveDec(k,Enc(k,x))=x \\text{Dec}(k, \\text{Enc}(k, x)) = xDec(k,Enc(k,x))=x And we have the length of the ciphertext(密文) is no less than that of the plaintext, written as: lc(n)≥lp(n)l_{c}(n) \\geq l_{p}(n) lc​(n)≥lp​(n) Security We need to define the security with following assumption: A cryptosystem should be secure even if everything about the system, except the key, is public knowledge. Due to this assumption, the kkk must be generated randomly. So, let’s define the security: A valid encryption scheme (Enc,Dec)(\\text{Enc}, \\text{Dec})(Enc,Dec) with plaintext length l(⋅)l(\\cdot)l(⋅) is perfectly secret if for every n∈Nn\\in \\mathbb{N}n∈N and plaintexts x0,x1∈{0,1}l(n)x_{0}, x_{1} \\in \\{0, 1\\}^{l(n)}x0​,x1​∈{0,1}l(n), the following two distributions Y0Y_{0}Y0​and Y1Y_{1}Y1​ over {0,1}∗\\{0, 1\\}^{*}{0,1}∗ are identical:YiY_{i}Yi​ is obtained by sampling kkk and outputting Enc(k,xi)\\text{Enc}(k, x_{i})Enc(k,xi​) for i=0,1i = 0, 1i=0,1. Definition Analysis We have a secrecy experiment as follows: Sample k∈{0,1}nk \\in \\{0, 1\\}^{n}k∈{0,1}n Adversary A\\mathcal{A}A outputs x0,x1x_{0}, x_{1}x0​,x1​ given input 1n1^{n}1n Randomly choose bbb and send y=Enc(k,xb)y = \\text{Enc}(k, x_{b})y=Enc(k,xb​) to A\\mathcal{A}A A\\mathcal{A}A returns b′∈{0,1}b&#x27;\\in\\{0, 1\\}b′∈{0,1} If b=b′b = b&#x27;b=b′, A\\mathcal{A}A wins We can prove A\\mathcal{A}A has at most 12\\frac{1}{2}21​ probability to succeed under perfectly secret. P(Y=y ∣ b=i)=P(Yi=y)=p(y)\\begin{align*} P(Y = y\\,|\\, b = i) &amp;= P(Y_{i} = y) = p(y)\\end{align*}P(Y=y∣b=i)​=P(Yi​=y)=p(y)​And P(Y=y)=p(y)P(Y = y) = p(y)P(Y=y)=p(y) due to YYY is perfect, so:P(Y=y,b=i)=P(b=i)P(Y=y ∣ b=i)=P(b=i)p(y)=P(b=i)P(Y=y)\\begin{align*} P(Y = y, b = i) &amp;= P(b = i)P(Y = y \\,|\\, b = i) \\\\ &amp;= P(b = i)p(y) \\\\ &amp;= P(b = i)P(Y = y)\\end{align*}P(Y=y,b=i)​=P(b=i)P(Y=y∣b=i)=P(b=i)p(y)=P(b=i)P(Y=y)​This means YYY is independent to bbb, so A\\mathcal{A}A cannot improve its probability to win by getting yyy Construction Give the One-time pad construction: lp(n)=lc(n)=nEnc(k,x)=x⊕kDec(k,c)=k⊕c\\begin{align*} l_{p}(n) = l_{c}&amp;(n) = n \\\\ \\text{Enc}(k, x) &amp;= x \\oplus k \\\\ \\text{Dec}(k, c) &amp;= k \\oplus c\\end{align*}lp​(n)=lc​Enc(k,x)Dec(k,c)​(n)=n=x⊕k=k⊕c​ In this construction, we have one glaring dis advantage, that is the kkk has the same length of plaintext! This is necessary for perfect secrecy: For every perfectly secret encryption scheme, the length function lp(n)l_{p}(n)lp​(n) satisfies lp(n)≤nl_{p}(n)\\leq nlp​(n)≤n Computational Secrecy and PRG The long kkk is unadorable in reality. So we use computational secrecy instead. An encryption scheme is computationally secret if no probabilistic polynomial-time(PPT) algorithms can break it. Let Enc,Dec\\text{Enc}, \\text{Dec}Enc,Dec be a valid encryption scheme. The scheme is computationally secret if, for every PPT adversary algorithm A\\mathcal{A}A in the secrecy experiment, there is negligible function negl\\text{negl}negl such thatP(A succ)≤12+negl(n) P(\\mathcal{A} \\text{ succ}) \\leq \\frac{1}{2} + \\text{negl}(n)P(A succ)≤21​+negl(n)where the probability is taken over the randomness of A\\mathcal{A}A and the experiment. Pseudorandom Generators Definition: A cryptographic pseudorandom generator (PRG) with stretch l(⋅)l(\\cdot)l(⋅) is a PPT computable function G:{0,1}∗→{0,1}∗G: \\{0, 1\\}^{*} \\to \\{0, 1\\}^{*}G:{0,1}∗→{0,1}∗ such that:∀ n∈N\\forall \\, n\\in \\mathbb{N}∀n∈N and s∈{0,1}ns\\in \\{0, 1\\}^{n}s∈{0,1}n, ∣G(s)∣=l(n)|G(s)| = l(n)∣G(s)∣=l(n)For any poly-algorithm A\\mathcal{A}A, s∈{0,1}ns\\in \\{0, 1\\}^{n}s∈{0,1}n and r∈{0,1}l(n)r\\in \\{0, 1\\}^{l(n)}r∈{0,1}l(n):∣P(A((G(s)))=1)−P(A((r))=1)∣=negl(n) |{P (\\mathcal{A}((G(s))) = 1) - P (\\mathcal{A}((r)) =1)}| = \\text{negl}(n)∣P(A((G(s)))=1)−P(A((r))=1)∣=negl(n) Intuitively, this means we can get the difference between output of PRG G(⋅)G(\\cdot)G(⋅) and a actual random output rrr in a negligible probability. Its existance is obtained by the cryptographic PRG conjecture: PRG with l(n)=na∀a∈Nl(n) = n^{a}\\quad \\forall a\\in\\mathbb{N}l(n)=na∀a∈N exists. If the cryptographic PRG conjecture is true, then computationally secret encryption exists for l(n)≥na∀a∈Nl(n) \\geq n^{a}\\quad \\forall a\\in\\mathbb{N}l(n)≥na∀a∈N Enc(k,x)=x⊕G(k)Dec(c,k)=c⊕G(k)\\begin{align*} \\text{Enc}(k, x) &amp;= x \\oplus G(k) \\\\ \\text{Dec}(c, k) &amp;= c \\oplus G(k)\\end{align*}Enc(k,x)Dec(c,k)​=x⊕G(k)=c⊕G(k)​ This can be proved to be computation secret by contradiction proof, which is to construct a adversary for PRG by the assuming contradiction. CPA Security and PRF Choose Plaintext Attack (CPA) The CPA experiments is similar to secrecy experiments, while A\\mathcal{A}A can interact freely with Enc(k,⋅)\\text{Enc}(k, \\cdot)Enc(k,⋅) as a black-box. An encryption scheme (Enc,Dec)(\\text{Enc}, \\text{Dec})(Enc,Dec) is CPA-secure if, for all PPT adversary A\\mathcal{A}A, there exists a negligible function negl\\text{negl}negl such thatP(A succ)≤12+negl(n) P(\\mathcal{A} \\text{ succ}) \\leq \\frac{1}{2} + \\text{negl}(n)P(A succ)≤21​+negl(n) All CPA-secure encryption scheme must be probabilistic otherwise A\\mathcal{A}A can query all Enc(k,x)\\text{Enc}(k, x)Enc(k,x) to get ciphertexts and compare. Pseudorandom Functions (PRF) Consider a keyed family of functions: Fk:{0,1}∗→{0,1}∗F_{k}: \\{0, 1\\}^{*} \\to \\{0, 1\\}^{*}Fk​:{0,1}∗→{0,1}∗, then we can define PRF as follows: Let FkF_{k}Fk​ be a keyed family of functions that is efficient and length-preserving. We say FkF_{k}Fk​ is a pseudorandom function if, for all PPT distinguishers D\\mathcal{D}D, there exists a negligible function negl\\text{negl}negl such that:∣P(DFk(⋅)(1n)=1)−P(Dfn(⋅)(1n)=1)∣≤negl(n) |{P \\bigl( D^{F_k(\\cdot)}(1^n) = 1 \\bigr) - P \\bigl( D^{f_n(\\cdot)}(1^n) = 1 \\bigr)}| \\le \\text{negl}(n)∣P(DFk​(⋅)(1n)=1)−P(Dfn​(⋅)(1n)=1)∣≤negl(n)where k←{0,1}nk\\gets\\{0, 1\\}^{n}k←{0,1}n is chosen uniformly at random and fnf_{n}fn​ is chosen uniformly from the set of functions mapping nnn-bit strings to nnn-bit strings. Intuitively, this means we can get the difference between PRF Fk(⋅)F_{k}(\\cdot)Fk​(⋅) and a actual random funtions fff in a negligible probability. PRF =&gt; CPA-secure Encryption Let FFF be a PRF. Define a CPA-secuee private-key encryption scheme for messages of length nnn as follows: Enc(k,x)=⟨r,Fk(r)⊕x⟩\\text{Enc}(k, x) = \\langle r, F_{k}(r)\\oplus x \\rangleEnc(k,x)=⟨r,Fk​(r)⊕x⟩Dec(k,⟨r,s⟩)=Fk(r)⊕s\\text{Dec}(k, \\langle r, s\\rangle) = F_{k}(r) \\oplus sDec(k,⟨r,s⟩)=Fk​(r)⊕sIn this r←{0,1}nr\\gets\\{0, 1\\}^{n}r←{0,1}n is randomly chosen. Proof is as follows, we use contradiction-proof and construct a contradiction to PRF with assumption. Consider above scheme as Π=(Enc,Dec)\\Pi = (\\text{Enc}, \\text{Dec})Π=(Enc,Dec) and the similar scheme Π~=(Enc~,Dec~)\\widetilde{\\Pi} = (\\widetilde{\\text{Enc}}, \\widetilde{\\text{Dec}})Π=(Enc,Dec) in which we replace FkF_{k}Fk​ with an actual random function.Assume that Π\\PiΠ is not CPA-secure, which means there exists an A\\mathcal{A}A:P(AΠ succ)≥12+1poly(n)P(\\mathcal{A}_{\\Pi}\\text{ succ}) \\geq \\frac{1}{2} + \\frac{1}{\\text{poly}(n)}P(AΠ​ succ)≥21​+poly(n)1​Let rcr_{c}rc​ represents the randomness used in encrypting xxx, while r1,…,rqr_{1}, \\dots, r_{q}r1​,…,rq​ represents the q(n)=poly(n)q(n) = \\text{poly}(n)q(n)=poly(n) randomness used when A\\mathcal{A}A queries to the oracle Enc\\text{Enc}Enc.Then we can argue that:P(AΠ~ succ)≤12+negl(n)P(\\mathcal{A}_{\\widetilde{\\Pi}}\\text{ succ}) \\leq \\frac{1}{2} + \\text{negl}(n) P(AΠ​ succ)≤21​+negl(n)We can argue this according to whether rc∈{r1,…,rq}r_{c} \\in \\{r_{1},\\dots ,r_{q}\\}rc​∈{r1​,…,rq​}If rc∈{r1,…,rq}r_{c} \\in \\{r_{1},\\dots ,r_{q}\\}rc​∈{r1​,…,rq​}, then A\\mathcal{A}A can actually get rcr_{c}rc​ as we contain it in the ciphertext. So A\\mathcal{A}A can always succeed in this case.But this case has a negligible probability q(n)2n\\frac{q(n)}{2^{n}}2nq(n)​ to appear.If rc∉{r1,…,rq}r_{c} otin \\{r_{1},\\dots ,r_{q}\\}rc​∈/{r1​,…,rq​}, then the encryption is like a perfect OTP as we cannot get any information about the random function fff. So A\\mathcal{A}A has a probability of 12\\frac{1}{2}21​ to win.So, we have that:P(AΠ~ succ)=P(AΠ~ succ ∣∈)P(∈)+P(AΠ~ succ ∣∉)P(∉)≤q(n)2n+12=12+negl(n)\\begin{align*}P(\\mathcal{A}_{\\widetilde{\\Pi}}\\text{ succ}) &amp;= P(\\mathcal{A}_{\\widetilde{\\Pi}}\\text{ succ}\\,|\\in)P(\\in) + P(\\mathcal{A}_{\\widetilde{\\Pi}}\\text{ succ}\\,| otin)P( otin) \\\\&amp;\\leq \\frac{q(n)}{2^{n}} + \\frac{1}{2} = \\frac{1}{2} + \\text{negl}(n)\\end{align*}P(AΠ​ succ)​=P(AΠ​ succ∣∈)P(∈)+P(AΠ​ succ∣∈/)P(∈/)≤2nq(n)​+21​=21​+negl(n)​Define the distinguisher D\\mathcal{D}D, who has an orecal O\\mathcal{O}O, as follows:Run A(1n)\\mathcal{A}(1^{n})A(1n), for each oracle query of A\\mathcal{A}A with xxx, do:Choose r←{0,1}nr\\gets \\{0, 1\\}^{n}r←{0,1}nreturn ⟨r,O(r)⊕x⟩\\langle r, \\mathcal{O}(r)\\oplus x\\rangle⟨r,O(r)⊕x⟩ to A\\mathcal{A}AWhen A\\mathcal{A}A gives D\\mathcal{D}D two string x0,x1x_{0}, x_{1}x0​,x1​, randomly choose a bit bbb and:Choose r←{0,1}nr\\gets \\{0, 1\\}^{n}r←{0,1}nreturn ⟨r,O(r)⊕xb⟩\\langle r, \\mathcal{O}(r)\\oplus x_{b}\\rangle⟨r,O(r)⊕xb​⟩ to A\\mathcal{A}AAnswer A\\mathcal{A}A as Step 1 until A\\mathcal{A}A gives an output b′b&#x27;b′. Then we output 111 if b′=bb&#x27; = bb′=b otherwise 000.Actually, this distinguisher is simulating the CPA-experiment. So:P(DFk(⋅)(1n)=1)=P(AΠ succ)P(Df(⋅)(1n)=1)=P(AΠ~ succ)\\begin{align*}P(\\mathcal{D}^{F_{k}(\\cdot)}(1^{n}) = 1) &amp;= P(A_{\\Pi}\\text{ succ}) \\\\P(\\mathcal{D}^{f(\\cdot)}(1^{n}) = 1) &amp;= P(A_{\\widetilde{\\Pi}}\\text{ succ})\\end{align*}P(DFk​(⋅)(1n)=1)P(Df(⋅)(1n)=1)​=P(AΠ​ succ)=P(AΠ​ succ)​This means that:∣P(DFk(⋅)(1n)=1)−P(Df(⋅)(1n)=1)∣=∣P(AΠ succ)−P(AΠ~ succ)∣≥∣1poly(n)−negl(n)∣≥1poly(n)\\begin{align*}&amp;\\quad |P(\\mathcal{D}^{F_{k}(\\cdot)}(1^{n}) = 1) -P(\\mathcal{D}^{f(\\cdot)}(1^{n}) = 1)| \\\\&amp;= |P(\\mathcal{A}_{\\Pi}\\text{ succ}) - P(\\mathcal{A}_{\\widetilde{\\Pi}}\\text{ succ})| \\\\&amp;\\geq |\\frac{1}{\\text{poly}(n)} - \\text{negl}(n)| \\geq \\frac{1}{\\text{poly}(n)}\\end{align*}​∣P(DFk​(⋅)(1n)=1)−P(Df(⋅)(1n)=1)∣=∣P(AΠ​ succ)−P(AΠ​ succ)∣≥∣poly(n)1​−negl(n)∣≥poly(n)1​​This is contradict to that FkF_{k}Fk​ is PRF!","tags":["笔记","TCS","伪随机","私钥加密"],"categories":["理论计算机科学导引"]},{"title":"网原笔记6","path":"/2024/05/22/网原笔记6/","content":"计算机网络原理 笔记 6 链路层与局域网 链路层 运行链路层协议的任何设备称为节点，相邻节点的通信信道称为链路，在链路上传递的数据报被封装为链路层帧 可能提供的服务 成帧：将网络层数据添加一些首部字段封装为链路层帧 链路接入：由MAC规定了帧在链路上的传输规则 可靠交付：保证无差错地传递网络层数据报，但是对于部分低比特差错链路，可靠性是不必要的 差错检测和纠正：由于硬件设备原因可能会导致部分比特被错误传输，因此可以在帧中包含相应差错检测/纠正比特来检测 实现 通常通过网络适配器（又称为网络接口卡）来实现 网络适配器 差错检测与纠正 接收方需要判断接收到的数据是否是原始数据，通常通过EDC来实现，但是需要注意的是EDC本身也有可能被损坏，并且即使使用了EDC也可能出现漏检的情况 差错检测与纠正 常见的EDC方法包括奇偶校验、检验和与循环冗余检测 奇偶校验 增加一位校验位，用于指示原始数据中’1’的个数的奇偶性，但是这只能判断出数据中出现了奇数个比特差错 为了提升其鲁棒性，采用二维奇偶校验的方式，即将原始数据划分为一个矩阵，分别检测每一行与每一列的奇偶校验和，这样可以检测并纠正单个比特差错，并且可以检测双比特差错 二维奇偶校验 检验和 与运输层协议中内容相似，将数据按字节求和取反码 循环冗余检测 现代广泛使用CRC编码，主要思想为将数据流看做系数为0, 1的多项式 CRC编码 如上图，编码操作为： 发送方与接收方协商r+1r+1r+1位比特模式作为生成多项式，记为GGG，要求GGG的最高位必须是1 对于数据段DDD，发送方将其附加rrr个比特RRR，得到d+rd+rd+r位比特模式，使其可以被GGG整除 接收方检测是否可以整除即可 因此关键问题在于如何选取RRR使得： (D&lt;&lt;r)⊕R=nG (D &lt;&lt; r) \\oplus R = nG (D&lt;&lt;r)⊕R=nG 该式可化简为： D&lt;&lt;r=nG⊕R D &lt;&lt; r = nG \\oplus R D&lt;&lt;r=nG⊕R 在CRC中，所有的加法和减法都等价于异或操作，因此乘法和出发需要对应的变化（竖式中的加减法也变成了异或） 因此，可以计算RRR为： R=(D&lt;&lt;r)(modG) R = (D &lt;&lt; r) \\pmod{G} R=(D&lt;&lt;r)(modG) 国际标准的生产多项式为： GCRC-32=10000001001100000010001110110110111 G_{\\text{CRC-32}} = 10000001001100000010001110110110111 GCRC-32​=10000001001100000010001110110110111 CRC可以检测出所有的奇数位、双比特、不大于∣G∣|G|∣G∣长度的错误 多路访问链路和协议 由于所有的节点都可以传输帧，因此同时被接收的信号会在接收方处发生碰撞，导致所有信号丢失，因此需要多路访问协议用于协调多个发送和接收节点共享一个信道的访问。 其应当具有的特性为： 仅有一个节点发送数据时，需要使用完整信道 有多个节点发送数据时，每个节点吞吐量可以趋近于平均 协议是分散的：不会因一个节点崩溃而崩溃 造价便宜！ 可以划分为信道划分协议、随机接入协议与轮流协议 信道划分协议 采用时分复用或频分复用的方式，平均划分时间或频率 时分复用与频分复用 TDM/FDM的特点： 消除了碰撞，并且非常公平 在及诶单书很少的时候效果很差 另一种信道划分协议为码分多址，为每个节点分配一种不同的编码，如果编码选择合适，即可同时传输同时接收并且互不干扰 随机接入协议 每个节点都以信道最大速率发送分组，当发生碰撞时，等待一个随机时延并重发该分组，直到发送成功。 随机试验种类繁多，具体介绍ALOHA协议与载波侦听多路访问(CSMA)协议 时隙ALOHA 假设： 所有帧大小相同，记为LLL 时间被划分为等长的时隙，每个时隙可以发送一帧 只在时隙起点传输帧 节点之间同步时隙信息 如果在一个时隙中发生了碰撞，则时隙结束时所有节点可以检测到碰撞 则时隙ALOHA的操作如下： 当有数据需要发送时，等待下一个时隙起点传输 如果没有碰撞则万事大吉 反之，则在后续的每个时隙起点独立地以 ppp 的概率重传，直到成功发送 定义成功时隙为恰好只有一个节点传输的时隙，成功时隙占所有时隙的比例为效率，则N(N≫1)N(N\\gg 1)N(N≫1)个节点的效率为： f(p)=CN1p(1−p)N−1=Np(1−p)N−1 f(p) = C_{N}^{1}p(1-p)^{N-1} = Np(1-p)^{N-1} f(p)=CN1​p(1−p)N−1=Np(1−p)N−1 极值点为： p=1N p = \\frac{1}{N} p=N1​ 因此： f(p)≤(1−1N)N−1lim⁡N→∞f(p)≤1e\\begin{align*} f(p) &amp;\\leq (1 - \\frac{1}{N})^{N - 1} \\\\ \\lim\\limits_{N\\to\\infty}&amp;f(p) \\leq \\frac{1}{e} \\end{align*} f(p)N→∞lim​​≤(1−N1​)N−1f(p)≤e1​​ 所以当节点数充分大时，效率最高约为37% ALOHA 纯ALOHA将不考虑时隙问题，在分组到达时则立刻进行传输，碰撞时立即以概率 ppp 重传分组，以概率 1−p1-p1−p 等待一个分组传输时间，重复循环直到成功发送 若一个节点想要成功发送，则必须要保证其发送时间的前后各一个分组传输时间之内不能有其他节点发送，因此可以计算得其最大效率为： lim⁡N→∞f(p)≤12e\\lim\\limits_{N\\to\\infty}f(p) \\leq \\frac{1}{2e} N→∞lim​f(p)≤2e1​ CSMA 模仿人类聚会时的发言，CSMA新增了如下协议： 说话之前先听：在传输之前先“听”信道，如果有其他节点正在发送则等待，直到一段时间内没有传输 而CSMA/CD则进一步增加了如下协议 和别人同时开始说话时停止说话：节点在传输过程中保持“听”信道，当有其他节点也开始传输的时候立刻停止传输，等待一段随机时间后，进入上一种状态 采用时空图说明： 时空图，其中B与D碰撞 可以看出，决定其碰撞发生的概率即决定性能的关键因素为信道传播时延，即在节点之间通信的效率 CSMA/CD的一些讨论 讨论两个问题： 每次检测到碰撞后应该等待多久 CSMA/CD效率如何 对于等待时间的问题，采用二进制指数后退算法，具体为： 在nnn次碰撞之后，令K∼U({0,1,…,2n−1})K \\sim \\text{U}(\\{0, 1, \\dots, 2^{n} - 1\\})K∼U({0,1,…,2n−1})，并且等待发送512K512K512K比特所需要的时间 对于效率问题，定义CSMA/CD的效率为： 节点数和分组数充分多的时候，分组能够无碰撞传输所占有的时间比例 定义dpropd_{\\text{prop}}dprop​为两个适配器之间传递信号的最大时间，dtransd_{\\text{trans}}dtrans​为传输一个最大以太网帧的时间，则CSMA/CD的效率为： f=11+5dprop/dtrans f = \\frac{1}{1 + 5d_{\\text{prop}}/ d_{\\text{trans}}} f=1+5dprop​/dtrans​1​ 轮流协议 是一大类能够满足大量节点平均享有吞吐量的协议，主要讨论轮询协议与令牌传递协议 轮询协议 指定主节点，主节点可以轮询每个节点，告知其可以传输的最大帧数 这种方法引入了轮询时延，并且是集中式的（主节点坏了就似了） 令牌传递协议 讲一个称之为令牌的特殊帧在节点之间以一个固定的次序进行交换，每个节点持有令牌当且仅当自己需要发送帧，反之立刻传递给下一节点 同样的，该协议单个节点的故障可能导致整体的崩溃 DOCSIS 一个实际应用的综合性协议 上行与下行信道 DOCSIS定义了电缆数据网络体系结构及其协议，对于上行信道与下行信道都采用FDM，每个信道均为广播信道，并且下行信道不会存在多路访问问题，主要考虑上行信道 上行信道被划分时间间隔，每个时间间隔包含微时隙序列（类似TDM） CMTS发送MAP报文指定特定的调制解调器在特定时间间隔发送分组 调制解调器们在一组特殊的微时隙间隔内向CMTS发送请求帧，向其请求分配微时隙用于发送数据 请求帧以随机接入的方式发送，当其发生碰撞时（调制解调器只能通过下行信道的数据推测是否碰撞），采用二进制指数回退算法决定等待时间并重新发送 交换局域网 链路层寻址 MAC地址 链路层地址的所有者并非主机或路由器，而是其中的适配器（网络接口），链路层地址有多种称呼，包括LAN地址，物理地址，MAC地址，常用的为MAC地址，长度为6字节即48bits MAC地址示意图 每个适配器拥有唯一MAC地址，其唯一性由IEEE保证（付费！） 与IP不同，MAC具有扁平化特征，即不再具有类似IP的层次化特征，其地址值是随着设备而固定下来的 MAC寻址方法为，向帧中插入目的MAC地址后发送到局域网上，当接收时，检测目的MAC地址是否与己方一致，一致则解析数据，反之则丢给局域网，如果需要广播则使用广播地址（全1） ARP 地址解析协议用于实现网络层地址与链路层地址之间的转换，是一个介于链路层和网络层之间的协议 ARP只解析同一个子网下的IP地址，不能解析任意的IP地址 工作方式为询表，ARP表中存有IP地址到MAC地址的映射，以及该表项的过期时间，当表项缺失时，会广播ARP查询分组，向子网上所有的主机询问此IP地址对应的MAC地址，相应的适配器接收到报文后会发送ARP响应报文 注意：查询报文是广播而发送报文不是 向子网外发送 通过路由器的转发表实现不同子网之间的转换，每个子网中的主机只需要把数据传到路由器就可以了，不然就会到达数据报天国！ 以太网 简易、便宜、首发、告诉，在物理结构上采用交换机（之前是总线与集线器），有效避免了碰撞 帧结构 以太网帧架构 字段意义如下： 数据字段（46-1500字节）：过长的数据需要被分片 目的地址（6字节）：目的地MAC 源地址（6字节）：发送方MAC 类型字段（2字节）：用于指示需要使用的网络层协议 CRC（4字节）：检测差错 前同步码（8字节）：AA-AA-AA-AA-AA-AA-AA-AB，用于同步时钟，最后两个1用于指示数据到来 以太网是无连接的，没有类似TCP的握手需求，也没有重传机制 以太网技术 100MHz以太网标准 一种标准是IEEE 803.2z： 使用上述帧格式 允许点对点以及广播 使用CSMA/CD来共享广播信道 点对点信道允许40Gbps全双工 链路层交换机 主要任务是接收入链路层帧并将其转发到出链路，对于主机来说是透明的，可能会存在过量数据，因此设有缓存 转发与过滤 过滤是决定一帧应该被转发到某个接口还是被丢弃，转发决定帧应该被导向哪个接口，这两项操作由交换机表完成，每个表项包含： MAC地址 通往该地址的交换机接口 表项存在时间 当接口xxx到达一帧链路层帧后，交换机表的索引过程如下： 如果不存在目的地MAC地址对应的表项，则广播 如果存在，但是表中的接口是xxx，也即想发给自己所在子网，直接丢弃（过滤） 如果存在，并且接口为y≠xy eq xy=x，则转发到yyy的输出缓存 自学习 交换机表的建立是自动、自治、动态建立的，即自学习，方法如下： 初始表为空 对于每个接口存储到的每个入帧，存储：源MAC地址、到达的接口、当前时间 一段时间后，如果没有接收到该MAC为源的帧，则删除之 交换机是即插即用的、双工的 链路层交换机的性质 清除碰撞：有缓存，网段上至多同时传输一帧 异质的：链路之间彼此隔离，不同链路能够以不同速率在不同媒体上运行 管理：更加安全，易于管理，例如可以自行解决异常适配器 交换机与路由器 交换机是链路层的分组交换机，路由器是网络层的分组交换机 路由器与交换机 虚拟局域网 交换局域网的缺点： 缺乏流量隔离：广播可以被整个机构的网络接收到，无法局部广播 交换机的无效使用：如果组小而交换机大，每个交换机将会有大量端口被浪费 用户管理：用户换组或多组将会导致复杂的物理布线变化 因此引入虚拟局域网，由网管将一个交换机的不同端口划分为不同组，每个VLAN中的端口形成一个广播域 VLAN实例 但是这样会导致VLAN之间完全隔离，常用的解决办法是引入一个路由器进行VLAN间的通信，与交换机中某个空闲的端口连接，该端口可以视作同时属于多个VLAN 如果需要多个交换机，并且交换机之间的VLAN需要互联（也许就是同种VLAN），采用的方法为VLAN干线互联而并非将各个VLAN分别互联，每个干线端口同时属于所有VLAN 为了确保帧可以正确跨越干线，对以太网帧进行扩展 扩展以太网帧 增加字段有： 标签协议控制符（2字节）：固定81-00 标签控制信息（2字节）：包含12比特的VLAN标识符字段和3比特的优先权字段 新增字段被干线端口添加与删除 EVPN 链路层交换机在逻辑上联结在一起，并且使用更顶层的协议进行通信，如将链路层帧包裹在IP数据报中 链路虚拟化 引入多协议标签交换，用于改善IP路由器交换速度，目标是通过选择性的标识数据报，并允许路由器通过固定长度标签转发数据报 MPLS首部 MPLS会扩展链路层帧，增加的首部字段位于链路层和网络层首部之间，新增字段包括： 标签 实验字段（3比特）：预留 S字段：用于指示MPLS首部栈的结束 TTL：寿命字段 MPLS使能的路由器称为标签交换路由器，其在转发表中查找MPLS标签并将数据报传递给相应的输出接口进行转发，但是这要求通信的两个路由器都是MPLS使能的 利用MPLS进行通信，其中R1234都是MPLS使能的 从上图可以看出，MPLS可以提供多条路径（同样的也可以人为限制路径），例如图中R4-A具有两条MPLS路径，这被称为流量工程 相比于传统网络，MPLS的优势有： 交换速度增加 流量工程 转发路径的快速恢复 VPN 数据中心网络 数据中心：包含大量主机，称为刀片，刀片堆叠在机架上，每个机架顶部有一台交换机，交换机之间互联，数据中心网络需要支持两种类型的流量——外部通信与内部交换，由边界路由器负责与公共因特网相连 数据中心网络示意 负载均衡 外部请求首先被重定向到负载均衡器，向主机发送请求并在主机之间尽量保持负载均衡，基于目的端口号和IP地址做决定 同时其提供了客户与数据中心网络之间的屏障，形成了类似NAT的效果（公网内网转换） 等级体系结构 大型数据中心需要使用路由器和交换机等级结构，以保证交换机工作的稳定性（如上图），其中每台接入路由器下的主机构成了子网，并且可以被进一步划分为多个VLAN子网 但是这种等级体系结构可能导致主机之间容量首先，即交换机速率不足以支撑并发时主机之间能够以最大速率通信 发展趋势 全连接拓扑示意 克服等级结构缺陷可以采用FCT，即第一层和第二层之间全连接，提升了主机之间不相交的路径数，并且未直接连接到同一个交换机的机架之间的通信逻辑上是等价的 另一种修改方式称为模块化数据中心，略过 目前数据中心常用的协议有： 链路层：RoCE 运输层：DCTCP/DCQCN 路由选择：SDN","tags":["笔记","网原","链路层","局域网"],"categories":["计算机网络原理"]},{"title":"IAI-对抗搜索","path":"/2024/05/20/IAI-对抗搜索/","content":"人智导 对抗搜索 对抗搜索 对于一些分支极多的对抗式问题，穷举法所需要消耗的时间、空间资源无法承受，无法实现，因此考虑充分的剪枝 极小-极大模型 进行有限步内的穷举，从根节点出发，叶节点标记得分，并且期望每位选手都选择对于自己最有利的走法，最后选择期望得分最高的一步 极大-极小模型示例图 图中，两种图形代表两位选手，分别记为A,BA, BA,B，叶节点上所标记的为AAA在四步之后的期望得分，每位选手每步都是期望自身得分尽量高（对方得分尽量低） 但是极小极大仍然没有剪枝，时空资源仍然无法承受 α-β剪枝算法 α\\alphaα-β\\betaβ 剪枝算法如下 α\\alphaα为极大节点（我方选手尽量多得分）的下界 β\\betaβ为极小节点（我方选手尽量少得分）的上界 后辈 β≤\\beta \\leqβ≤ 祖先 α\\alphaα 时，α\\alphaα 剪枝 后辈 α≥\\alpha \\geqα≥ 祖先 β\\betaβ 时，β\\betaβ 剪枝 实例如下：注意比较时需要和祖先节点而不是父节点比较 alpha-beta剪枝实例 每次α\\alphaα-β\\betaβ 剪枝只能得到下一步的走法 局限性：非常依赖局面估计（也就是叶节点的得分）的准确性，需要大量的专家知识与人工整理 蒙特卡洛(MCTS) 基本思想： 可能出现的状态用状态树表示 逐步扩展树节点 父节点利用子节点的结果 随时得到行为评价 基本过程为： 选择 →\\to→ 扩展 →\\to→ 模拟 →\\to→ 回传 选择策略 考虑两方面因素： 充分探索尚未探索的节点 利用效果尽量好的节点 因此采用多臂老虎机模型 拥有kkk个拉杆的老虎机，拉动每个拉杆的收益相互独立并且遵循一定分布，求如何使得受益最大化 采用信心上限算法：每次选择信心上限最大的节点，节点jjj的信心上限计算方式为： Ij=X‾j+c2ln⁡(n)Tj(n)I_{j} = \\overline{X}_{j} + c\\sqrt{\\frac{2\\ln(n)}{T_{j}(n)}} Ij​=Xj​+cTj​(n)2ln(n)​​ 其中参数含义为： ccc：调节参数 nnn：访问次数 Tj(n)T_{j}(n)Tj​(n)：此时节点 jjj 被访问的次数 X‾j\\overline{X}_{j}Xj​：此时节点 jjj 的平均收益 以围棋为例，每一次模拟可以看成是随机落点，平均收益可以看成是胜率，如下图，其中为简便令c=0c = 0c=0，最终选择根节点的子节点中胜率最大的节点作为下一步： 采用UBC选择的MCTS 注意： 每个节点的胜率是站在己方的角度考虑的！ AlphaGo 为了解决MCTS的盲目性问题（随机落子），将神经网络与蒙特卡洛结合起来，使用了策略网络与估值网络两种神经网络 策略网络 一个神经网络，用于提供行棋概率 输入：48个通道，每个通道大小19*19，记录了棋局的相关信息输出：棋盘上每个节点的行棋概率 策略网络 策略网络可以看成是一个361类别分类问题，通过人类棋手的棋谱进行训练，损失函数为 L(w)=−talog⁡(pa)L(w) = -t_{a}\\log(p_{a}) L(w)=−ta​log(pa​) 其中tat_{a}ta​为实际落子概率，pap_{a}pa​为网络落子概率 估值网络 一个神经网络，用于提供棋局收益 输入：49个通道，每个通道大小19*19，记录了棋局的相关信息（比策略网络多一个）输出：当前棋局收益 ∈[−1,1]\\in [-1, 1]∈[−1,1] 估值网络 估值网络可以看成回归问题，也是通过人类棋手棋谱进行训练，损失函数： L(w)=(R−V(s))2L(w) = (R - V(s))^{2} L(w)=(R−V(s))2 其中RRR为实际收益，111 胜 −1-1−1 负，V(s)V(s)V(s)为网络输出 与MCTS融合 给定参数 λ\\lambdaλ 每次模拟收益为： vi(s)=λvalue(s)+(1−λ)sim(s)v_{i}(s) = \\lambda \\text{value}(s) + (1 - \\lambda)\\text{sim}(s) vi​(s)=λvalue(s)+(1−λ)sim(s) 其中 value(s)\\text{value}(s)value(s) 为估值网络输出，sim(s)\\text{sim}(s)sim(s) 为模拟结果 因此定义平均收益： Q(sa)=∑i=1nvi(sa)nQ(s_{a}) = \\frac{\\sum\\limits_{i=1}^{n}v_{i}(s_{a})}{n} Q(sa​)=ni=1∑n​vi​(sa​)​ 定义探索项： u(sa)=c⋅p(sa)N(s)N(sa)+1u(s_{a}) = c \\cdot p(s_{a})\\frac{\\sqrt{N(s)}}{N(s_{a}) + 1} u(sa​)=c⋅p(sa​)N(sa​)+1N(s)​​ 其中： sas_{a}sa​代表棋局sss在aaa处落子后的棋局 N(s)N(s)N(s)代表对于棋局sss的模拟次数 p(s)p(s)p(s)代表策略网络对于sss的输出 ccc为系数 信心上限切换为： Ij=Q(sa)+u(sa)I_{j} = Q(s_{a}) + u(s_{a})Ij​=Q(sa​)+u(sa​) MCTS过程如下： 信息：每个节点记录收益、到达该节点概率与被选择次数 选择：从根节点开始，每次选择子节点中信心上限最大的节点，直到叶节点即停止并选中 生成：生成选中节点的所有叶节点（也即所有可能的落子），并规定了最大的节点深度 模拟：采用推演策略网络（更快），计算其viv_{i}vi​ 回传：注意正负号（即注意行棋是双方依次进行） 最终将根节点的子节点中，被选择次数最多的节点作为选择 深度强化学习方法 强化学习：学习“做什么可以使得收益最大化”深度强化学习：利用深度学习实现的强化学习 以围棋为例，通过自己博弈训练策略网络，三种实现方法： 策略梯度：学习每个点获胜的概率 价值评估：学习每个点获得最大收益的概率 演员评价方法：学习到每个落子点获得最大收益增量的概率 策略梯度 数据由自我博弈产生，损失函数为： L(w)=−talog⁡(pa)L(w) = - t_{a}\\log(p_{a}) L(w)=−ta​log(pa​) 其中，pap_{a}pa​为当前棋局在aaa处下棋的概率，ta∈{−1,1}t_{a}\\in\\{-1, 1\\}ta​∈{−1,1}为胜负值 基于策略梯度的强化学习流程 注意点： 强化学习过程中，每个样本只使用一次 该方法学习到的是每个可落子点行棋的获胜概率 价值评估 输入为当前棋局和行棋点，输出为该行棋点的价值，在[−1,1][-1, 1][−1,1]之间，数据也是自我博弈产生，损失函数为： L(w)=(R−V(s,a))2L(w) = (R - V(s, a))^{2} L(w)=(R−V(s,a))2 其中，RRR为胜负值，V(s,a)V(s, a)V(s,a)为棋局sss在aaa处落子后网络的输出 演员-评价方法 利用收益增量评价一步棋的好坏： A=Q(s,a)−V(s)A = Q(s, a) - V(s) A=Q(s,a)−V(s) 其中，V(s)∈[−1,1]V(s)\\in[-1, 1]V(s)∈[−1,1]为棋局sss的预期收益，Q(s,a)∈[−1,1]Q(s, a)\\in[-1, 1]Q(s,a)∈[−1,1]为sss在aaa处行棋之后的收益，在实际中常去Q(s,a)=RQ(s, a) = RQ(s,a)=R为胜负值，最终AAA越大收益越好 演员-策略网络，评价-估值网络 损失函数为： L(w)=L1(w)+λL2(w)=A2−λAlog⁡(pa)\\begin{align*} L(w) &amp;= L_{1}(w) + \\lambda L_{2}(w) \\\\ &amp;= A^{2} - \\lambda A\\log(p_{a}) \\end{align*} L(w)​=L1​(w)+λL2​(w)=A2−λAlog(pa​)​ AlphaGo Zreo 将估值网络和策略网络合并为“双输出”网络 输入：17个通道，记录8个棋局，每个棋局2通道，1个通道记录行棋方输出：策略网络输出362维，增加的一维为放弃；估值网络输出棋局的估值 Alpha-Zero原理 与MCTS融合 与AlphaGo基本相同，差别如下： 模拟被估值网络完全取代，模拟收益vi(s)v_{i}(s)vi​(s)即为估值网络的输出 规定了总模拟次数 结合MCTS与深度强化学习 Alpha-Zero中的深度强化学习 损失函数为： Lvalue=(R−v)2Lstrategy=−∑i=1362πilog⁡(pi)L=Lvalue+Lstrategy+∣∣θ∣∣2\\begin{align*} L_{value} &amp;= (R - v)^{2} \\\\ L_{strategy} &amp;= -\\sum\\limits_{i=1}^{362}\\pi_{i}\\log(p_{i}) \\\\ L &amp;= L_{value} + L_{strategy} + ||\\theta||^{2} \\end{align*} Lvalue​Lstrategy​L​=(R−v)2=−i=1∑362​πi​log(pi​)=Lvalue​+Lstrategy​+∣∣θ∣∣2​ 其中RRR为胜负值，vvv为估值网络输出，πi\\pi_{i}πi​为MCTS给出的该走法概率，pip_{i}pi​为策略网络给出的该走法概率 引入多样性 人为引入噪声，增加策略网络输出的随机性，通常增加一个狄利克雷分布，生成一些大多值为0，小部分值较大的随机变量，并修正策略网络输出为： p⇐λp+(1−λ)pdp \\Leftarrow \\lambda p + (1 - \\lambda) p_{d} p⇐λp+(1−λ)pd​","tags":["笔记","IAI","对抗搜索"],"categories":["人工智能导论"]},{"title":"TCS-Lecture-B","path":"/2024/05/15/TCS-Lecture-B/","content":"TCS: Randomized Computation Randomized Computation Randomized Algorithm A randomized algorithm outputs the correct value with good probability on every possible input. Matrix multiplication Input matrix A,B,CA, B, CA,B,C, decide if C=ABC = ABC=AB Obviously there is a deterministic and polynomial algorithm for this. A random algorithm: Freivalds’ algorithm Repeat the following for kkk times.Randomly choose v∈{0,1}nv\\in \\{0, 1\\}^{n}v∈{0,1}nCompute (d=A(Bv)−Cv)(d = A(Bv) - Cv)(d=A(Bv)−Cv)Reject if d≠0d eq 0d=0Accept We have that this algorithm can solve this problem in O(kn2)O(kn^{2})O(kn2) time with a probability of failure ≤2−k\\leq 2^{-k}≤2−k Proof: If AB≠CAB eq CAB=C, we prove P(d=0)≤12P(d = 0) \\leq \\frac{1}{2}P(d=0)≤21​ for each time.So D=AB−C≠0D = AB - C eq 0D=AB−C=0. Let Dij≠0D_{ij} eq 0Dij​=0The iii-th entry of ddd holds that:di=∑Dikvk=Dijvj+∑k≠jDikvkd_{i} = \\sum D_{ik}v_{k} = D_{ij}v_{j} + \\sum\\limits_{k eq j}D_{ik}v_{k}di​=∑Dik​vk​=Dij​vj​+k=j∑​Dik​vk​Let s=∑k≠jDikvks = \\sum\\limits_{k eq j}D_{ik}v_{k}s=k=j∑​Dik​vk​, so:P(di=0)=P(di=0 ∣ s=0)P(s=0)+P(di=0 ∣ s≠0)P(s≠0)≤P(vi=0)P(s=0)+P(vi=1)P(s≠0)≤12(P(s=0)+P(s≠0))≤12\\begin{align*}P(d_{i} = 0) &amp;= P(d_{i} = 0 \\,|\\, s = 0)P(s = 0) \\\\&amp;\\quad +P(d_{i} = 0 \\,|\\, s eq 0)P(s eq 0) \\\\&amp;\\leq P(v_{i} = 0)P(s = 0) + P(v_{i} = 1)P(s eq 0)\\\\&amp;\\leq \\frac{1}{2}(P(s = 0) + P(s eq 0)) \\leq \\frac{1}{2}\\end{align*}P(di​=0)​=P(di​=0∣s=0)P(s=0)+P(di​=0∣s=0)P(s=0)≤P(vi​=0)P(s=0)+P(vi​=1)P(s=0)≤21​(P(s=0)+P(s=0))≤21​​So P(d=0n)≤P(di=0)≤12P(d = 0^{n}) \\leq P(d_{i} = 0) \\leq \\frac{1}{2}P(d=0n)≤P(di​=0)≤21​ Maxcut Approximation The MAX-CUT problem is NP-Complete. So our task is to find a cut CCC whose size is not far from the optimal one C∗C^{*}C∗. If sizeC≥α sizeC∗\\text{size}_C \\ge \\alpha\\,\\text{size}_{C^*}sizeC​≥αsizeC∗​, we call CCC is an α\\alphaα-approximation, then we have an easily way to find 12\\frac{1}{2}21​-approximation, which is universal randomly distribute each vertex into set 000 or 111. E(sizeC)=E∑{u,v}∈E1xu≠xv=12∣E∣≥12sizeC∗.\\begin{equation*} \\mathbb{E}(\\text{size}_C) = \\mathbb{E} \\sum_{\\{u, v\\} \\in E} 1_{x_u e x_v} = \\frac{1}{2} |E| \\ge \\frac{1}{2} \\text{size}_{C^*}. \\end{equation*} E(sizeC​)=E{u,v}∈E∑​1xu​=xv​​=21​∣E∣≥21​sizeC∗​.​ This is just sufficient expection, but we can give an always-large-enough cut by conditional expection if we can compute this equation efficiently. E(sizeC(x1,…,xi,Xi+1,…Xn)),\\begin{equation*} \\mathbb{E}(\\text{size}_C(x_1, \\ldots, x_i, X_{i+1}, \\ldots X_{n})), \\end{equation*} E(sizeC​(x1​,…,xi​,Xi+1​,…Xn​)),​ We maximize this in each choice. Derandomize Above algorithm uses nnn random choices, covering 2n2^{n}2n possibilities. We can try to reduce the randomness to a polynomial number of possibilities, we can derandomize the algorithm. Considering Universal hash function: Consider a family of hash functions H={h:U→R}\\mathcal{H} = \\{ h : U \\to R \\}H={h:U→R}. Universal hash functions are a family of functions with the random-like property while the size of the family is small. We can use a small seed to choose hash functions from the family. Pairwise independent hash functions. A family H={h:U→R}\\mathcal{H} = \\{h : U \\to R\\}H={h:U→R} is called Pairwise independent if for any distinct x1,x2∈Ux_{1}, x_{2}\\in Ux1​,x2​∈Uand any y1,y2∈Ry_{1}, y_{2}\\in Ry1​,y2​∈R, we have:Ph∈H(h(x1)=y1 and h(x2)=y2)=1∣R∣2.\\begin{equation*}P_{h \\in \\mathcal{H}} \\bigl( h(x_1) = y_1 \\text{ and } h(x_2) = y_2 \\bigr) = \\frac{1}{|R|^2}.\\end{equation*}Ph∈H​(h(x1​)=y1​ and h(x2​)=y2​)=∣R∣21​.​ A pairwise independent hash functions mapping {0,1}k\\{0, 1\\}^{k}{0,1}k to {0,1}\\{0, 1\\}{0,1}. H={h(x)=(ax+b)(mod 2) ∣ a∈{0,1}kb∈{0,1}}\\mathcal{H} = \\{ h(x) = (ax + b)(\\text{mod }2) \\,|\\, a \\in \\{0, 1\\}^{k}\\quad b\\in\\{0, 1\\} \\}H={h(x)=(ax+b)(mod 2)∣a∈{0,1}kb∈{0,1}} This family size is ∣H∣=2k+1|\\mathcal{H}| = 2^{k+1}∣H∣=2k+1. Assign k=⌈log⁡n⌉k = \\lceil \\log n\\rceilk=⌈logn⌉, then UUU can encoding each vertex in GGG. So ∣H∣≤2n|\\mathcal{H}| \\leq 2n∣H∣≤2n, which means we can go through all the hash function in H\\mathcal{H}H and output the maximized cut. BPP Define Prob TM as follows: A probabilistic Turing machine is a type of NTM in which each nondeterministic step is called a coin-flip step and has two legal next moves. We assign a probability 2−k2^{-k}2−k to each branch of the machine’s computation where kkk is the number of coin flips occur in the branch.The probability of the machine accepting the input is defined asP(M accepts w)=∑b:b is acceptingP(b).\\begin{equation*}P(M \\text{ accepts } w) = \\sum_{b:b \\text{ is accepting}} P(b).\\end{equation*}P(M accepts w)=b:b is accepting∑​P(b).​ This is equvilant to that each son of a vertex in NTM can be reach in the same probability. Define the error probability ε\\varepsilonε: If w∈Aw \\in Aw∈A, then P(M(w)=1)≥1−εP(M(w) = 1) \\geq 1 - \\varepsilonP(M(w)=1)≥1−εIf w∉Aw otin Aw∈/A, then P(M(w)=1)≤εP(M(w) = 1) \\leq \\varepsilonP(M(w)=1)≤ε Then we can define BPP\\text{BPP}BPP with error probability: BPP\\text{BPP}BPP is the class of languages decided by probabilistic polynomial-time Turing machines with an error probability of 13\\frac{1}{3}31​Actually, the 13\\frac{1}{3}31​ can be replaced by any constant exactly greatly than 12\\frac{1}{2}21​ BPP\\text{BPP}BPP can be also defined with verifier: A decision problem AAA is in BPP\\text{BPP}BPP if and only if there is a polynomial-time verifier VVV such that for all xxx, x∈Ax\\in Ax∈A if and only ifPr(V(x,r)=1)≥23.\\begin{equation*}P_{r} \\bigl(V(x, r) = 1 \\bigr) \\ge \\frac{2}{3}.\\end{equation*}Pr​(V(x,r)=1)≥32​.​ Error Reduction Any decision problem A∈BPPA\\in\\text{BPP}A∈BPP has a polynomial-time randomized algorithm whose error probability is 2−p(n)2^{-p(n)}2−p(n) where ppp is a polynomial and nnn is the input size. This can be proved by Chernoff bound or Sampling Theroem Circuits v.s. BPP Define SIZEn(s)\\text{SIZE}_{n}(s)SIZEn​(s): For a finite function g:{0,1}n→{0,1}g: \\{0, 1\\}^{n}\\rightarrow\\{0, 1\\}g:{0,1}n→{0,1}, g∈SIZEn(s)g \\in \\text{SIZE}_{n}(s)g∈SIZEn​(s) if there is a circuit of at most sss NAND gates computing ggg. And we define the restricted function: F↾n(x)=F(x) for x∈{0,1}n.\\begin{equation*} F_{\\restriction n} (x) = F(x) \\text{ for } x\\in \\{0,1\\}^n. \\end{equation*} F↾n​(x)=F(x) for x∈{0,1}n.​ Then FFF is non-uniformly computable in T(n)T(n)T(n) size, as F∈SIZE(T)F\\in\\text{SIZE}(T)F∈SIZE(T) if there is a sequence C0,C1,…C_{0}, C_{1}, \\dotsC0​,C1​,… of NAND circuits such that: CnC_{n}Cn​ computes F↾nF_{\\restriction n}F↾n​CnC_{n}Cn​ has at most T(n)T(n)T(n) gates when nnn is sufficiently large So the non-uniform analog P\\text{P}P: P/poly=⋃c∈NSIZE(nc)\\text{P}/\\text{poly} = \\bigcup\\limits_{c\\in\\mathbb{N}}\\text{SIZE}(n^{c})P/poly=c∈N⋃​SIZE(nc) Obviously, P⊊P/poly\\text{P}\\subsetneq\\text{P}/\\text{poly}P⊊P/poly and it can be proved BPP⊂P/poly\\text{BPP}\\subset\\text{P}/\\text{poly}BPP⊂P/poly as follows: Due to error reduction, A∈BPPA\\in \\text{BPP}A∈BPP has a polynomial-time randomized algorithm whose error probability is less than 2−n2^{-n}2−n, which means there is a verifier VVV, such that∀x Py(V(x,y)≠A(x))&lt;12n\\forall x \\,\\, P_{y}(V(x, y) eq A(x)) &lt; \\frac{1}{2^{n}}∀xPy​(V(x,y)=A(x))&lt;2n1​So due to the union bound:Py(∃x V(x,y)≠A(x))≤∑xPy(V(x,y)≠A(x))&lt;1P_{y}(\\exist x\\,V(x, y) eq A(x)) \\leq \\sum\\limits_{x}P_{y}(V(x, y) eq A(x)) &lt; 1Py​(∃xV(x,y)=A(x))≤x∑​Py​(V(x,y)=A(x))&lt;1As this probability is not 111, there must exist some y∗y^{*}y∗ for which ∀x V(x,y∗)=A(x)\\forall x\\, V(x, y^{*}) = A(x)∀xV(x,y∗)=A(x).Thus there exists a circuit with poly(n)\\text{poly}(n)poly(n) gates to caculate problem AAA beacuse y∗y^{*}y∗ is polynomial P = BPP &lt;= P = NP Sipser–Gács Theorem: BPP∈Σ2P∩Π2P\\text{BPP} \\in \\Sigma^{P}_{2} \\cap \\Pi_{2}^{P}BPP∈Σ2P​∩Π2P​, while the ΣP\\Sigma^{P}ΣP and ΠP\\Pi^{P}ΠP are defined as:ΣiP=∃∀∃…PΠiP=∀∃∀…P \\begin{align*} \\Sigma_{i}^{P} &amp;= \\exists\\forall\\exists\\dots \\text{P} \\\\ \\Pi_{i}^{P} &amp;= \\forall\\exists\\forall\\dots \\text{P} \\end{align*}ΣiP​ΠiP​​=∃∀∃…P=∀∃∀…P​ And we have the following theroem P=NP\\text{P} = \\text{NP}P=NP implies P=BPP\\text{P} = \\text{BPP}P=BPP The proof is diffcult with the technique ‘probabilistic method’ And there is also a theroem that reveals the relation between B\\text{B}B and BPP\\text{BPP}BPP Relations with P NP EXP We know P⊊EXP\\text{P} \\subsetneq \\text{EXP}P⊊EXP and BPP⊆EXP\\text{BPP} \\subseteq \\text{EXP}BPP⊆EXP Expected: P=BPP⊊NP⊆EXP\\text{P} = \\text{BPP} \\subsetneq \\text{NP} \\subseteq \\text{EXP}P=BPP⊊NP⊆EXP Extreme: P⊊NP⊆BPP=EXP\\text{P} \\subsetneq \\text{NP} \\subseteq \\text{BPP} = \\text{EXP}P⊊NP⊆BPP=EXP Extreme also: P=BPP=NP⊊EXP\\text{P} = \\text{BPP} = \\text{NP} \\subsetneq \\text{EXP}P=BPP=NP⊊EXP","tags":["笔记","TCS","随机计算"],"categories":["理论计算机科学导引"]},{"title":"网原单词表","path":"/2024/05/15/网原单词表/","content":"计算机网络原理 中-英对照表 packet：分组 circuit switching：电路交换 packet switching：分组交换 packet switch：分组交换机 rooter：路由器 linker layer：链路层交换机 store-and-forword transmission：存储转发传输 output buffer/queue：输出缓存/队列 queuing delay：排队时延 packet loss：丢包 forwarding table：转发表 routing protocol：路由转发协议 Frequency-Division Multiplexing Address(FDMA)：频分复用地址 Time-Division Multiplexing Address(TDMA)：时分复用地址 bandwidth：带宽 slient period：静默期 Internet service provider(ISP)：因特网提供商 Point of Presence(PoP)：存在点 multi-home：多宿 peer(P2P)：对等 Internet Exchange Point(IXP)：因特网交换点 content provider network：内容提供商网络 nodal processing delay：节点处理时延 queuing delay：排队时延 transmission delay：传输时延 propagation delay：传播时延 total nodal delay：节点总时延 traffic intensity：流量强度 instantaneous throughout：瞬时吞吐量 average throughout：平均吞吐量 bottleneck link：瓶颈链路 layer：分层 protocol stack：协议栈 top-down approach：自顶向下方法 application-layer：应用层 message：报文 transport-layer：运输层 segment：报文段 network-layer：网络层 datagram：数据报 link-layer：链路层 frame：帧 encapsulation：封装 payload field：有效载荷字段 malware：恶意软件 botnet：僵尸网络 self-replicating：自我复制 worm：蠕虫 Denial-of-Service(DoS) attack：拒绝服务攻击 Distributed Dos(DDoS)：分布式拒绝网络攻击 packet sniffer：分组嗅探器 IP spoofing：IP哄骗 application architexture：应用程序体系结构 data ceenter：数据中心 process：进程 socket：套接字 Appllication Programming Interface(API)：应用程序编程接口 port numbe：端口号 reliable data transfer：可靠数据传输 bindwidth-sensitive application：带宽敏感应用 elastic application：弹性应用 Secure Socket Layer：安全套接字层 HyperText Transfer Protocol(HTTP)：超文本传输协议 stateless protocol：无状态协议 persistent connection：持续连接 non-persistent connection：非持续连接 Round-Trip Time(RTT)：往返时间 request line：请求行 header line：首部行 entity body：实体体 Web cache：Web缓存器 proxy server：代理服务器 Simple Mail Transfer Protocol(SMTP)：简单邮件传输协议 Post Office Protocol-Version3(POP3)：第三版的邮局协议 Internet Mail Access Protocol：因特网邮件访问协议 authorization：特许 transaction：事务处理 update：更新 Domain Name System(DNS)：域名系统 host aliasing：主机别名 canonical hostname：规范主机名 mail server aliasing：邮件服务器别名 load distribution：负载分配 distant centralized database：远距离集中式数据库 torrent：洪流 chunk：块 unchoked：疏通 tit-for-tat：一报还一报 Dynamic Adaptive Streaming over HTTP(DASH)：经HTTP的动态适应性流 manifest file：告示文件 content distribution network(CDN)：内容分发网络 reliable data transfer：可靠数据传输 Automatic Repeat reQuest(ARQ)：自动重传请求 Positive acknowledgment(ACK)：肯定确认 negative acknowledgment(NCK)：否定确认 duplicate packet：冗余分组 alter-nating-bit protocol：比特交替协议 Go-Back-N(GBN)：回退N步 sliding-window protocol：滑动窗口协议 cumulative acknowledgmemt：累计确认 Transmission Control Protocol：传输控制协议 connection oriented：面向连接的 full-duplex service：全双工服务 three-way handshake：三次握手 Maximum Segment Size(MSS)：最大报文长度 Maximum Transmission Unit(MTU)：最大设置单元 piggybacked：捎带 Exponential Weighted Moving Average(EWMA)：指数加权移动平均 congestion control：拥塞控制 per-connection throughput：每连接的吞吐量 Available Bite Rate(ABR)：可用比特率 congestion window：拥塞窗口 self-clocking：自计时 Additive-Increase, Multiplicative-Decrease(AIMD)：加性增，乘性减 Explicit Congestion Notification：明确拥塞通告 Explicit Congestion Notification Echo：明确拥塞通告回显 forwarding：转发 routing：路由选择 forwarding table：转发表 Software Defined Network(SDN)：软件定义网络 best-effort service：尽力而为服务 Tenary Content Address Memory(TCAM)：三态内容可寻址寄存器 Active Queue Management(AQM)：主动队列管理 Random Early Detection(RED)：随机早期检测 packet scheduler：分组调度 non-preemptive priority ququeing：非抢占式优先权排队 round robin queuing discipline：循环排队规则 work-conserving queuing：保持工作排队 weighted fair queuing：加权公平排队 Maximum Transmission Unit(MTU)：最大传送单元 dotted-decimal notation：点分十进制记法 Classless Interdomain Routing(CIDR)：无类别域间路由选择 address aggreration：地址聚合 Dynamic Host Configuration Protocol(DHCP)：动态主机配置协议 Plug-and-play Protocol：即插即用协议 Nonzero Protocol：需配置协议 Network Address Translation(NAT)：网络地址转换 tunneling：建隧道 Link State(LS) Algorithm：链路状态算法 Distance Vector(DV) Algorithm：距离向量算法 link state broadcast：链路状态广播 Autonomous System(AS)：自治系统 Open Shortest Path First(OSPF)：开放最短路优先 Broder Gateway Protocol(BGP)：边界网关协议 Routing Information Protocol(RIP)：路由信息协议 anycast：任播 switch farbic：交换结构 northbound/southbound API：北向/南向API Internet Control Messsage Protocol(ICMP)：因特网控制报文协议 Simple Network Management Protocol(SNMP)：简单网络管理协议 Management Information Base(MIB)：管理信息库 Structure of Management Information(SMI)：管理信息结构 Prorocol Data Unit(PDU)：协议数据单元 framing：成帧 Medium Access Control(MAC)：媒体访问控制 network adapter：网络适配器 Network Interface Card(NIC)：网络接口卡 Error Detection and Correction(EDC)：差错检测和纠正 undetected bit error：未检出比特差错 parity bit：奇偶校验位 two-dimensional parity：二维奇偶校验 Cyclic Redundancy Check(CRC)：循环冗余检测 polynomial code：多项式编码 generator：生成多项式 point-to-point link：点对点链路 point-to-point protocol(PPP)：点对点协议 high-level data link control(HIDC)：高级数据链路控制 broadcast link：广播链路 myltiple access problem：多路访问问题 Myltiple Access Control(MAC)：多路访问控制 collide：碰撞 channel partitioning protocol：信道划分协议 random access protocol：随机接入协议 taking-turns protocol：轮流协议 time-frame：时间帧 slot：时隙 Code Division Multiple Access(CDMA)：码分多址 Carrier Sense Multiple Access(CSMA)：载波侦听多路访问 CSMA with Collision Detection(CSMA/CD)：具有碰撞检测的CSMA channel propagation delay：信道传播时延 binary exponential backoff：二进制指数后退 polling protocol：轮询协议 token-passing protocol：令牌传递协议 Cable Modem Termination System(CMTS)：电缆调制解调器端接系统 Data-Over-Cable Service Interface CMTS(DOCSIS)：数据经电缆服务接口 Address Resolution Protocol(ARP)：地址解析协议 repeater:：转发器 filtering：过滤 forwording：转发 self-learning：自学习 aging-time：老化期 plug-and-play device：即插即用设备 jabbering：快而含糊的 switch poisoning：交换机毒化 Virtual Local Network(VLAN)：虚拟局域网 VLAN trunking：VLAN干线互联 Tag Protocol Identifier(TPID)：标签协议标识符 Multiprotocol Label Switchig(MPLS)：多协议标签交换 Ether VPN(EVPN or VXLAN)：链路层虚拟专用网络 Virtual Circuit(VC)：虚拟电路 label-switched router：标签交换路由器 Virtual Private Network(VPN)：虚拟专用网 traffic engineering：流量工程 data center network：数据中心网络 Top of Rack(TOR)：机架顶部交换机 blade：刀片 board router：边界路由器 load balancer：负载均衡器 hierachy of router and swtich：路由器和交换机等级结构 fully connected topology(FCT)：全连接拓扑 Modular Data Center(MDC)：模块化数据中心 Remote DMA(RDMA) over Converged Ethernet(RoCE)：??? base station：基站 cell tower：蜂窝塔 Access Point(AP)：接入点 infrastructure mode：基础设施模式 ad hoc network：自组织网络 handoff：切换 single/Multiple hop：单/多跳 mesh network：网状网络 mobile ad hoc network(MANET)：移动自组织网络 vehihcular ad hoc network(VANET)：车载自组织网络 path loss：路径损耗 multipath propagation：多径传播 coherence time：相干时间 Signal-to-Noise Ratio(SNR)：信噪比 Bit Error Ratio(BER)：比特差错率 hidden terminal problem：隐藏终端问题 fading：衰减 chipping rate：码片速率 Basic Service Set(BSS)：基本服务集 Service Set Identifier(SSID)：服务集标识 WiFi jungle：WiFi丛林 associate：关联 beacon frame：信标帧 active/passive scanning：主动/被动扫描 CSMA with Collision Avoidance(CSMA/CA)：具有碰撞避免的CSMA link-layer acknowledgement：链路层确认 Short Inter-Frame Spacing(SIFS)：短帧间间隔 Distributed Inter-Frame Spacing(DIFS)：分布式帧间间隔 Request to Send(RTS)：请求发送 Clear to Send(CTS)：允许发送 Wireless Personal Area Network(WPAN)：无线个人域网络 Frequency-Hopping Spread Spectrum(FHSS)：跳频扩展频谱 piconet：皮克网 Long-Term Evolution(LTE)：长期演进 Mobile divice(UE)：移动设备 Base station(eNode-B)：基站 Mobility Management Entity(MME)：移动管理实体 Home Subscriber Service(HSS)：家庭订阅者服务 Serving/PDN Gateway(S/P-GW)：服务/PDN网关 Mobile Subscriber Identity(IMSI)：移动订阅者身份 Subscriber Identity Module(SIM)：订阅者身份模型 Radio Access Network(RAN)：无线接入网络 Orthogonal Frequency Division Multiplexing(OFDM)：正交频分复用 All-IP Enhanced Packet Core(EPC)：全IP加强分组核 Packet Data Convergence(PDC)：数据收敛 Radio Link Control(PLC)：无线链路控制 GPRS Tunneling Protocol(GTP)：GPRS信道协议 Massive Machine Type Communications(mMTC)：大信息传输 Ultra-reliable and low latency communications(URLLC)：高可信低延迟交流 Multiple Directional Antennae(MIMO)：什么什么天线","tags":["笔记","网原"],"categories":["计算机网络原理"]},{"title":"网原笔记5","path":"/2024/05/15/网原笔记5/","content":"计算机网络原理 笔记 5 控制平面 路由选择算法 在路由器中寻找到最短路径，对于一个路由器，主要寻找到将其数据转发到其他路由器所需要的最短路径 算法分类方式 根据信息量 集中式路由选择：全局，了解该路由网络的全部信息并据此进行计算 分散式路由选择：局部，每个节点只知道与自己到相邻接点的花销 根据可变性 静态：路由基本不随时间变化 动态：随着网络流量或拓扑变化而动态改变路径，更加方便但是受一些特殊问题的影响 对负载敏感性： 敏感：趋近于绕开拥塞链路 迟钝：拥塞无影响，现代多采用这种，原因是链路开销不明确反映拥塞水平 LS 信息的全局性通过链路状态广播算法来完成 之后使用——伟大的Dijkstra!!! 路由振荡问题： 如图的链路为了避免选择高拥塞的道路，每次LS之后都会改变道路，导致了路由实际上处在振荡之中，并且从结果上来看，其选择的也并不是全局最优解 解决方案： 要求链路开销不依赖负载（不合理） 确保并非所有路由器同时运行LS，但是由于自同步的存在很困难，避免自同步可以采用链路通告随机化的方式 DV 迭代：循环计算直到没有更多信息需要交换 异步：不要求所有计算同步执行 分发式：每个节点计算接收邻居的信息，执行计算之后再发回去 基本原理是动态规划Bellman-Ford方程： dx→y=min⁡v∈Γ(x)(cx→v+dv→y)d_{x\\to y} = \\min_{v\\in\\Gamma(x)}(c_{x\\to v} + d_{v\\to y}) dx→y​=v∈Γ(x)min​(cx→v​+dv→y​) 算法如下： 给定图G=(V,E)G = (V, E)G=(V,E) ∀x∈V\\forall x \\in V∀x∈V，维护如下信息： 其与每个直接邻居的开销cx→vc_{x\\to v}cx→v​ 距离向量Dx→=[Dx→y: ∀y∈V]\\overrightarrow{D_{x}} = [D_{x\\to y}:\\text{ } \\forall y \\in V]Dx​​=[Dx→y​: ∀y∈V] 其所有邻居的距离向量 每个节点不时向邻居发送自己的距离向量 某个节点接收到邻居的信息或发现与自己连接的链路开销有变的时候，根据BF方程更新自己的距离向量 如果距离向量发生了变化，则发送给邻居 可以证明，lim⁡Dx→y→dx→y\\lim D_{x\\to y} \\to d_{x\\to y}limDx→y​→dx→y​ 注：图中cx→yc_{x\\to y}cx→y​应该是222而非212121 链路开销改变与链路故障 当链路开销增加时，很容易导致链路故障，如下图 右边的图会出现选择选择环路，即： Init: Dz→x=5(z→y→∗x),Dy→x=4(y→z→∗x)D_{z\\to x} = 5(z \\to y \\to^{*}x), D_{y\\to x} = 4(y \\to z \\to^{*}x)Dz→x​=5(z→y→∗x),Dy→x​=4(y→z→∗x) 1st forward: Dy→x=6(y→z→∗x),Dz→x=7(z→y→∗x)D_{y\\to x} = 6(y \\to z \\to^{*}x), D_{z\\to x} = 7(z \\to y \\to^{*}x)Dy→x​=6(y→z→∗x),Dz→x​=7(z→y→∗x) 2nd forward: Dz→x=8(z→y→∗x),Dy→x=9(y→z→∗x)D_{z\\to x} = 8(z \\to y \\to^{*}x), D_{y\\to x} = 9(y \\to z \\to^{*}x)Dz→x​=8(z→y→∗x),Dy→x​=9(y→z→∗x) … Until: Dz→x=50(z→x),Dy→x=51(y→z→x)D_{z\\to x} = 50(z\\to x), D_{y\\to x} = 51(y\\to z \\to x)Dz→x​=50(z→x),Dy→x​=51(y→z→x) 在最终情况之前，y,zy, zy,z所保存的到xxx的路径都是错误的，当变化后的开销（此处是4→604\\to 604→60）过大的时候，迭代轮次会过大导致传播速率迅速降低 毒性逆转 解决上述特定问题的方式（对于节点度数超过333的环路将无法解决） 如果zzz需要通过yyy到达xxx，则在zzz发送过去的信息中，记Dz→x=∞D_{z\\to x} = \\inftyDz→x​=∞ 善意的小谎言~ 两种算法的比较 记n=∣V∣,m=∣E∣n = |V|, m = |E|n=∣V∣,m=∣E∣： 报文复杂性：由于LS是全局的，因此其需要O(mn)O(mn)O(mn)个报文进行初始化，并且在一条链路发生改变时需要传递给所有节点，更复杂 收敛速度：LS复杂度O((m+n)log⁡n)O((m + n)\\log n)O((m+n)logn)，DV收敛很慢 鲁棒性：LS鲁棒性更强，因为全局算法相对来说路由器是解耦的，但是DV中一个不正确的节点会扩散到全局 OSPF 自治系统：由一组通常处在相同管理控制下的路由器组成，通常一个ISP中的路由器和其链路构成同一个AS，一个自治系统内的路由选择算法为自治系统内部路由选择协议 OSPF是一种LS，也即一个自治系统内采用全局广播的形式，并且即使未发生变化，也要周期性的广播链路状态，同时各条链路的开销，同时要检查链路运行状态，并允许路由器向相邻路由器广播 优点： 安全：能够鉴别OSPF路由器之间的交换，防止数据入侵 并发：允许使用多条相同开销的路径 可综合：容易扩展为MOSPF，从而支持多播 可层次化：支持一个AS中的层次化，即一个自治系统可以被划分为多个区域，每个区域之间可以相互交流，并且只包含一个主干 BGP 用于AS之间的通信，是一种分布式、异步的协议。 作用 在BGP中，一个路由器的转发表具有(xi,I)(x_{i}, I)(xi​,I)的形式，分别代表前缀与接口号 从邻接AS处获得前缀的可达性信息 并且每个子网可以向其他部分广播自己的存在性 确定到该前缀的最优路由 可达性通告 如上图，每对路由器中间使用179端口的半永久TCP连接，每个AS内部的会话为iBGP，跨AS的称为eBGP，于是通告路径如下： xxx子网向自己所在的AS的网关路由器发报文通知自己存在 网关路由器3a3a3a告知邻接的AS网关路由器：AS3中存在子网xxx 2c2c2c接收到这个消息，并通知AS2内的所有路由器 2a2a2a将信息发送给相邻的AS1，告知其xxx存在AS3内，并且可由AS2到达 同样的，不同AS之间可以增加对等链路，这样会导致子网和路由器之间存在多条路径 最优路由选择 在通告的子网前缀中增加一些属性，称为BGP属性，前缀及其属性称为路由，比较重要的属性包括： AS-PATH：这个AS是一条路由器路径中的一个，包含了已经通过的路由器列表，可以用于防止环路 NEXT-HOP：AS-PATH的起始路由器接口地址，每个AS的不同路由器接收到的NEXT-HOP属性可能不一样，用于指示从该路由器出发怎么找到子网 热土豆 查找AS内部路由转发信息，找到通往不同NEXT-HOP的最低开销路径，进而选择开销最低的那条 也即，热土豆追求的是贪心的尽快将报文传递出这个AS 路由器选择 当一个路由器希望到达一个前缀时，会将到该前缀的路由集合进行优先级排序，优先级如下： 每个路由增加一个本地偏好属性，属性值取决于管理员，本地偏好越高越优先 本地偏好相同时，选择AS-PATH最短的路由，由此规则确定路由之后通过DV决定路径 都相同时，采用热土豆 热土豆仍然无法选择时，采用BGP标识符 IP 任播 用于DNS中的服务，通常用语降低时延，例如CDN会向其下的多台服务器分配相同的IP，这样当一台路由器向这个IP发送信息的时候，路由器会向最近的一个服务器转发请求 路由选择策略 客户网络在多宿的情况下，可能会有类似提供商网络的行为，因此，其需要向相邻的所有提供商网络通告自己不能连通任何其他目的地，这样可以确保客户与提供商身份的相对稳定性 任何穿越ISP主干网的流量必须是其源或目的中至少一个唯一ISP的客户网络中（商业原因） SDN SDN体系结构具有4个关键特征： 基于流的转发：分组转发规则被规定在流表中，SDN控制平面用于计算、管理和安装流表项 数据平面和控制平面分离 位于数据平面交换机外部的网络控制：控制平面独立于数据平面之外 可编程的网络：网络控制应用程序是可编程的 控制器与控制程序 控制器的功能需要有： 通信层：负责控制器与数据平面之间的交流，称为南向API 网络范围管理层：控制决定层，配置流表以完成端到端转发、负载均衡、防火墙等功能 与应用程序接口：负责控制器与控制程序之间的交流，称为北向API OpenFlow协议 运行在实现了OpenFlow API的设备上，例如SDN控制器和数据平面之间，基于TCP，默认端口6653 控制器发送的重要报文包括： 配置：查询并设置交换机的配置参数 修改状态：增加、删除或修改交换机的流表项，设置交换机端口特性 发送分组：在交换机的特定端口发送特定报文 交换机发送的重要报文报告： 流删除：通知控制器删除一个流表项 端口状态：通知控制器端口状态的变化 分组入：将分组发送给控制器，如果该分组不能被流表匹配则控制器会做额外处理，如果可以匹配则会将该分组作为一个动作 实例 ICMP 主机和路由器之间用来沟通网络层信息的协议，最典型的用途是差错报告 通常被认为是IP的一部分，体系上位于IP之上，其内容作为IP报文的有效载荷 报文中包含一个类型字段和一个编码字段，包含引发该ICMP的IP的首部和前8个字节 这些报文可被用于探测，例如Traceroute程序利用ICMP来探测路由器的名字与IP地址 网络管理和SNMP 定义是一个冗长的单句： 网络管理框架 网络管理关键组件 如上图，关键组件包括： 管理服务器：控制网络管理信息的收集、处理、分析与显示，由人类控制 被管设备：被管理的真实设备，有若干个被管对象组成，被管对象包括实际硬件与配置参数 MIB：位于一个被管设备中收集被管对象的关联信息的数据库，其每个对象由SMI语言定义 网络管理代理：运行在被管设备中的进程，用于与管理服务器通信 网络管理协议：运行在管理服务器和被管设备之间的协议，为管理者提供了相应操作的能力 SNMP 一种网络管理协议，最常用的是请求响应模式，即管理服务器向代理发送请求，通常用于检索或修改MIB对象。其次可被用于代理向管理服务器发送陷阱报文，通知服务器有异常情况导致了MIB对象的改变 SNMP的PDU通过UDP传输，超时重传由管理服务器决定","tags":["笔记","网原","网络层"],"categories":["计算机网络原理"]},{"title":"网原笔记4","path":"/2024/05/08/网原笔记4/","content":"计算机网络原理 笔记 4 网络层 概述 网络层分为数据平面和控制平面 数据平面是将数据在输入链路和输出链路之间进行转发，控制平面是协调转发操作 转发和路由选择 转发：将数据报从输入链路转移到输出链路（数据平面） 路由选择：决定每个分组的路由（控制平面） 转发表：由路由选择确定，决定了转发的路由 确定转发表 传统方法：人工 路由器决定自身的转发表，但是需要路由器间的通信 SDN：由远程控制器决定每个路由器的转发表 网络服务模型 可能的服务： 确保交付 时延上界 有序分组 最小带宽 安全性 尽力而为服务：不提供任何服务 工作原理 四个组件： 输入端口：查询转发表决定输出路由，并且将数据转移至交换结构 交换结构：连接输入端口与输出端口 输出端口：从建环结构获取数据并此昂输出链路传输 路由选择处理器：执行控制平面功能，计算转发表（通常是一种传统的CPU） 转发策略： 基于目的地转发 通用转发 输入端口处理和基于目的地转发 最简单的情况下，每一个目的地址有一个对应的链路接口对应，采用前缀匹配的方法与最长前缀匹配规则（同时匹配多个前缀的时候选择最长的那个），为了效率通常使用SRAM，DRAM，TCAM 排队是指不同输入端口的数据在进入交换结构时排队 交换 交换方式很多 内存：直接经由CPU控制，这导致了速率较慢（受到内存带宽的限制），并且不能同时转发多个分组 总线：每一段数据会加上一个标签用于标记输出端口，所有输出端口都能接收数据但是只有被标记的可以保存数据，速率受到总线速率的影响，并且不允许并发 互联网：如纵横式交换机，共2N2N2N条总线，当想从输入端口发送到特定输出端口时只需要闭合对应交点即可，不同输出端口的分组可以并行 输出端口 何处排队 输入排队 当交换结构的速率不够快的时候会发生，并且会有线路前部阻塞，即同一个输出端口队列中前部的分组被堵塞会导致后面的也被堵塞 如果分组到达速率达到容量的58%58\\%58%，则输入队列会无限增长 输出排队 发送过快时会发生，采用丢弃新包或已有包来解决，同时有主动队列管理策略，如随机早期检测 传输顺序由分组调度决定 缓存的数量BBB与链路容量CCC的关系为： B=RTT∗CB = \\text{RTT} * C B=RTT∗C 分组调度 排队的分组怎么经过输出链路传输问题 FCFS：先来先服务 优先权排队：被分类放入优先权类中，同一类中的分组采用FCFS，非抢占式优先权排队中，分组开始传输就不能被打断 循环和加权公平排队：分组会被分类，但是不同类之间是平等的，也即会依次循环发送每一个类中的队列头，当某个类为空（链路空闲）时立即寻找其下一个类，例如加权公平排队，每个类会分配一个权重，并且加权分类吞吐量 网际协议 IPv4 版本号：确定剩余解释方式 首部长度：确定载荷与选项的分隔 服务类型：区别不同类型IP数据报 数据报长度：首部 + 数据 标识、标志、片偏移：与分片有关 寿命（TTL）：确保数据报不会循环 协议：指定运输层协议 检验和：检测比特错误，求和取反码 IPv4分片 一个链路层帧能承载的最大数据量称为最大传送单元，限制着IP数据包长度，并且一段路径上的链路之间可能有不同协议不同的MTU，因此采用分片技术，每一个大数据报被分为若干片 标识、标志、片偏移三个字段用于分片，标志比特用于标记最后一个片，片偏移用于决定正确的顺序 IPv4编址 点分十进制记法：每个字节用十进制书写，不同字节用句点隔开，如127.0.0.1127.0.0.1127.0.0.1 具有相同前缀的一些主机或路由器可以连接形成子网 其中223.1.1.0/24223.1.1.0/24223.1.1.0/24代表前242424为相同，/24/24/24为子网掩码 地址分配策略为无类别域间路由选择 地址聚合 得到地址 获取组织地址 由上游管理机构分配 获取主机地址 采用动态主机配置协议（又称即插即用或零配置），主机可以通过其来自动获取IP地址 DHCP发现：主机发现能够交互的DHCP服务器，通过DHCP发现报文，由链路层进行广播 DHCP提供：DHCP服务器回应一份DHCP提供报文，包含一些必要信息 DHCP请求：客户选择一个服务器，向其发送DHCP请求报文 DHCP ACK：服务器回应DHCP ACK报文 网络地址转换 在小型区域内合理使用一个IP地址的方法 NAT路由器是一个具有单一IP地址的打你设备，其中包含一张NAT转换表，用于将公网IP和端口转换为子网IP与端口 也即NAT用公网的端口进行寻址 中间盒 网络核心中的非交换机组成，包括NAT，流量负载均衡，流量防火墙等 IPv6 改动： 地址容量扩大：32→12832\\rightarrow 12832→128，引入任播地址，将数据报交给一组主机中的任意一个 简化首部 流标签：发送方要求进行特殊处理的流 字段： 版本号，指明是IPv4还是IPv6 流量类型：同IPv4的TOS 下一个首部：运输层协议 跳限制：最多能经过的路由器数目 去除了分片，取而代之的是差错报文，即数据太大时会被直接丢弃，并且向发送方返回一个ICMP差错报文 IPv4迁移到IPv6 好笑版：宣布标志日 实用版：建隧道 隧道指两台IPv6路由器之间的IPv4路由器的集合，方法是将IPv6的整个数据报作为数据包裹在IPv4载荷中进行传输，并且在下一个IPv6节点进行解包 通用转发和SDN 匹配加动作转发表称为流表，每个表项包括： 首部字段值的集合 计数器集合 动作集合 匹配 对来自不同层次的协议首部的一部分字段进行匹配，允许通配，例如192.118.∗192.118.*192.118.∗将匹配所有192.118192.118192.118开头的IP地址 动作 转发：转发到特定端口、端口集合或其余所有端口 丢弃 修改字段：在被转发之前重写首部字段（IP协议不可重写） 封装并转发给远程控制器","tags":["笔记","网原","网络层"],"categories":["计算机网络原理"]},{"title":"网原笔记3","path":"/2024/05/08/网原笔记3/","content":"计算机网络原理 笔记 3 网络原理 运输层 无连接运输：UDP 优势： 关于发送什么数据以及何时发送的应用层控制更加精细 无需建立连接（无需握手） 无连接状态 报文段首部短 报文段 检验和 发送方将所有161616比特字段求和并取反码，得到检验和，接收方将所有161616比特字段（包括检验和）进行求和，如果结果不是全111则有错，只能检验不能恢复 可靠数据传输 下层协议可能不可靠 (ARQ) 功能： 差错检测 接收方反馈 重传 在上一组数据传完并得到ACK相应之前不会有下一组数据，也即上层协议的send不会被调用，称为停等协议 考虑处理ACK/NAK受损的问题： 在2.22.22.2中，可以看出在接收到ACK的时候会判断其数字是否与当前状态相同，如果不相同则视作NAK 为了处理丢包，在发送发建立一个定时器，使得其能够在一定时间未接收到ACK之后默认为丢包，重发分组并且重置定时器 流水线 减少停等带来的传输利用率低下（传播时延远远大于传输时延） 增加序号 双方缓存多个分组 差错恢复：回退N步与选择重传 回退N步 当base得到确认之后窗口开始滑动，具体的FSM如下： 超时的时候，重传所有已发送但是未被确认的分组，同时接收方会丢弃所有失序的分组 选择重传 窗口长度必须不大于分组序号空间大小的一半，反之无法正常工作，接收方会出现无法分辨重传与新分组的现象 接收方收到自身的滑动窗口之前的分组时仍要发送ACK，否则发送方无法知道已被接收，窗口不能滑动 可靠数据传输总结 TCP 连接 全双工服务：双向传输 点对点：一对一传输 传输路径：进程 -&gt; 套接字 -&gt; 发送缓存 -&gt; 网络层 -&gt; 接收缓存 -&gt; 套接字 -&gt; 进程 典型的MSS的值为146014601460字节 报文 接收窗口字段：用于流量控制，指示接收方愿意接受的字节数量 选项字段：协商MSS，或在高速网络下作为窗口调解因子 标志字段： ACK：确认接收 RST, SYN, FIN：用于建立和拆除连接 PSH：指示接收方立即上传数据 URG：指示紧急数据 序号和确认号 一个报文段的序号是指该报文段首字节的字节流编号，TCP将数据看成有序字节流，对每一个字节分别标号 确认号指的是期待收到的最小字节标号，例如发送方已经收到0∼1000\\sim1000∼100和200∼300200\\sim300200∼300，则确认号为101101101 往返时间与超时 时限必须要大于RTT SampleRTT：报文段从发出到接被确认接收所需要的时间，在任意时刻仅测量一个报文段的SampleRTT而不是计算所有待确认的报文段，得到结果后加权更新，同时计算RTT偏差：EstimatedRTT=(1−α)EstimatedRTT+αSampleRTT\\text{EstimatedRTT} = (1-\\alpha)\\text{EstimatedRTT} + \\alpha\\text{SampleRTT} EstimatedRTT=(1−α)EstimatedRTT+αSampleRTT DevRTT=(1−β)DevRTT+β∣SampleRTT−EstimatedRTT∣\\text{DevRTT} = (1-\\beta)\\text{DevRTT} + \\beta|\\text{SampleRTT} - \\text{EstimatedRTT}| DevRTT=(1−β)DevRTT+β∣SampleRTT−EstimatedRTT∣ 时限应当确定为：Timeout=EstimatedRTT+4DevRTT\\text{Timeout} = \\text{EstimatedRTT} + 4\\text{DevRTT} Timeout=EstimatedRTT+4DevRTT 在真实处理中，有一种技术是在每次超时之后将时限翻倍 可靠数据传输 冗余ACK：用于指示报文丢失，当重复收到一个报文段的333次冗余ACK，之后，立即重传其下一个报文 流量控制 使得发送速率与接收方的读取速率相匹配，通过发送发来维护接收窗口实现，指示接收方剩余的缓存空间 发送方保证发送到连接中但是未被确认的数据量小于rwnd即可 特例：当缓存已经满了的时候发送仅含一字节数据的报文段，此时接收方开始清空缓存，并在确认报文里发送新rwnd 三次握手 客户向服务器发送一个SYN为111的报文段，随机选择一个初始序号，请求连接 服务器接收，分配缓存与变量，选择初始序号，返回SYNACK报文段表示允许连接 客户端接收，分配缓存与变量，连接建立 关闭过程： 客户端发送FIN置111的报文段表示关闭请求，并接收ACK，清理变量和缓存 服务端发送FIN置111的报文段表示关闭请求，并接收ACK，清理变量和缓存 拥塞控制 原因 理想路由器，分组的到达速率接近链路容量时，排队时间趋近于无穷大 有缓存的路由器，发送方因为大时延进行不必要重传占据链路带宽 上游路由器发送的分组最终被丢弃，这样发送它所占用的资源就被浪费了 控制方法 端到端：网络层不反馈，全部依靠运输层 网络辅助：网络层会反馈一些信息 TCP拥塞控制 发送方维护一个拥塞窗口cwnd，满足 LastByteSent−LastByteAck≤min⁡(cwnd,rwnd)\\text{LastByteSent} - \\text{LastByteAck} \\leq \\min(\\text{cwnd}, \\text{rwnd}) LastByteSent−LastByteAck≤min(cwnd,rwnd) 发送速率为 cwndRTT字节/秒\\frac{\\text{cwnd}}{\\text{RTT}}字节/秒 RTTcwnd​字节/秒 发送方判定丢包为超时或三个冗余ACK TCP为自计时的 TCP拥塞控制算法 慢启动： cwnd初始值被确定为一个MSS，传输的报文段被首次确认的时候增加一个MSS，因此整体呈现几何级数增长的形式，同时在发送方维护ssthresh（慢启动阈值），结束增长有如下情况： 超时丢包：重新初始化并慢启动，令ssthresh=cwnd/2\\text{ssthresh} = \\text{cwnd}/2ssthresh=cwnd/2 cwnd=ssthresh\\text{cwnd} = \\text{ssthresh}cwnd=ssthresh：进入拥塞避免 333个冗余ACK：快速重传，进入快速恢复 拥塞避免 每个RTT只增加一个MSS而不是翻倍，例如每个ACK增加MSS2/cwnd\\text{MSS}^{2}/\\text{cwnd}MSS2/cwnd，结束控制如下： 超时丢包：同慢启动 333个冗余：快速重传，令ssthresh=cwnd/2,cwnd=ssthresh+3MSS\\text{ssthresh} = \\text{cwnd}/2, \\text{cwnd} = \\text{ssthresh} + 3\\text{MSS}ssthresh=cwnd/2,cwnd=ssthresh+3MSS 快速恢复 每个冗余ACK增加一个MSS，结束控制： 超时丢包：同慢启动 回顾 整体拥塞控制方法成为加性增，乘性减 另一种拥塞控制方法为基于延迟，即实时检测吞吐量，并于最大吞吐量cwnd/RTT\\text{cwnd}/\\text{RTT}cwnd/RTT进行比较，并且线性增减去趋近最大吞吐量 宏观吞吐量 以WWW代表窗口长度，则 Mean=0.75∗WRTTMean = \\frac{0.75*W}{\\text{RTT}} Mean=RTT0.75∗W​ 高带宽TCP 设LLL为丢包率，则 Mean=1.22∗MSSRTT∗LMean = \\frac{1.22*\\text{MSS}}{\\text{RTT}*\\sqrt{L}} Mean=RTT∗L​1.22∗MSS​ 这代表高吞吐率需要非常低的丢包率来支持 TCP公平性 公平性代表瓶颈链路分配给每条链路的资源应该是相近的 TCP趋近于多条链路之间平等分享，但是在实际应用中，RTT较小的通常吞吐量更大 公平与UDP UDP无拥塞控制，并且可能会抑制TCP 公平与并行TCP 一个应用使用多条TCP并行会导致占用过多资源，但是资源的公平应该是在应用层面上的 明确拥塞公告 网络层辅助的拥塞控制机制 IP协议的首部中有两个比特被用于标记ECN，当接收方收到ECN时则在回复的ACK中设置ECE，发送发收到之后进行窗口减半处理（和超时丢包相同），并在下一个报文段首部标记CWR字段（拥塞窗口缩减）","tags":["笔记","网原","运输层"],"categories":["计算机网络原理"]},{"title":"网原笔记2","path":"/2024/05/08/网原笔记2/","content":"计算机网络原理 笔记 2 网络原理 应用层 应用层协议原理 APP是运行在端系统上，而不是诸如路由器等的网络核心设备上，因为网络核心设备基本只在网络层及以下的地方起作用 网络应用程序体系结构 两种主流体系结构： 客户-服务器体系 对等体系 客户-服务器 服务器：一个总是打开的主机，处理来自其他客户主机的请求，例如Web浏览器等 客户之间不会直接通信，而是需要经过服务器中转，并且服务器具有固定的地址（称之为IP地址） 数据中心：为了防止单一的服务器主机无法处理大量请求，部分服务商会部署数据中心，其中配备有大量主机，用于模拟服务器 对等 端系统主机之间直接通信，无需经过服务器中转，一些流量密集型应用采用的是P2P结构 P2P结构具有自扩展性，每个对等方通过请求文件产生工作负载，但是其也可以通过向其他对等方分发文件提升系统服务能力 进程通信 这里讨论的是不同端系统之间进程的通信，其通过交换报文相互通信 客户与服务器进程 对于任意一对进行通信的进程，在会话开始时等待联系的一方为服务器，发起通信的一方为客户，无论其采用的体系结构是可恶-服务器或P2P 进程与计算机网络的接口 进程间通过套接字（也被称为API）接口进行报文的发送和接收，套接字是应用层与运输层的接口，开发者可以选择运输层协议，并借助该协议进行开发 进程寻址 接收进程的地址包括： 主机地址 在目标主机中指定接收进程的标识符 主机由其IP地址确定，是一个32bit32bit32bit的量并且可以唯一标识一个主机，指定接收进程由目的地端口号保证，一个端口号只能接收一个进程的信息 可供应用程序使用的运输服务 一个运输层协议所能提供的服务分为四类： 可靠数据传输 吞吐量 定时 安全性 可靠数据传输 由于丢包、数据损坏等情况，数据可能发生丢失，因此，我们需要一种使得发送的数据一定可以正确、完全的交付给另一方的协议，称之为可靠数据运输 部分应用（例如音视频、原神等）允许一定量的数据丢失，这被称为容忍丢失的应用 吞吐量 可用吞吐量：两个进程之间发送比特的速率，由于其他会话会共享带宽，因此可用吞吐量会随着时间波动 因此，一部分协议保证了应用可用吞吐量的下界，这对于一些带宽敏感应用（具有特定的吞吐量要求）是很有必要的，与之相对，弹性应用不需要限制吞吐量 定时 定时协议可以保证时延的上界，例如可以确保发送的比特一定会在100ms100ms100ms内到达接收方，广泛应用于实时交互的应用中 安全性 协议能够提供数据的加密、解密，以保证只有进程可以直接观察到发送的的数据，同时还有数据完整性鉴别、端点鉴别等 因特网提供的运输服务 包括TCP与UDP两种 TCP 包括面向连接服务与可靠传输服务 面向连接服务：应用岑报文开始流动之前，TCP让客户服务器进行握手（交换运输层控制信息），握手结束后即建立了一条TCP连接，双方进程可以在该连接上进行报文的收发，结束发送至后必须拆除连接 可靠传输服务：同上 同时，TCP拥有拥塞控制机制，可以在适当时机抑制发送进程，并且限制每个连接使之公平共享带宽 由于其没有加密机制，因此有基于TCP的SSL，可以提供关键的安全性服务,SSL不是一种新的因特网运输协议 UDP 轻量级，仅提供最小服务，没有握手机制、拥塞控制机制、可靠传输等 因特网运输协议所不提供的服务 没有包括定时和吞吐量等，这些操作被巧妙的设计所尽量保障，但是在一些极端情况下仍然会被限制 应用层协议 应用层协议定义了进程之间传递报文的格式，如： 交换报文的类型 各种报文类型的语法 报文中字段的语义 确定进程何时、如何发送报文 对报文进行响应的规则 应用层协议是网络应用的重要组成部分 Web &amp; HTTP HTTP概况 Web页面有对象组成，可以通过URL（由主机名+路径名构成）寻址访问Web服务器实现了HTTP服务器端，用于存储Web对象，基本通信方式如下： HTTP是一个无状态协议，也即其不会存储有关客户的任何信息（例如短时间内连续请求信息，则服务器每次会重新发送） 持续连接与非持续连接 客户与服务器之间需要进行一系列请求，并且两个请求之间的间隔可能是随机的或是周期性的，所以不同请求可以使用不同的TCP连接或者同一个TCP连接，被分为非持续连接和持续连接两种，HTTP1.1及更高版本默认情况下使用的是持续连接，HTTP1.0采用的是非持续连接，而HTTP2.0版本更新了队列机制，不强制要求FCFS，而是可以让用户自己定义优先级 非持续连接 每一个对象需要一次TCP连接 定义往返时间：一个短分组从客户到服务器再返回客户的时间，如下： 上图中设计三次握手过程，其中前两个过程消耗了一个RTT，最后一次握手以及发送HTML文件这个响应操作消耗了一个RTT，因此总响应时间为2∗RTT+2*\\text{RTT} +2∗RTT+传输文件时间 持续连接 非持续连接需要为每个请求的对象建立并维护一个连接，需要大量TCP缓冲区与变量，并且会导致相对更高的时延 持续连接即建立连接后，即使完成请求也不关闭，因此不同请求可以使用这一条连接进行，避免了反复建立连接，当连接在一定时间内没有被使用时才会被关闭 HTTP 报文格式 分为请求报文与响应报文两种 HTTP请求报文 第一行称作请求行，其后续都称为首部行 请求行分为方法、URL、HTTP版本三个字段 主机的信息提供给Web代理高速缓存 第三行用于关闭持续连接 第四行用于指明用户代理，即浏览器类型 第五行用于指明需要得到的语言版本 通用格式如下： 实体体在GET方法中为空，在POST等方法中包含信息，例如用户在搜索框内的输入信息等，而需要给服务器提供信息的操作不一定是POST操作，例如可以将信息附在URL中然后使用GET操作 HTTP响应报文 比请求报文多了一个实体体的部分，同时第一行称作状态行 状态行包括协议版本，状态码与状态信息三部分 第二行与请求报文相同 第三行表示了发送响应报文的时间 第四行表示服务器 第五行表示发送的对象被最后修改的时间，对于缓存来说非常重要 第六行表示发送对象的字节数 第七行表示对象类型 通用格式如下： 用户与服务器的交互：cookie 允许站点对用户进行跟踪，技术有如下四个组件： 响应报文中的cookie首部行 请求报文中的cookie首部行 端系统中的cookie文件 后端数据库 用户首次访问一个站点的时候，服务器会为其建立一个 cookie用来唯一标识这个客户，并将其发送给浏览器，后续会话中浏览器与服务器之间可以通过cookie来确定用户信息 Web缓存 又称代理服务器，可以理解为是客户和初始服务器之间的一个代理，可以提升用户的访问速度，用户可以往缓存中发请求，如果缓存中拥有的话则可以直接返回响应，反之则需要在初始服务器中进行寻找，因此可以有效的降低时延并且减少供应商成本 条件GET方法 用于确定缓存中的数据是最新的方法，具体来说，缓存数据会存储数据的最近修改时间，因此，如果用户在请求报文中增加了行 1If-modified-since: xxx 则缓存与代理之间会经过一次通信确定文件在xxx时间之后是否有被修改过，这种报文称之为条件GET请求 电子邮件 &amp; SMTP SMTP 步骤如下： 发送方代理将报文发送给自身邮件服务器，并存储在报文队列中 发送方服务器SMTP与接收方服务器SMTP直接建立TCP连接 握手结束后，通过该连接发送报文 接收方服务器接收报文，并将其放入接收方的邮箱中 SMTP可以通过可靠数据传输将邮件完整的发送到接收方，并且其采用的是直接连接的方式 SMTP有一条特殊规则是：只包含nnn个句点符号的单行，表示的是n−1n-1n−1个句点，因为单个句点是指示报文结束，对话形式如下图： 由用户端发送的、全大写的字符串代表特殊命令，其含义可以直接翻译理解 与HTTP比较 同： 都用于在两台主机之间传送文件 都是持续连接 异： HTTP是拉协议，即此时连接由接收方向发送方发起；TCP是推协议，即此时连接由发送方向接收方发起 SMTP要求发送数据必须编码为ASCII字符，但是HTTP没有限制 对于多对象（例如包含图片、视频、音频等）文档，HTTP将每个对象分别封装，但是SMTP将其全部封装在一起 报文格式 使用的是RFC 5322定义，其中的From, To两行是必选的 访问协议 由于SMTP是一个推协议，因此用户是无法通过自己设备上的代理向服务器请求邮件的，因此我们没有办法实时读取到存储在服务器中的邮件，为了解决这个问题引入了邮件访问协议，如POP3, IMAP, HTTP POP3 由RFC 1939定义，极其简单，首先用户向服务器提出建立TCP连接，建立之后依次分为三个阶段：特许，事务处理与更新 鉴权（特许）阶段：客户端使用命令user 与pass 鉴别身份信息（明文发送） 事务处理：客户端允许使用list, retr , dele 三条命令，分别代表：列出所有邮件长度，接收id号邮件，删除id邮件 更新：当用户使用了quit命令之后进入更新阶段，服务器删除被标记的斑纹，POP3会话结束 每次客户端发来一个命令之后，服务端的回复是+OK 或-ERR IMAP 允许用户可以在远程服务器上操作邮件，包括创建文件夹、移动邮件、在远程文件夹上查询邮件等，更加方便，并且允许用户获取报文的部分，以避免大量信息造成网络负担过重 邮件中的HTTP 在代理和服务器直接发送信息的时候采用HTTP协议，在服务器之间传输的时候采用SMTP，也即将浏览器当成用户代理 DNS：因特网的目录 用于转换主机在主机名与IP地址之间的一种系统，主机名方便人类记忆，而IP更容易被计算机处理 DNS的服务 DNS是指： 由分层的DNS服务器实现的分布式数据库 使主机能够查询分布式数据库的应用层协议 DNS服务器运行BIND软件，协议运行在UDP之上，使用53号端口 DNS通常是一种被其他应用层协议使用的应用程协议，用于将它们请求报文中的主机名转换为IP地址，而并不常与用户直接通信 除了转换外还有其他服务： 主机别名，部分主机拥有多个主机名，而其中有一个成为规范主机名，而别名的存在是为了更方便的记忆，因此DNS可以识别别名 邮件服务器别名：电子邮件的后缀可以是别名，由邮件app调用DNS进行处理 负载分配：部分站点有多个服务器，也即一个规范主机名会对应多个IP地址，因此DNS用于调配这些地址之间的负载，用户向这个主机名发送请求等效于向当前队列中最前方的IP发送请求 工作机理概述 从用户来看，DNS是一个提供转换服务的黑盒，但是其内部是由大量DNS服务器及应用层协议组成的 简单设计：全球仅有一台DNS服务器，会有诸多问题 单点故障导致全球故障 通信容量巨大 远距离的集中式数据库导致高时延 维护复杂 因此采用了分布式的设计方案 分布式层次数据库 从上到下依次为：根服务器，顶级域服务器与权威服务器，访问一个主机名的时候从上至下访问服务器，从右至左依次匹配 根服务器：全球有400400400多个 顶级域：例如.com, .edu等 权威：每个公共可访问主机的组织需要提供公共可访问的DNS记录，这些记录被记录在权威服务器中 本地DNS服务器：属于ISP，起到的是代理、加速作用 每次向服务器发送请求是，得到的是下一级的服务器IP地址列表，权威服务器将会返回查询地址的IP，同时，权威服务器有可能需要用过中间服务器再次分层，也即权威-&gt;中间-&gt;权威 查询方式分为递归查询与迭代查询，上图中，请求主机与本地服务器之间为递归查询，其与全部为迭代查询，即迭代查询是接受请求后直接返回，而递归查询是接受请求后向其他服务器查询之后再返回给请求方 DNS缓存 为了改善时延并且减少报文数量，DNS服务器可以将请求/回答信息环城存在本地存储器中，以便更快返回，由于IP对应关系不永久，因此缓存信息会被定时清除 DNS记录与报文 DNS服务器存储了资源记录，其提供了主机名到IP地址的映射，形式为： (Name, Value, Type, TTL)\\text{(Name, Value, Type, TTL)} (Name, Value, Type, TTL) 其中TTL代表的是应当删除的时间，其余三个的对应如下： Type = A，则Name是主机名，Value是对应的IP地址 Type = NS，则Name是一个域，Value是域中可获取主机IP的权威服务器主机名 Type = CNAME，则Name是一个别名，Value是对应的规范主机名 Type = MX，则Name是一个别名，Value是对应邮件服务器的规范主机名 报文 前121212字节称为首部区域，标识符用于匹配，标志用于提供一些额外信息 问题区域包含查询信息，包括主机名、问题类型（查规范主机名还是邮件服务器等） 回答区域包含了资源记录，可以包含多条 在DNS数据库中插入 略 P2P文件分发 对等方直接通信，减少对服务器的依赖 P2P体系的可扩展性 FFF：文件长度 NNN：对等方数量 usu_sus​：服务器接入链路的上传速率 uiu_iui​：第iii个对等方接入链路的上传速率 did_idi​：第iii个对等方接入链路的下载速率 考虑在客户-服务端与P2P两种模式下所需要的分发时间 客户-服务器体系Dcs≥max(NFus,Fdmin)D_{cs} \\geq max(\\frac{NF}{u_s}, \\frac{F}{d_{min}}) Dcs​≥max(us​NF​,dmin​F​) P2PDP2P≥max(Fus,Fdmin,NFus+∑ui)D_{P2P} \\geq max(\\frac{F}{u_s}, \\frac{F}{d_{min}}, \\frac{NF}{u_s + \\sum u_i}) DP2P​≥max(us​F​,dmin​F​,us​+∑ui​NF​) 当对等方数量非常多时，采用P2P将会具有很好的效果（增长缓慢） BitTorrent 一种P2P协议，其中，参与特定文件分发的所有对等方集合被称为一个洪流，每个洪流有一个基础设施节点称为追踪器，其中记录并追踪了每个对等方及其是否离开了洪流 新对等方向追踪器请求对等方列表，并尝试向所有对等方建立TCP连接，成功建立连接之后称为邻近对等方，并向所有的邻近对等方请求未含有的块 请求块的方法是最稀缺优先，请求其邻居中所含副本最少的块，以便尽快达到块数量的均衡 给其他对等方上传数据时，每隔一段时间选择向其发送数据最快的444个对等方，其集合成为疏通，并向它们上传块，同时会每隔一段时间随机寻找新对等方进行对换，也即最多会向555个对等方上传块，这种激励机制成为一报还一报 视频流和CDN 因特网视频 比特率决定视频质量以及对传输所需要的流量 HTTP流和DASH 常规的HTTP流为对视频进行编码后使用常规方法进行发送，在用户端有两种方式，一种为缓存字节数超过一定数目就开始播放，另一种为流式视频，即按帧缓存，从接受视频开始即播放 DASH被称为经HTTP的动态适应性流，其将视频编码为不同版本，随着带宽的变化选择不同版本，服务器中存在告示文件，提供不同版本的URL与分辨率 CDN 分布在多个地理位置上的服务器，用于帮助世界各地的用户尽快获取内容 服务器安置原则为： 深入：遍历接入ISP来深入其中，靠近端用户 邀请做客：在关键位置部署少量大集群，邀请ISP做客 CDN采用拉策略，并非将视频存储在每一个集群中，当集群缺少这个视频时则会向其他集群检索并缓存 CDN操作","tags":["笔记","网原","应用层"],"categories":["计算机网络原理"]},{"title":"网原笔记1","path":"/2024/05/08/网原笔记1/","content":"计算机网络原理 笔记 1 网络原理 概要 考核 随堂测 15% 作业 25% 期末考试 60% Key problems Multiple access control: MAC rooting naming: how to give each user a unique id Congestion control（阻塞控制） RDT: Reliable Data Transfrom Protocol 协议：协议定义了网络实体之间发送和接收消息的格式、顺序，以及对消息传输、接收所采取的操作 网络核心 分组交换 端系统（主机）之间交换报文，报文包含了通讯者需要的信息，并通过通信链路从源发送至目的地 源会将长报文划分为较小的数据块，称为分组，通信链路上拥有分组交换机，以使得以链路允许的最大传输速率传输报文，分为路由器和链路层交换机两种 存储转发传输：在链路发送信息时必须要整组发送，即需要等待源将一组信息完全发送至交换机才能发送至目的地 端到端时延：通过NNN条传输速率为RRR的链路组成的路径，从源向目的地发送一个LLL比特的分组，端到端时延（传输时延）为d=NLRd = N\\frac{L}{R} d=NRL​ 发送PPP个分组的时延为d=(N+P−1)LRd = (N+P-1)\\frac{L}{R} d=(N+P−1)RL​ 排队时延与丢包：分组交换机拥有一个输出队列（缓存），当交换机的输出速率小于输入速率时，后进入的包会进入缓存中等待，这一种延迟称之为排队时延，主要取决于网络阻塞程度，当缓存被填满时，再次有包进入时会导致有的包（可能是新来的或队列中的）被丢弃，即为丢包 转发表：路由器决定应该将信息往哪一条链路发送的方式，可以将目的地IP地址（或其一部分）映射到对应的输出链路，转发表通常会根据路由转移协议来自动设置 电路交换 与分组交换的最大区别在与：电路交换会提前预留端到端通信所需要的资源，包括缓存、链路传输数据等，每一条链接称为一条电路，因此其时延主要来自于建立电路与在电路上传播 复用：分为频分复用与时分复用 频分复用指一组连接共用一段频谱，每个连接有一个独享的频段。例如调频无线电台使用FDM共享88MHz-108MHz的频谱，每一个电台会被分配一个特定的频段（通常带宽为4MHz），使用完毕之后会被回收投入下一次使用 时分复用指，将时间分割为固定的帧，每一帧被分割为固定量的时隙，每个时隙只传播一个连接的数据（类似并行的概念） 电路交换会有静默期，也即建立了电路之后不传播信息，可能会引发资源浪费，但是电路交换的端到端时延与链路数量无关（在路由器处无需等待） 通常情况下，分组交换的效率会优于电路交换 网络的网络 此节讨论我们怎么能够使用网络资源，也即因特网的结构（发展动力主要是商业竞争） 网络结构111：一个单一的全球ISP互联所有接入ISP，所有的客户直接向该全球ISP付费 网络结构222：多个全球ISP，全球ISP之间是互联的，用户可以向性价比最高的全球ISP付费 网络结构333：有区域ISP，ISP按照层级高低分为接入、区域、第一层（全球传输），低层需要向直接连接的高层付费 网络结构444：在结构333的基础上增加了PoP、多宿，对等与IXP PoP：提供商网络中位于相同位置的一组路由器，客户ISP可以通过其与提供商ISP相连接 多宿：任何非顶层ISP可以与多台上游ISP相连接 对等：同层的相邻ISP之间直接连接，无需通过上游ISP进行中转，通常对等无需付费 IXP：可以使得多个ISP一起对等，类似于一个中转站 网络结构555：在网络结构444的基础上增加了内容提供商网络，更多的用于直接与较低层的ISP互联，避免由于中间ISP的收费 时延、丢包与吞吐量 时延 delay=dproc+dqueue+dtrans+dpropdelay = d_{proc} + d_{queue} + d_{trans} + d_{prop} delay=dproc​+dqueue​+dtrans​+dprop​ 处理时延：检查数据是否有错，决定输出链路等，数量级为μs\\mu sμs 排队时延：在输出队列中等待的时间，取决于网络拥塞程度，数量级为ms−μsms-\\mu sms−μs 传输时延：将数据从路由器传输到链路的时间，数量级在ms−μsms-\\mu sms−μs 传播时延：数据在链路上传播的时间，通常很小（传播速率在108m/s10^8m/s108m/s量级），但是在长距离传播中需要考虑 排队时延和丢包 流量强度定义为 ti=LaRti=\\frac{La}{R} ti=RLa​ 其中，aaa代表数据到达队列的平均速率，LLL代表分组的平均比特数，ti→0ti\\rightarrow 0ti→0时排队时延很小，ti→1−ti\\rightarrow 1^-ti→1−时排队时延很大，ti&gt;1ti&gt;1ti&gt;1的时候排队时延无界 当ti≤1ti\\leq 1ti≤1的时候，分组到达的方式（周期性或突发性等）将会影响排队时延 当路由器的输出队列已满时，新进入的分组会被丢弃，称为丢包 端到端时延 dend−end=N(dproc+dtrans+dprop)d_{end-end} = N(d_{proc} + d_{trans} + d_{prop}) dend−end​=N(dproc​+dtrans​+dprop​) 其中 dtrans=LRd_{trans} = \\frac{L}{R} dtrans​=RL​ 这个式子假设了网络是畅通的 吞吐量 定义：主机接受文件的速率，分为瞬间吞吐量与平均吞吐量 瓶颈链路：指平均传输速率最小的一条链路，限制了整个网络的吞吐量 上述的平均指的是，分配给每条链路的传输速率，例如一条速率为100Mbps的高速链路，需要同时承担100010001000个客户-服务器的通信，那平均速率将会降至100kbps，有可能成为瓶颈链路 协议层次及其服务模型 分层的体系结构 协议分层 为了更好的结构化与模块化，网络以分层的方式组织协议与硬软件，每一层通过在该层中执行动作或使用直接下层的服务来提供服务。 各层的所有协议被称为协议栈，因特网的协议栈自顶向下为应用层，运输层，网络层，链路层，物理层 应用层：网络应用程序以及其应用层协议存留的地方，其中的信息分组称为报文。应用层提供了许多协议，如： HTTP：Web文档协议 SMTP：电子邮件报文协议 FTP：端系统协议 DNS：域名系统协议 运输层：用于在应用程序端点之间传送应用层报文，有两种运输协议：TCP, UDP，其中TCP提供了截断机制与拥塞控制机制。运输层的分组称为报文段 网络层：分组称为数据报，网络层负责将数据报从一台主机移动到另一台主机，网际协议包括IP，其定义了数据报中的各个字段以及端、路由器如何作用在这些字段上 链路层：在节点之间传递数据，每个节点的网络层将数据下放给链路层，传递至下一个节点之后再上传至网络层。如以太网、WiFi、电缆接入网的DOCSIS协议，其中的分组称为帧 物理层：在物理层面上将帧中的比特移到下一个节点，每一种链路层中包括很多物理层协议，与实际的物理媒介有关 OSI模型 777层分层，包括应用层、表示层、会话层、运输层、数据链路层、物理层。 表示层用于将交换的数据可以被应用程序解释，会话层提供了数据交换的定界和同步功能，包括检查与恢复等 封装 每一层会将来自上一层的数据进行封装，也即附加一些首部信息，包括一些权限信息以及检测信息等，引测，每一层的分组都有两种字段，首部字段和有效载荷字段 坏家伙 危害终端设备 坏家伙将恶意软件植入设备中，并利用僵尸网络（被控制的主机）展开攻击，从而实现自我复制。恶意软件分为病毒与蠕虫两类： 病毒：需要用户交互来感染用户设备的恶意软件 蠕虫：无需用户交互即可进入设备 危害服务器和网络设施 拒绝服务攻击，也即DoS, 包括以下三种： 弱点攻击：攻击不完备的应用或操作系统 带宽洪泛：往目标发送大量分组，例如DDoS通过大量源向目标发送分组造成目标瘫痪 连接洪泛：在目标中创建大量的半开或全开TCP连接 嗅探分组：在传输分组时，精心布置的被动接收机可以得到传输信息的副本，并且由于其不会注入信息，所以很难被检测出来，这种接收机被称为分组嗅探器 身份伪装：通过IP哄骗，向目标发送具有恶意的信息","tags":["笔记","网原","概要"],"categories":["计算机网络原理"]},{"title":"Hexo + Stellar","path":"/2024/05/07/Hexo初探/","content":"终于弄好了www 经过一天的不懈奋斗，在经过了jekyll配置环境的痛苦折磨之后，最后选择了用hexo+github pages配置，hexo是一款静态博客工具，使用起来比较简单，指比让从来没有配过ruby的我去弄明白jekyll简单多了！ 配置方法 新建github仓库，名为&lt;username&gt;.github.io，这个是后来访问用的 回到本地命令行（笔者用的是wsl）123456npm install -g hexo-clisudo npm install -g hexo-cli (Mac) # 据说Mac得这么干cd AN-EMPTY-FLRODER # 进入你想放置的本地文件夹 一定要是空的！hexo init # 初始化hexo内容npm install # 下载配置npm install hexo-deployer-git --save # 下载部署工具 此时环境基本配置完毕了，开始修改配置，打开_config.yml，将其Deployment部分改为1234deploy: type: git repository: git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git branch: main 部署准备完成1hexo g -d # 生成页面并部署 此时可以访问https://&lt;username&gt;.github.io访问自己的博客！ 操作方法 12345678hexo new &quot;name&quot; # 新建文章hexo new page &quot;name&quot; # 新建页面hexo g # 生成页面hexo d # 部署hexo g -d # 生成页面并部署hexo s # 本地预览hexo clean # 清除缓存和已生成的静态文件hexo help # 帮助 更换主题 进入网站https://hexo.io/themes/可以选择主题，挑选好后进行配置，配置方法为在博客根目录.下执行如下命令： 1git clone THEME-REPO themes/&lt;theme-name&gt; 例如本博客使用的是stellar主题，执行命令为： 1git clone git@github.com:xaoxuu/hexo-theme-stellar.git themes/stellar 下载好后，在./_config.yml下修改theme的内容为你想要的即可 致谢 https://zhuanlan.zhihu.com/p/60578464 Debug TypeError: Cannot read properties of null (reading 'utcOffset') 时区设置错误，允许的中国时区只有Asia/Harbin,Asia/Shanghai,Asia/Chongqing,Asia/Urumqi,Asia/Kashgar 无法生成.html文件 检查themes下的文件名和配置文件中的是否相同，如果相同尝试先clean再重新创建，如果还是不行则直接重新clone一下主题库（原因未知） 行内公式无法渲染 更换md渲染器，并添加Katex支持，方法为：123npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --savenpm i @traptitech/markdown-it-katex --save 并在_config.yml中加入（注意缩进）：123456789101112131415161718markdown: preset: &#x27;default&#x27; render: html: true xhtmlOut: true breaks: true langPrefix: &#x27;language-&#x27; linkify: true typographer: true quotes: &#x27;“”‘’&#x27; plugins: - plugin: name: &#x27;@traptitech/markdown-it-katex&#x27; options: # see https://katex.org/docs/options.html blockClass: &quot;math-block&quot; strict: false throwOnError: false errorColor: &quot;#cc0000&quot; 单纯做了上面的操作之后会出现行内数字/字母重复渲染的现象，也即一遍纯文本一遍公式文本，例如’ISP’会被渲染成’ISPISPISPISP’ 在文章头部加上katex: true即可 静态图片问题 使用相对于source文件夹的绝对路径，例如/assets/...代表存在/blog/sources/assets/...下，这样在本地md可能显示会有问题，但是stellar可以正常生成"},{"title":"杂记","path":"/freenotes/index.html","content":"本文会记录一些平常开发过程中遇到的小小问题 WSL配置Chrome环境 在wsl环境下进行一些Web相关的开发时需要使用chrome来进行，因此我们需要在其中安装chrome浏览器，总结出可行的步骤如下： 换源，否则在apt install的时候会出现404的错误，以更换清华源为例，具体方法为： 进入/etc/apt文件夹，保存一份source.list的副本（副本名字任取）： $ cd /etc/apt &amp;&amp; cp ./source.list ./source.copy.list lsb_release -a查看wsl版本相关信息 前往清华镜像源网址 找到自己对应的版本格式对应的镜像信息，例如本人是Ubuntu 22.04 LTS(jammy) 利用vim等工具将source.list中的内容修改为清华镜像的内容 更新apt： $ sudo apt-get upgrade 进入想要安装的目录，依次执行： 123$ wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb$ sudo apt install --fix-broken -y$ sudo dpkg -i google-chrome-stable_current_amd64.deb 使用google-chrome --version确认已安装成功 参考资料：https://blog.csdn.net/m0_63834988/article/details/135044587https://blog.csdn.net/m0_63834988/article/details/135044587 https://blog.csdn.net/Jason_Todd/article/details/125479130https://blog.csdn.net/Jason_Todd/article/details/125479130 尚未解决的问题 无法显示中文 谷歌搜索不能进行（代理？ 会报一些关于dbus的错误，例如Failed to connect to the bus: Could not parse server address: Unknown address type (examples of valid types are &quot;tcp&quot; and on UNIX &quot;unix&quot;)等等比较多的 用windows吧 Windows环境配置问题 Anaconda在不同环境下python的优先级不同，有的用本地的python有得用虚拟环境里的，原因未知 Latex使用问题记录 minipage 如果想让多个minipage在水平方向上排列，则需要所有组件水平方向上的宽度之和小于\\linewidth vscode 编译Beamer的时候vscode有的情况下会出现一些奇怪的问题，例如和lstlisting不兼容，但是关机重启之后就能编译过了（不能理解 verb 在写Beamer的时候，在一个frame里面如果想使用\\verb命令的话需要在\\begin&#123;frame&#125;后加上选项[fragile] Go语言 超时错误 并没理解为什么go run也会超时）解决方法是使用国内的代理： 12go env -w GOPROXY=https://goproxy.cngo env -w GO111MODULE=on"},{"title":"关于","path":"/about/index.html","content":"欢迎来到我的个人博客！本人为清华大学计算机系二字班学生，第一次尝试写博客还请大家多多担待噢~ 这里主要想写一写平时学习中遇到的笔记和内容，也方便统一管理一些，当然以后也会有可能有一些随笔和杂谈之类的"}]